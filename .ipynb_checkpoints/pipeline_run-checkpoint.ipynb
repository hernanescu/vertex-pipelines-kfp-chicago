{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3109952b-ddd7-49d2-a4da-3183e757ac9e",
   "metadata": {},
   "source": [
    "# Vertex Pipelines - A Serverless framework for MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9aaf1-b8fc-4532-9a8c-d682321cd90c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345caf8f-187c-41c9-8e38-b715e6c55060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros cloud (almacenamiento de objetos, outputs, etc)\n",
    "# cloud parameters (object storage, outputs, etc)\n",
    "KFP_ARTIFACTS_BUCKET = 'chicago_taxi_artifacts'\n",
    "STAGE_DATA_BUCKET = 'chicago_taxi_stage'\n",
    "PIPELINE_BUCKET = 'chicago_taxi_pipelines'\n",
    "PIPELINE_ROOT = f\"gs://{PIPELINE_BUCKET}/pipeline_root/\"\n",
    "SERVING_CONTAINER = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'\n",
    "MACHINE_TYPE = 'n1-standard-16'\n",
    "\n",
    "# configuracion de intentos de tuneo de hiperparametros\n",
    "# configuration of hp tuning job trials\n",
    "HP_TRIALS = 1\n",
    "PARALLEL_TRIALS = 1\n",
    "\n",
    "# umbral de aceptabilidad para prediccion batch\n",
    "# acceptability threshold for batch prediction\n",
    "THRESHOLD= 0.6\n",
    "\n",
    "# rutas de acceso a los datasets de train, val y test\n",
    "# access paths for train, val and test datasets\n",
    "TRAIN_DATA_PATH = 'chicago_taxi_train.csv'\n",
    "VAL_DATA_PATH = 'chicago_taxi_val.csv'\n",
    "TEST_DATA_PATH = 'chicago_taxi_test.csv'\n",
    "\n",
    "\n",
    "#SAMPLE_SIZE = 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3418e6-0bda-49e1-9e66-b476df15dc47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROJECT_ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24976/2537429202.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mBQ_CURRENT_STAGE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stage_ml'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mBQ_CURRENT_RAW_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{PROJECT_ID}.{BQ_DATASET_CURRENT_NAME}.{BQ_CURRENT_RAW}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mBQ_CURRENT_STAGE_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{PROJECT_ID}.{BQ_DATASET_CURRENT_NAME}.{BQ_CURRENT_STAGE}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PROJECT_ID' is not defined"
     ]
    }
   ],
   "source": [
    "#BQ_ORIGINAL_SOURCE = 'bigquery-public-data.chicago_taxi_trips.taxi_trips'\n",
    "\n",
    "# BigQuery: definiciones de variables, pueden ser facilmente reemplazadas para adaptarse a otros propositos\n",
    "# BigQuery variable definitions: those can be easily changed to suit another purpose\n",
    "\n",
    "BQ_DATASET_HISTORIC_NAME = 'chicago_taxi_historic'\n",
    "BQ_DATASET_CURRENT_NAME = 'chicago_taxi_current'\n",
    "\n",
    "BQ_HISTORIC_RAW = 'raw'\n",
    "BQ_HISTORIC_STAGE = 'stage_ml'\n",
    "\n",
    "BQ_CURRENT_RAW = 'raw'\n",
    "BQ_CURRENT_STAGE = 'stage_ml'\n",
    "\n",
    "BQ_CURRENT_RAW_URL = f\"{PROJECT_ID}.{BQ_DATASET_CURRENT_NAME}.{BQ_CURRENT_RAW}\"\n",
    "BQ_CURRENT_STAGE_URL = f\"{PROJECT_ID}.{BQ_DATASET_CURRENT_NAME}.{BQ_CURRENT_STAGE}\"\n",
    "\n",
    "BQ_HISTORIC_RAW_URL = f\"{PROJECT_ID}.{BQ_DATASET_HISTORIC_NAME}.{BQ_HISTORIC_RAW}\"\n",
    "BQ_HISTORIC_STAGE_URL = f\"{PROJECT_ID}.{BQ_DATASET_HISTORIC_NAME}.{BQ_HISTORIC_STAGE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd984ac-7ab1-454a-bac9-fbbba93d7cee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  vertex-testing-327520\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Obtener el project ID\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e83c04-8ac8-4f76-8a42-e6f5a1a21c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ultimas variables de entorno: PATH local, region de GCP y timestamp\n",
    "\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "REGION=\"us-central1\" # disponibilidad completa de Vertex / Complete Vertex availability\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ad1f8-fee3-41b5-8a53-756be97ab822",
   "metadata": {},
   "source": [
    "### Librerias - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec0e429-6768-469b-bb9e-5b39252d944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  vertex-testing-327520\n",
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import kfp\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath, ClassificationMetrics\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform, bigquery\n",
    "\n",
    "# We'll use this namespace for metadata querying\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental.custom_job.utils import create_custom_training_job_op_from_component\n",
    "\n",
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f0d57-1619-4410-8356-adcd633a98a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd0a322-6cfa-4832-83b8-130ffce8d5d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @component(\n",
    "#     packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\", \"google-cloud-aiplatform\"],\n",
    "#     base_image=\"python:3.9\",\n",
    "#     #output_component_file=\"get_dataframe_herno.yaml\"\n",
    "# )\n",
    "# def bq_dw_creation(\n",
    "#     project: str,\n",
    "#     region: str,\n",
    "#     bq_original_source: str,\n",
    "#     bq_current_dest: str,\n",
    "#     bq_historic_dest: str,\n",
    "#     sample_size: int,\n",
    "#     output_current_path: OutputPath(str),\n",
    "#     output_historic_path: OutputPath(str)\n",
    "# ):\n",
    "#     from google.cloud import bigquery\n",
    "#     import pandas as pd\n",
    "#     import datetime as dt\n",
    "#     from google.cloud import aiplatform\n",
    "    \n",
    "#     aiplatform.init(project = project,\n",
    "#                     location = region)\n",
    "    \n",
    "#     def get_year_and_month():\n",
    "#         previous_month = (dt.date.today().replace(day=1) - dt.timedelta(days=33)).month\n",
    "#         year = dt.date.today().year\n",
    "    \n",
    "#         if previous_month == 12:\n",
    "#             year = year-1\n",
    "#         else:\n",
    "#             year\n",
    "#         return year, previous_month\n",
    "    \n",
    "#     def get_year_and_month_hist(current_year, current_month):\n",
    "#         month_hist = current_month - 1\n",
    "#         if month_hist == 0:\n",
    "#             year_hist = current_year -1\n",
    "#             month_hist = 12\n",
    "#         else:\n",
    "#             year_hist = current_year\n",
    "    \n",
    "#         return year_hist, month_hist\n",
    "    \n",
    "#     YEAR, MONTH = get_year_and_month()\n",
    "#     HIST_YEAR, HIST_MONTH = get_year_and_month_hist(YEAR, MONTH)\n",
    "#     SAMPLE_SIZE = sample_size\n",
    "#     BQ_CURRENT_DEST = bq_current_dest\n",
    "#     BQ_HISTORIC_DEST = bq_historic_dest\n",
    "#     PROJECT_ID = project\n",
    "#     BQ_LOCATION = region\n",
    "    \n",
    "    \n",
    "#     current_sql_script = '''\n",
    "#     CREATE OR REPLACE TABLE `@CURRENT_TABLE` \n",
    "#     AS (\n",
    "#         WITH\n",
    "#           taxitrips AS (\n",
    "#           SELECT\n",
    "#             trip_start_timestamp,\n",
    "#             trip_seconds,\n",
    "#             trip_miles,\n",
    "#             payment_type,\n",
    "#             pickup_longitude,\n",
    "#             pickup_latitude,\n",
    "#             dropoff_longitude,\n",
    "#             dropoff_latitude,\n",
    "#             tips,\n",
    "#             fare\n",
    "#           FROM\n",
    "#             `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "#           WHERE 1=1 \n",
    "#           AND pickup_longitude IS NOT NULL\n",
    "#           AND pickup_latitude IS NOT NULL\n",
    "#           AND dropoff_longitude IS NOT NULL\n",
    "#           AND dropoff_latitude IS NOT NULL\n",
    "#           AND trip_miles > 0\n",
    "#           AND trip_seconds > 0\n",
    "#           AND fare > 0\n",
    "#           AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "#           AND EXTRACT(MONTH FROM trip_start_timestamp) = @MONTH\n",
    "#         )\n",
    "\n",
    "#         SELECT\n",
    "#           trip_start_timestamp,\n",
    "#           EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "#           EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "#           EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "#           EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "#           trip_seconds,\n",
    "#           trip_miles,\n",
    "#           payment_type,\n",
    "#           ST_AsText(\n",
    "#               ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "#           ) AS pickup_grid,\n",
    "#           ST_AsText(\n",
    "#               ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "#           ) AS dropoff_grid,\n",
    "#           ST_Distance(\n",
    "#               ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "#               ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "#           ) AS euclidean,\n",
    "#           CONCAT(\n",
    "#               ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
    "#                   pickup_latitude), 0.1)), \n",
    "#               ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
    "#                   dropoff_latitude), 0.1))\n",
    "#           ) AS loc_cross,\n",
    "#           IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "#           IF(ABS(MOD(FARM_FINGERPRINT(STRING(trip_start_timestamp)), 10)) < 9, 'UNASSIGNED', 'TEST') AS data_split\n",
    "#         FROM\n",
    "#           taxitrips\n",
    "#         LIMIT @LIMIT\n",
    "#     )\n",
    "#     '''\n",
    "    \n",
    "#     current_sql_script = current_sql_script.replace(\n",
    "#         '@CURRENT_TABLE', BQ_CURRENT_DEST).replace(\n",
    "#         '@YEAR', str(YEAR)).replace(\n",
    "#         '@LIMIT', str(SAMPLE_SIZE)).replace(\n",
    "#         '@MONTH', str(MONTH))\n",
    "    \n",
    "#     historic_sql_script = '''\n",
    "#     CREATE OR REPLACE TABLE `@HISTORIC_TABLE` \n",
    "#     AS (\n",
    "#         WITH\n",
    "#           taxitrips AS (\n",
    "#           SELECT\n",
    "#             trip_start_timestamp,\n",
    "#             trip_seconds,\n",
    "#             trip_miles,\n",
    "#             payment_type,\n",
    "#             pickup_longitude,\n",
    "#             pickup_latitude,\n",
    "#             dropoff_longitude,\n",
    "#             dropoff_latitude,\n",
    "#             tips,\n",
    "#             fare\n",
    "#           FROM\n",
    "#             `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "#           WHERE 1=1 \n",
    "#           AND pickup_longitude IS NOT NULL\n",
    "#           AND pickup_latitude IS NOT NULL\n",
    "#           AND dropoff_longitude IS NOT NULL\n",
    "#           AND dropoff_latitude IS NOT NULL\n",
    "#           AND trip_miles > 0\n",
    "#           AND trip_seconds > 0\n",
    "#           AND fare > 0\n",
    "#           AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "#           AND EXTRACT(MONTH FROM trip_start_timestamp) = @MONTH\n",
    "#         )\n",
    "\n",
    "#         SELECT\n",
    "#           trip_start_timestamp,\n",
    "#           EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "#           EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "#           EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "#           EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "#           trip_seconds,\n",
    "#           trip_miles,\n",
    "#           payment_type,\n",
    "#           ST_AsText(\n",
    "#               ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "#           ) AS pickup_grid,\n",
    "#           ST_AsText(\n",
    "#               ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "#           ) AS dropoff_grid,\n",
    "#           ST_Distance(\n",
    "#               ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "#               ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "#           ) AS euclidean,\n",
    "#           CONCAT(\n",
    "#               ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
    "#                   pickup_latitude), 0.1)), \n",
    "#               ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
    "#                   dropoff_latitude), 0.1))\n",
    "#           ) AS loc_cross,\n",
    "#           IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "#           IF(ABS(MOD(FARM_FINGERPRINT(STRING(trip_start_timestamp)), 10)) < 9, 'UNASSIGNED', 'TEST') AS data_split\n",
    "#         FROM\n",
    "#           taxitrips\n",
    "#         LIMIT @LIMIT\n",
    "#     )\n",
    "#     '''\n",
    "    \n",
    "#     historic_sql_script = historic_sql_script.replace(\n",
    "#         '@TABLE', BQ_HISTORIC_DEST).replace(\n",
    "#         '@YEAR', str(YEAR)).replace(\n",
    "#         '@LIMIT', str(SAMPLE_SIZE)).replace(\n",
    "#         '@MONTH', str(MONTH))\n",
    "    \n",
    "#     bq_client = bigquery.Client(project=PROJECT_ID, location=BQ_LOCATION)\n",
    "#     job_current = bq_client.query(current_sql_script)\n",
    "#     _ = job_current.result()\n",
    "    \n",
    "#     job_historic = bq_client.query(historic_sql_script)\n",
    "#     _ = job_historic.result()\n",
    "    \n",
    "#     with open(output_current_path, 'w') as f:\n",
    "#         f.write(BQ_CURRENT_DEST)\n",
    "        \n",
    "#     with open(output_historic_path, 'w') as f:\n",
    "#         f.write(BQ_HISTORIC_DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d4abf84-9565-4f73-8b75-60cd865edafd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### pipeline mio\n",
    "# @dsl.pipeline(name='xgb-chicago-parte2',\n",
    "#                 pipeline_root=PIPELINE_ROOT)\n",
    "# def pipeline(\n",
    "#     project_id: str = PROJECT_ID,\n",
    "#     gcp_region: str = REGION,\n",
    "#     #bq_source_url: str = BQ_SOURCE_URL, ojo aca\n",
    "#     kfp_artifacts_bucket: str = KFP_ARTIFACTS_BUCKET,\n",
    "#     stage_data_bucket: str = STAGE_DATA_BUCKET,\n",
    "#     pipelines_bucket: str = PIPELINE_BUCKET,\n",
    "#     pipeline_root: str = PIPELINE_ROOT,\n",
    "#     train_container: str = TRAIN_CONTAINER,\n",
    "#     serving_container: str = SERVING_CONTAINER,\n",
    "#     machine_type: str = MACHINE_TYPE,\n",
    "#     #thresholds_dict_str: str = '{\"roc\":0.8}',\n",
    "#     test_data_path: str = TEST_DATA_PATH,\n",
    "#     trials: int = HP_TRIALS,\n",
    "#     parallel_trials: int = PARALLEL_TRIALS,\n",
    "#     bq_current_raw_url: str = BQ_CURRENT_RAW_URL,\n",
    "#     bq_current_stage_url: str = BQ_CURRENT_STAGE_URL,\n",
    "#     bq_historic_raw_url: str = BQ_HISTORIC_RAW_URL,\n",
    "#     bq_historic_stage_url: str = BQ_HISTORIC_STAGE_URL,\n",
    "#     bq_original_source: str = BQ_ORIGINAL_SOURCE,\n",
    "#     sample_size: int = SAMPLE_SIZE\n",
    "    \n",
    "    \n",
    "# ):\n",
    "    \n",
    "#     bq_dw_creation(\n",
    "#         project = project_id,\n",
    "#         region = gcp_region,\n",
    "#         bq_original_source = bq_original_source,\n",
    "#         bq_current_dest = bq_current_raw_url,\n",
    "#         bq_historic_dest = bq_historic_raw_url,\n",
    "#         sample_size = sample_size\n",
    "        \n",
    "#     )\n",
    "#     bq_stage_ml = bq_historic_raw_to_stage_ml(\n",
    "#         project = project_id,\n",
    "#         region = gcp_region,\n",
    "#         bq_historic_raw_url = bq_historic_raw_url,\n",
    "#         bq_historic_stage_url = bq_historic_stage_url\n",
    "        \n",
    "#     )\n",
    "    \n",
    "#     bq_current_ml = bq_current_raw_to_stage_ml(\n",
    "#         project = project_id,\n",
    "#         region = gcp_region,\n",
    "#         bq_current_raw_url = bq_current_raw_url,\n",
    "#         bq_current_stage_url = bq_current_stage_url\n",
    "        \n",
    "#     )\n",
    "    \n",
    "#     dataframe = get_chicago_data(project = project_id,\n",
    "#                                  region = gcp_region,\n",
    "#                                  bq_source_url = bq_stage_ml.output,\n",
    "#                                  stage_data_bucket = stage_data_bucket)\n",
    "    \n",
    "#     train_xgb_op = train_xgb_chicago_op(dataframe.outputs['dataset_train'],\n",
    "#                                          project = project_id,\n",
    "#                                          location = gcp_region)\n",
    "    \n",
    "#     train_rf_op = train_rf_chicago_op(dataframe.outputs['dataset_train'],\n",
    "#                                          project = project_id,\n",
    "#                                          location = gcp_region)\n",
    "    \n",
    "#     model_selection = model_evaluation(\n",
    "#         test_set = dataframe.outputs['dataset_test'],\n",
    "#         xgb_chicago_model = train_xgb_op.outputs['model'],\n",
    "#         rf_chicago_model = train_rf_op.outputs['model'],\n",
    "#         #thresholds_dict_str = '{\"roc\":0.8}'\n",
    "    \n",
    "#     )\n",
    "    \n",
    "#     hp_search = best_model_hp_tuning(\n",
    "#         project = project_id,\n",
    "#         region = gcp_region,\n",
    "#         #bq_source_url = bq_source_url,\n",
    "#         stage_data_bucket = stage_data_bucket,\n",
    "#         winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "#         trials = trials,\n",
    "#         parallel_trials = parallel_trials\n",
    "#         #model_spec = model_selection.outputs['model_spec']\n",
    "#     )\n",
    "    \n",
    "#     best_model = train_best_model_op(\n",
    "#         dataset_train = dataframe.outputs['dataset_train'], # idealmente, hacer otro que junte ambos\n",
    "#         dataset_test = dataframe.outputs['dataset_test'],\n",
    "#         project = project_id,\n",
    "#         location = gcp_region,\n",
    "#         winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "#         parameters = hp_search.outputs['model_spec']\n",
    "#     )\n",
    "    \n",
    "#     batch_predict_op = gcc_aip.ModelBatchPredictOp(\n",
    "#         project=project_id,\n",
    "#         location=gcp_region,\n",
    "#         job_display_name=\"chicago-batch-predict\",\n",
    "#         model=best_model.outputs[\"model\"],\n",
    "#         #gcs_source_uris=[\"{0}/batch_examples.csv\".format(BUCKET_NAME)],\n",
    "#         bigquery_source_input_uri = bq_current_ml.output,\n",
    "#         instances_format=\"csv\",\n",
    "#         gcs_destination_output_uri_prefix=pipelines_bucket,\n",
    "#         machine_type=\"n1-standard-8\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ba39ab-806a-4348-9bb3-e822bd3fd60b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "#         package_path='xgb-chicago-parte2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "331bf388-6556-4a43-9ac2-506a10632042",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline_job = aiplatform.PipelineJob(\n",
    "#     display_name=\"xgb-chicago-displaynamepipejob\",\n",
    "#     template_path=\"xgb-chicago-parte2.json\",\n",
    "#     job_id=\"xgb-chicago-parte2-{0}\".format(TIMESTAMP),\n",
    "#     enable_caching=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3fb0390-ff8b-4633-bc7e-8d80426f0d8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec6af77-1787-4c17-88e3-5eb5912d314e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07938d9-3700-4dce-a24b-c0ccb51da3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\", \"pandas-gbq\", \"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"bq_current_raw_to_stage.yaml\"\n",
    ")\n",
    "\n",
    "def bq_current_raw_to_stage_ml(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    bq_current_raw_url: str,\n",
    "    bq_current_stage_url: str,\n",
    "    stage_data_bucket: str,\n",
    "    gcs_predict_source: OutputPath(str)\n",
    "):\n",
    "    '''\n",
    "    Toma el dataset de BigQuery establecido como el presente y lo procesa, colocandolo en la tabla stage_ml. Tambien sube una version timestamped de la data en csv.\n",
    "    Takes the Bigquery Dataset established as the present and processes it, placing it in the stage_ml table. It also uploads a timestamped csv version of the data.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    import numpy as np\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage import Blob\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    aiplatform.init(project = project,\n",
    "                    location = region)\n",
    "    \n",
    "    ### get data from bq_source\n",
    "    bqclient = bigquery.Client(project = project, location = region)\n",
    "    \n",
    "\n",
    "    # Download a table\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_current_raw_url\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table\n",
    "    )\n",
    "    data = rows.to_dataframe(\n",
    "        create_bqstorage_client=True, # guarda ac치\n",
    "    )\n",
    "    \n",
    "    # process\n",
    "    \n",
    "    df = data[['trip_month', 'trip_day', 'trip_day_of_week', 'trip_hour', 'trip_seconds', 'trip_miles', 'payment_type', 'euclidean']]\n",
    "    \n",
    "    df2 = pd.get_dummies(df, columns = ['payment_type'], drop_first = True)\n",
    "    \n",
    "    df2.columns = df2.columns.str.replace(' ','_')\n",
    "    \n",
    "    # upload to bq\n",
    "    \n",
    "    df2.to_gbq(bq_current_stage_url,\n",
    "               project,\n",
    "               chunksize=None, \n",
    "               if_exists='replace', # el default tira error, aca queremos que siempre reemplace\n",
    "               table_schema=[{'name': 'trip_month','type': 'INTEGER'},\n",
    "                             {'name': 'trip_day','type': 'INTEGER'},\n",
    "                             {'name': 'trip_day_of_week','type': 'INTEGER'},\n",
    "                             {'name': 'trip_hour','type': 'INTEGER'},\n",
    "                             {'name': 'trip_seconds','type': 'INTEGER'},\n",
    "                             {'name': 'trip_miles','type': 'FLOAT'},\n",
    "                             {'name': 'euclidean','type': 'FLOAT'},\n",
    "                             #{'name': 'target','type': 'INTEGER'}, eliminamos el target para simular la realidad\n",
    "                             {'name': 'payment_type_Credit_Card','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Dispute','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Mobile','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_No_Charge','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Prcard','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Unknown','type': 'INTEGER'}\n",
    "                             ]\n",
    "    )\n",
    "    \n",
    "    # ponerle a la data tambien un timestamp\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "\n",
    "    TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    DATA_PATH = f\"predicted_data_{TIMESTAMP}.csv\"\n",
    "    \n",
    "    df2.to_csv(DATA_PATH, index = False)\n",
    "    \n",
    "    gcsclient = storage.Client() # tal vez vaya stage_data_bucket\n",
    "    bucket = gcsclient.get_bucket(stage_data_bucket)\n",
    "    \n",
    "    blob_train = bucket.blob(DATA_PATH)\n",
    "    blob_train.upload_from_filename(DATA_PATH)\n",
    "    \n",
    "    \n",
    "    GCS_PREDICT_SOURCE = f\"gs://{stage_data_bucket}/{DATA_PATH}\"\n",
    "    \n",
    "    with open(gcs_predict_source, 'w') as f:\n",
    "              f.write(GCS_PREDICT_SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89fde0cb-8259-45b8-a37a-0886b0825cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\", \"pandas-gbq\", \"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"bq_historic_raw_to_stage.yaml\"\n",
    ")\n",
    "\n",
    "def bq_historic_raw_to_stage_ml(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    bq_historic_raw_url: str,\n",
    "    bq_historic_stage_url: str,\n",
    "    \n",
    ") -> str:\n",
    "    \n",
    "    '''\n",
    "    Toma el dataset de BigQuery establecido como el periodo historico y lo procesa, colocandolo en la tabla stage_ml.\n",
    "    Takes the Bigquery Dataset established as historic and processes it, placing it in the stage_ml table.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    import numpy as np\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage import Blob\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    aiplatform.init(project = project,\n",
    "                    location = region)\n",
    "    \n",
    "    ### get data from bq_source\n",
    "    bqclient = bigquery.Client(project = project, location = region)\n",
    "    \n",
    "\n",
    "    # Download a table.\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_historic_raw_url\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table\n",
    "    )\n",
    "    data = rows.to_dataframe(\n",
    "        create_bqstorage_client=True, # guarda ac치\n",
    "    )\n",
    "    \n",
    "    df = data[['trip_month', 'trip_day', 'trip_day_of_week', 'trip_hour', 'trip_seconds', 'trip_miles', 'payment_type', 'euclidean', 'tip_bin']]\n",
    "    \n",
    "    df = df.rename(columns = {'tip_bin':'target'})\n",
    "    \n",
    "    df2 = pd.get_dummies(df, columns = ['payment_type'], drop_first = True)\n",
    "    \n",
    "    df2.columns = df2.columns.str.replace(' ','_')\n",
    "    \n",
    "    df2.to_gbq(bq_historic_stage_url,\n",
    "               project,\n",
    "               chunksize=None, # I have tried with several chunk sizes, it runs faster when it's one big chunk (at least for me)\n",
    "               if_exists='replace', # el default tira error, aca no queremos eso\n",
    "               #verbose=False\n",
    "               table_schema=[{'name': 'trip_month','type': 'INTEGER'},\n",
    "                             {'name': 'trip_day','type': 'INTEGER'},\n",
    "                             {'name': 'trip_day_of_week','type': 'INTEGER'},\n",
    "                             {'name': 'trip_hour','type': 'INTEGER'},\n",
    "                             {'name': 'trip_seconds','type': 'INTEGER'},\n",
    "                             {'name': 'trip_miles','type': 'FLOAT'},\n",
    "                             {'name': 'euclidean','type': 'FLOAT'},\n",
    "                             {'name': 'target','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Credit_Card','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Dispute','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Mobile','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_No_Charge','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Prcard','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Unknown','type': 'INTEGER'}\n",
    "                             ]\n",
    "    )\n",
    "    \n",
    "    URL_TO_GO = bq_historic_stage_url\n",
    "    \n",
    "    return URL_TO_GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466e077c-ac25-4be7-8150-84fbc3406b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"get_chicago_data.yaml\"\n",
    ")\n",
    "\n",
    "def get_chicago_data(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    bq_source_url: str,\n",
    "    stage_data_bucket: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_val: Output[Dataset],\n",
    "    dataset_test: Output[Dataset] \n",
    "):\n",
    "    '''\n",
    "    Toma los datos que se consideran historicos de la tabla de BQ y separa en train, validation y test. Ademas de pasarlos como componentes del pipeline, guarda una version de los datos en el bucket de stage.\n",
    "    Takes the data considered as historic from the BQ table and splits it into train, validation and test. Besides passing them as pipeline component, it stores a version of the data in the stage bucket.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage import Blob\n",
    "    \n",
    "    TRAIN_DATA_PATH = 'chicago_taxi_train.csv'\n",
    "    VAL_DATA_PATH = 'chicago_taxi_val.csv'\n",
    "    TEST_DATA_PATH = 'chicago_taxi_test.csv'\n",
    "    \n",
    "    ### get data from bq_source\n",
    "    bqclient = bigquery.Client(project = project, location = region)\n",
    "    \n",
    "\n",
    "    # Download tje table.\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_source_url\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "\n",
    "    )\n",
    "    data = rows.to_dataframe(\n",
    "        create_bqstorage_client=True, # guarda ac치\n",
    "    )\n",
    "    \n",
    "    # splits in train, val and test\n",
    "      \n",
    "    train, test = tts(data, test_size=0.3)\n",
    "    train_data, val_data = tts(train, test_size = 0.2)\n",
    "    \n",
    "    train_data.to_csv(TRAIN_DATA_PATH)\n",
    "    val_data.to_csv(VAL_DATA_PATH)\n",
    "    test.to_csv(TEST_DATA_PATH)\n",
    "    \n",
    "    ### so far we have the paths, we have to upload them to the bucket / hasta aca est치n los csvs en los PATH\n",
    "    gcsclient = storage.Client() \n",
    "    bucket = gcsclient.get_bucket(stage_data_bucket)\n",
    "    \n",
    "    blob_train = bucket.blob(TRAIN_DATA_PATH)\n",
    "    blob_train.upload_from_filename(TRAIN_DATA_PATH)\n",
    "    \n",
    "    blob_train = bucket.blob(VAL_DATA_PATH)\n",
    "    blob_train.upload_from_filename(VAL_DATA_PATH)\n",
    "    \n",
    "    blob_test = bucket.blob(TEST_DATA_PATH)\n",
    "    blob_test.upload_from_filename(TEST_DATA_PATH)\n",
    "    \n",
    "    train_data.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    val_data.to_csv(dataset_val.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17baca31-e2a3-4605-bd38-15f9fb07f30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a32623ae-ef36-4b21-8e5e-44e8f77c5417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# componentes de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a717addd-2985-496c-9632-ff16498b9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42e4a753-425a-4ef2-bb01-c8b5f78ab9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @component(\n",
    "#     packages_to_install = [\n",
    "#         \"pandas\",\n",
    "#         \"sklearn\",\n",
    "#         \"xgboost==1.4\"\n",
    "#     ], base_image=\"python:3.9\",\n",
    "# )\n",
    "# def train_xgb_chicago(\n",
    "#     dataset:  Input[Dataset],\n",
    "#     model: Output[Model], \n",
    "# ):\n",
    "#     '''\n",
    "#     Definicion de componente custom: entrena un modelo XGB usando la data que viene de la particion de train, pasada como componente.\n",
    "#     Custom component definition: train an XGB model using the data that comes from the train partition, passed as component.\n",
    "#     '''\n",
    "    \n",
    "#     from xgboost import XGBClassifier\n",
    "#     import pandas as pd\n",
    "#     import pickle\n",
    "\n",
    "#     data = pd.read_csv(dataset.path+\".csv\")\n",
    "#     model_xgb = XGBClassifier()\n",
    "#     model_xgb.fit(\n",
    "#         data.drop(columns=[\"target\"]),\n",
    "#         data.target,\n",
    "#     )\n",
    "#     model.metadata[\"framework\"] = \"XGB\"\n",
    "#     file_name = model.path + f\".pkl\"\n",
    "#     with open(file_name, 'wb') as file:  \n",
    "#         pickle.dump(model_xgb, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e997ef06-ab96-4c8d-ad62-7aaf57f2af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definicion del job usando el componente custom previamente creado\n",
    "# # Job definition using previously created custom component\n",
    "\n",
    "# train_xgb_chicago_op = create_custom_training_job_op_from_component(\n",
    "#     train_xgb_chicago,\n",
    "#     machine_type='n1-standard-16'# todas las specs se las podes pasar aca / you can pass the specs here\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "578c93db-8170-498c-a498-8ea623ae8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "    ], base_image=\"python:3.9\",\n",
    ")\n",
    "def train_rf_chicago(\n",
    "    dataset:  Input[Dataset],\n",
    "    model: Output[Model], \n",
    "):\n",
    "    '''\n",
    "    Definicion de componente custom: entrena un modelo Random Forest usando la data que viene de la particion de train, pasada como componente.\n",
    "    Custom component definition: train a Random Forest model using the data that comes from the train partition, passed as component.\n",
    "    '''\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "\n",
    "    data = pd.read_csv(dataset.path+\".csv\")\n",
    "    model_rf = RandomForestClassifier()\n",
    "    model_rf.fit(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "    model.metadata[\"framework\"] = \"RF\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(model_rf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "229cb088-9424-4595-82cc-8fe56334e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion del job usando el componente custom previamente creado\n",
    "# Job definition using previously created custom component\n",
    "\n",
    "train_rf_chicago_op = create_custom_training_job_op_from_component(\n",
    "    train_rf_chicago,\n",
    "    machine_type='n1-standard-16'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "279420d4-422e-4d58-92ba-cc111f6fdfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "    ], base_image=\"python:3.9\",\n",
    ")\n",
    "def train_lr_chicago(\n",
    "    dataset:  Input[Dataset],\n",
    "    model: Output[Model], \n",
    "):\n",
    "    '''\n",
    "    Definicion de componente custom: entrena un modelo Regresion Logistica usando la data que viene de la particion de train, pasada como componente.\n",
    "    Custom component definition: train a Logistic Regression model using the data that comes from the train partition, passed as component.\n",
    "    '''\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "\n",
    "    data = pd.read_csv(dataset.path+\".csv\")\n",
    "    model_lr = LogisticRegression()\n",
    "    model_lr.fit(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "    model.metadata[\"framework\"] = \"LR\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(model_lr, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94716edb-1dca-42e4-9ba3-29fe5f16b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion del job usando el componente custom previamente creado\n",
    "# Job definition using previously created custom component\n",
    "\n",
    "train_lr_chicago_op = create_custom_training_job_op_from_component(\n",
    "    train_lr_chicago,\n",
    "    machine_type='n1-standard-16'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3b36703-29f3-4b47-9bce-a3784e7b9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "        #\"xgboost==1.4\"\n",
    "    ], base_image=\"python:3.9\",\n",
    ")\n",
    "def model_evaluation(\n",
    "    val_set:  Input[Dataset],\n",
    "    #xgb_chicago_model: Input[Model],\n",
    "    lr_chicago_model: Input[Model],\n",
    "    rf_chicago_model: Input[Model],\n",
    "    #thresholds_dict_str: str,\n",
    "    #metrics: Output[ClassificationMetrics],\n",
    "    #xgb_kpi: Output[Metrics],\n",
    "    lr_kpi: Output[Metrics],\n",
    "    rf_kpi: Output[Metrics],\n",
    "    #winning_model: Output[Model],\n",
    "    winning_model_name: Output[Artifact],\n",
    "):\n",
    "    '''\n",
    "    Evaluacion de modelos entrenados. Toma los modelos previamente entrenados (pkls) y los evalua segun la metrica F1. El nombre del ganador y NO el modelo en si mismo son pasados como componente, asi como tambien la metrica kpi deseada. \n",
    "    Evaluation of trained models. Grabs the previously trained models (pkls) and evaluates them according to F1 score metric. The name of the winner and NOT the model itself gets passed as component, as well as chosen kpi metrics.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #from xgboost import XGBClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import pickle\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, f1_score\n",
    "    import json\n",
    "    import typing\n",
    "\n",
    "    # xgb_model = XGBClassifier()\n",
    "    # file_name = xgb_chicago_model.path + \".pkl\"\n",
    "    # with open(file_name, 'rb') as file:  \n",
    "    #     xgb_model = pickle.load(file)\n",
    "        \n",
    "    rf_model = RandomForestClassifier()\n",
    "    file_name = rf_chicago_model.path + \".pkl\"\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        rf_model = pickle.load(file)\n",
    "        \n",
    "    lr_model = LogisticRegression()\n",
    "    file_name = lr_chicago_model.path + \".pkl\"\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        lr_model = pickle.load(file)\n",
    "    \n",
    "    data = pd.read_csv(val_set.path+\".csv\")\n",
    "    y_test = data.drop(columns=[\"target\"])\n",
    "    y_target=data.target\n",
    "    \n",
    "    \n",
    "    #y_pred_xgb = xgb_model.predict(y_test)\n",
    "    y_pred_rf = rf_model.predict(y_test)\n",
    "    y_pred_lr = lr_model.predict(y_test)\n",
    "\n",
    "  \n",
    "    # seleccion de modelo\n",
    "    #xgb_f1 = 0.3 ## HARDCODEO LA DERROTA DEL XGB f1_score(data.target, y_pred_xgb.round())\n",
    "    rf_f1 = 0.3 # HARDCODEO LA DERROTA DEL RF PARA PROBAR LA LR f1_score(data.target, y_pred_rf.round())\n",
    "    lr_f1 = f1_score(data.target, y_pred_lr.round())\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_dict = dict({lr_f1: lr_model, rf_f1: rf_model})\n",
    "    \n",
    "    def model_check(val1, val2):\n",
    "        if val1 >= val2:\n",
    "            return val1\n",
    "        else:\n",
    "            return val2\n",
    "    \n",
    "    best_f1 = model_check(lr_f1, rf_f1)\n",
    "    best_model = model_dict[best_f1]\n",
    "    \n",
    "        \n",
    "    #xgb_kpi.log_metric(\"f1_score\", float(xgb_f1))\n",
    "    rf_kpi.log_metric(\"f1_score\", float(rf_f1))\n",
    "    lr_kpi.log_metric(\"f1_score\", float(lr_f1))\n",
    "    \n",
    "    \n",
    "    winning_model_name_str = type(best_model).__name__\n",
    "    \n",
    "    winning_dict = {'model': winning_model_name_str}\n",
    "    \n",
    "    winning_model_name.metadata = winning_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9a629ff-3fb3-4e5a-945a-0b55304ecd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\", \"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"best_model_hp_tuning.yaml\"\n",
    ")\n",
    "\n",
    "def best_model_hp_tuning(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    #bq_source_url: str,\n",
    "    stage_data_bucket: str,\n",
    "    winning_model_name: Input[Artifact],\n",
    "    model_spec: Output[Artifact],\n",
    "    trials: int,\n",
    "    parallel_trials: int,\n",
    "    kpi: Output[Metrics],\n",
    "    model_name: Output[Metrics] \n",
    "): \n",
    "    '''\n",
    "    Tuneo de hiperparametros. Toma el nombre del modelo ganador y utiliza la imagen de Docker correspondiente para lanzar un job de entrenamiento. Los hiperparametros obtenidos son pasados como componentes.\n",
    "    Hyperparameter tuning. Takes the name of the winning model and uses the corresponding Docker image to launch a training job. The chosen hyperparameters are passed as a component.\n",
    "    '''\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "    from google.protobuf.json_format import MessageToDict\n",
    "    import pandas as pd\n",
    "    \n",
    "    aiplatform.init(project = project,\n",
    "                    location = region)\n",
    "    \n",
    "    # train images definition\n",
    "    #XGB_HP_IMAGE = \"gcr.io/vertex-testing-327520/xgb_hp_job:v1\"\n",
    "    RF_HP_IMAGE = \"gcr.io/vertex-testing-327520/rf_hp_job:v1\"\n",
    "    LR_HP_IMAGE = \"gcr.io/vertex-testing-327520/lr_hp_job:v1\"\n",
    "    \n",
    "    # get model name\n",
    "    model_dict = winning_model_name.metadata\n",
    "        \n",
    "    WINNING_MODEL_NAME = model_dict.get('model')\n",
    "    \n",
    "    from datetime import datetime\n",
    "\n",
    "    TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    if WINNING_MODEL_NAME == 'LogisticRegression':\n",
    "        WINNING_MODEL_IMAGE = LR_HP_IMAGE\n",
    "    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n",
    "        WINNING_MODEL_IMAGE = RF_HP_IMAGE\n",
    "    else:\n",
    "        WINNING_MODEL_IMAGE = None \n",
    "    \n",
    "    worker_pool_specs = [{\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": \"n1-standard-8\",\n",
    "        #\"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "        #\"accelerator_count\": 1\n",
    "    },\n",
    "    \"replica_count\": 1,\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": WINNING_MODEL_IMAGE\n",
    "    }\n",
    "    }]\n",
    "    \n",
    "    \n",
    "    metric_spec={'f1_score':'maximize'}\n",
    "\n",
    "    # Dictionary representing parameters to optimize.\n",
    "    # The dictionary key is the parameter_id, which is passed into your training\n",
    "    # job as a command line argument,\n",
    "    # And the dictionary value is the parameter specification of the metric.\n",
    "    \n",
    "    # xgb_parameter_spec = {\n",
    "    #     \"learning_rate\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n",
    "    #     \"max_depth\": hpt.DiscreteParameterSpec(values=[4, 8, 10], scale=None),\n",
    "    #     \"n_estimators\": hpt.DiscreteParameterSpec(values=[5, 7, 9], scale=None)\n",
    "    # }\n",
    "    \n",
    "    lr_parameter_spec = {\n",
    "        \"penalty\": hpt.CategoricalParameterSpec(values=['l1', 'l2']),\n",
    "        \"C\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n",
    "        \"solver\": hpt.CategoricalParameterSpec(values=['saga', 'liblinear'])\n",
    "    }\n",
    "    \n",
    "    rf_parameter_spec = {\n",
    "        \"max_leaf_nodes\": hpt.DiscreteParameterSpec(values=[4, 8, 10], scale=None),\n",
    "        \"max_depth\": hpt.DiscreteParameterSpec(values=[4, 8, 10], scale=None),\n",
    "        \"n_estimators\": hpt.DiscreteParameterSpec(values=[5, 7, 9], scale=None)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if WINNING_MODEL_NAME == 'LogisticRegression':\n",
    "        parameter_spec = lr_parameter_spec\n",
    "    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n",
    "        parameter_spec = rf_parameter_spec\n",
    "    else:\n",
    "        parameter_spec = None \n",
    "    \n",
    "    DISPLAY_NAME = f\"{WINNING_MODEL_NAME}-{TIMESTAMP}\"\n",
    "    \n",
    "    hp_custom_job = aiplatform.CustomJob(display_name=DISPLAY_NAME,\n",
    "                                         worker_pool_specs=worker_pool_specs,\n",
    "                                         staging_bucket=f'gs://{stage_data_bucket}')\n",
    "    \n",
    "    \n",
    "    hp_job = aiplatform.HyperparameterTuningJob(\n",
    "        display_name=DISPLAY_NAME,\n",
    "        custom_job=hp_custom_job,\n",
    "        metric_spec=metric_spec,\n",
    "        parameter_spec=parameter_spec, \n",
    "        max_trial_count=trials,\n",
    "        parallel_trial_count=parallel_trials\n",
    "    )\n",
    "\n",
    "    hp_job.run()\n",
    "    \n",
    "    # helper function\n",
    "    def get_trials_as_df(trials):\n",
    "        results = []\n",
    "        for trial in trials:\n",
    "            row = {}\n",
    "            t = MessageToDict(trial._pb)\n",
    "            # print(t)\n",
    "            row[\"Trial ID\"], row[\"Status\"], row[\"Start time\"], row[\"End time\"] = (\n",
    "                t[\"id\"],\n",
    "                t[\"state\"],\n",
    "                t[\"startTime\"],\n",
    "                t.get(\"endTime\", None),\n",
    "            )\n",
    "\n",
    "            for param in t[\"parameters\"]:\n",
    "                row[param[\"parameterId\"]] = param[\"value\"]\n",
    "\n",
    "            if t[\"state\"] == \"SUCCEEDED\":\n",
    "                row[\"Training step\"] = t[\"finalMeasurement\"][\"stepCount\"]\n",
    "                for metric in t[\"finalMeasurement\"][\"metrics\"]:\n",
    "                    row[metric[\"metricId\"]] = metric[\"value\"]\n",
    "            results.append(row)\n",
    "\n",
    "        _df = pd.DataFrame(results)\n",
    "        return _df\n",
    "    \n",
    "    df_trials = get_trials_as_df(hp_job.trials)\n",
    "    \n",
    "    # get trial id of the best run from the Trials\n",
    "    best_trial_id = df_trials.loc[df_trials[\"f1_score\"].idxmax()][\"Trial ID\"]\n",
    "    # get best run definition\n",
    "    best_run = df_trials[df_trials['Trial ID']==best_trial_id]\n",
    "    \n",
    "    # retrieve parameters tuned in this run\n",
    "    param_names = []\n",
    "\n",
    "    for i in parameter_spec.keys():\n",
    "        param_names.append(i)\n",
    "    \n",
    "    best_run_to_dict = best_run[param_names]\n",
    "    best_run_parameters = best_run_to_dict.to_dict('r')[0]\n",
    "    \n",
    "    model_spec.metadata = best_run_parameters\n",
    "    \n",
    "    kpi_acc = best_run['f1_score'].loc[0]\n",
    "    \n",
    "    kpi.log_metric(\"f1_score\", float(kpi_acc))\n",
    "    model_name.log_metric('model', WINNING_MODEL_NAME)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f2ad71d-a1d8-47f5-ba23-00e1e8833a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenar con lo mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32203d1e-f708-4df2-ba21-ddcb25de495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "        #\"xgboost==1.4\"\n",
    "    ], base_image=\"python:3.9\",\n",
    ")\n",
    "def train_best_model(\n",
    "    dataset_train:  Input[Dataset],\n",
    "    dataset_val: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    parameters: Input[Artifact],\n",
    "    winning_model_name: Input[Artifact],\n",
    "):\n",
    "    \n",
    "    '''\n",
    "    Entrenamiento del modelo seleccionado con los hiperparametros elegidos. Combina la seleccion de algoritmo e hiperparametros, entrena y pasa el pkl como componente.\n",
    "    Training of the chosen model with its hyperparameters. Combines the algorithm selection and training, and passes the pkl as a component.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #from xgboost import XGBClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    \n",
    "    # get model name and parameters \n",
    "    \n",
    "    best_parameters = parameters.metadata\n",
    "        \n",
    "    model_dict = winning_model_name.metadata\n",
    "    WINNING_MODEL_NAME = model_dict.get('model')\n",
    "    \n",
    "    # choose model and place parameters\n",
    "    if WINNING_MODEL_NAME == 'LogisticRegression':\n",
    "        best_model = LogisticRegression(**best_parameters)\n",
    "    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n",
    "        best_model = RandomForestClassifier(**best_parameters)\n",
    "    else:\n",
    "        best_model = None \n",
    "        \n",
    "    # get data \n",
    "\n",
    "    data_train = pd.read_csv(dataset_train.path+\".csv\")\n",
    "    data_val = pd.read_csv(dataset_val.path+\".csv\")\n",
    "    \n",
    "    data = pd.concat([data_train, data_val])\n",
    "    \n",
    "    # train\n",
    "    best_model.fit(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "    model.metadata[\"framework\"] = WINNING_MODEL_NAME\n",
    "    \n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(best_model, file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73b3e94b-0243-4f55-9c06-8be226f9ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best_model_op = create_custom_training_job_op_from_component(\n",
    "    train_best_model,\n",
    "    machine_type='n1-standard-16'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5b0d658-2e3b-4c1f-8bec-f21cd274cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "        #\"xgboost==1.4\"\n",
    "    ], base_image=\"python:3.9\",\n",
    ")\n",
    "def best_model_evaluation(\n",
    "    test_set:  Input[Dataset],\n",
    "    winning_model_name: Input[Artifact], # tiene que saber qu칠 objeto instanciar adentro\n",
    "    best_model: Input[Model], # y ac치 tomar los datos para cargarlo\n",
    "    best_model_kpi: Output[Metrics],\n",
    "    threshold: float\n",
    ")-> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):\n",
    "    \n",
    "    '''\n",
    "    Toma el mejor modelo entrenado y lo evalua usando el set de test. Si pasa un cierto umbral, devuelve \"true\" y marca el inicio del proximo paso, si no lo hace, el proceso se detiene.\n",
    "    Takes the trained best model and evaluates it using the test set. If it passes a certain threshold, it returns \"true\" and sets the beginning of the next step, if it doesn't, the process halts.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    #from xgboost import XGBClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import pickle\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, f1_score\n",
    "    import json\n",
    "    import typing\n",
    "    \n",
    "    model_dict = winning_model_name.metadata\n",
    "    WINNING_MODEL_NAME = model_dict.get('model')\n",
    "    \n",
    "    #TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    if WINNING_MODEL_NAME == 'LogisticRegression':\n",
    "        model = LogisticRegression()\n",
    "    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = None \n",
    "\n",
    "    \n",
    "    file_name = best_model.path + \".pkl\"\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        model = pickle.load(file)\n",
    "        \n",
    "    data = pd.read_csv(test_set.path+\".csv\")\n",
    "    y_test = data.drop(columns=[\"target\"])\n",
    "    y_target=data.target\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(y_test)\n",
    "    \n",
    "\n",
    "  \n",
    "    # evaluacion de modelo \n",
    "    f1_value = f1_score(data.target, y_pred.round())\n",
    "    \n",
    "    # toma decision\n",
    "    \n",
    "    if f1_value >= threshold:\n",
    "        dep_decision = 'true'\n",
    "    else:\n",
    "        dep_decision = 'false'\n",
    "    \n",
    "    # guarda la metrica\n",
    "    best_model_kpi.log_metric(\"f1_score\", float(f1_value))\n",
    "    \n",
    "    return (dep_decision, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfda9e4e-f9bb-4550-8c7c-e284463d4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\", \"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    #output_component_file=\"best_model_hp_tuning.yaml\"\n",
    ")\n",
    "def upload_model_to_vertex_and_batch_prediction(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    serving_container: str,\n",
    "    trained_model: Input[Model],\n",
    "    winning_model_name: Input[Artifact],\n",
    "    gcs_predict_source: str,\n",
    "    gcs_predict_dest: str\n",
    "\n",
    "):\n",
    "    '''\n",
    "    Toma el mejor modelo entrenado en formato pkl y lo convierte en un Vertex Managed Model a partir del cual se realizan las predicciones en formato batch.\n",
    "    Takes the trained best model in pkl format and uploads it to a Vertex Managed Model and uses it to do a batch prediction job.\n",
    "    '''\n",
    "    \n",
    "    from typing import Dict, Optional, Sequence\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    from datetime import datetime\n",
    "    \n",
    "    model_dict = winning_model_name.metadata\n",
    "    WINNING_MODEL_NAME = model_dict.get('model')\n",
    "    \n",
    "    TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    DISPLAY_NAME = WINNING_MODEL_NAME +'-' + TIMESTAMP\n",
    "    \n",
    "    MODEL_URI = trained_model.uri\n",
    "    #MODEL_PATH = f'{MODEL_URI}.pkl'\n",
    "    MODEL_PATH = MODEL_URI[:-5] # peque침o hack para que encuentre el directorio con el modelo\n",
    "    \n",
    "    def upload_model_sample(\n",
    "        project: str,\n",
    "        location: str,\n",
    "        display_name: str,\n",
    "        serving_container_image_uri: str,\n",
    "        artifact_uri: Optional[str] = None,\n",
    "        sync: bool = True,\n",
    "    ):\n",
    "        \n",
    "\n",
    "        aiplatform.init(project=project, location=location)\n",
    "\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name=display_name,\n",
    "            artifact_uri=artifact_uri,\n",
    "            serving_container_image_uri=serving_container,\n",
    "            sync=sync,\n",
    "        )\n",
    "\n",
    "        model.wait()\n",
    "\n",
    "        print(model.display_name)\n",
    "        print(model.resource_name)\n",
    "        return model\n",
    "    \n",
    "    model_test = upload_model_sample(\n",
    "        project = project,\n",
    "        location = region,\n",
    "        display_name = DISPLAY_NAME,\n",
    "        serving_container_image_uri= serving_container,\n",
    "        artifact_uri = MODEL_PATH\n",
    "    )\n",
    "    \n",
    "    batch_job = model_test.batch_predict(\n",
    "        job_display_name=DISPLAY_NAME,\n",
    "        gcs_source = gcs_predict_source,\n",
    "        instances_format=\"csv\",\n",
    "        gcs_destination_prefix=gcs_predict_dest,\n",
    "        machine_type = 'n1-standard-16'\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a7951b-6d7e-48ca-8934-33ba5bf25fd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Definicion de componentes - Component definition\n",
    "\n",
    "Hay una carpeta llamada *components* donde se encuentra una notebook que genera los distintos componentes necesarios para el pipeline y sus correspondientes yaml. Dada las caracter칤sticas de este pipeline, muchos componentes requieren armarse de manera customizada y no depender de los preconstruidos.\n",
    "\n",
    "There's a folder named *components* with a notebook that generates the needed components for the pipeline and their corresponding yaml files. Given the complexity of this pipeline, many components needs to be custom built and not use the pre-built one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34607a-8023-4d5d-a187-8afc438bfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_evaluation = kfp.components.load_component_from_file('./components/best_model_evaluation.yaml')\n",
    "best_model_hp_tuning = kfp.components.load_component_from_file('./components/best_model_hp_tuning.yaml')\n",
    "bq_current_raw_to_stage = kfp.components.load_component_from_file('./components/bq_current_raw_to_stage.yaml')\n",
    "bq_historic_raw_to_stage = kfp.components.load_component_from_file('./components/bq_historic_raw_to_stage.yaml')\n",
    "get_chicago_data = kfp.components.load_component_from_file('./components/get_chicago_data.yaml')\n",
    "model_evaluation = kfp.components.load_component_from_file('./components/model_evaluation.yaml')\n",
    "train_best_model = kfp.components.load_component_from_file('./components/train_best_model.yaml')\n",
    "train_lr_chicago = kfp.components.load_component_from_file('./components/train_lr_chicago.yaml')\n",
    "train_rf_chicago = kfp.components.load_component_from_file('./components/train_rf_chicago.yaml')\n",
    "upload_model_to_vertex_and_batch_prediction = kfp.components.load_component_from_file('./components/upload_model_to_vertex_and_batch_prediction.yaml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171bc804-9fdf-4410-ac11-2abbb5ed0f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba882edb-d1c2-4975-b927-7eeb3a824907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95815fd1-9127-4c4a-bb90-b2b61d51dc09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7634168-a538-4ef9-b482-ad273beae09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcb5cb-44ee-4c9c-8640-0bd6c63d413e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3ad96-639f-4a53-98fc-5bab773eb8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b64eccf5-9ba4-4549-aa50-7d2f671affb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pipeline mio\n",
    "@dsl.pipeline(name='xgb-chicago-parte2',\n",
    "                pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    #bq_source_url: str = BQ_SOURCE_URL,\n",
    "    kfp_artifacts_bucket: str = KFP_ARTIFACTS_BUCKET,\n",
    "    stage_data_bucket: str = STAGE_DATA_BUCKET,\n",
    "    pipelines_bucket: str = PIPELINE_BUCKET,\n",
    "    pipeline_root: str = PIPELINE_ROOT,\n",
    "    #train_container: str = TRAIN_CONTAINER,\n",
    "    serving_container: str = SERVING_CONTAINER,\n",
    "    machine_type: str = MACHINE_TYPE,\n",
    "    trials: int = HP_TRIALS,\n",
    "    parallel_trials: int = PARALLEL_TRIALS,\n",
    "    bq_current_raw_url: str = BQ_CURRENT_RAW_URL,\n",
    "    bq_current_stage_url: str = BQ_CURRENT_STAGE_URL,\n",
    "    bq_historic_raw_url: str = BQ_HISTORIC_RAW_URL,\n",
    "    bq_historic_stage_url: str = BQ_HISTORIC_STAGE_URL,\n",
    "    threshold: float = THRESHOLD,\n",
    "    bq_predict_source: str = BQ_PREDICT_SOURCE,\n",
    "    bq_predict_dest: str = BQ_PREDICT_DEST\n",
    "    \n",
    "    \n",
    "):\n",
    "    \n",
    "    bq_stage_ml = bq_historic_raw_to_stage_ml(\n",
    "        project = project_id,\n",
    "        region = gcp_region,\n",
    "        bq_historic_raw_url = bq_historic_raw_url,\n",
    "        bq_historic_stage_url = bq_historic_stage_url\n",
    "        \n",
    "    )\n",
    "    \n",
    "    bq_current_ml = bq_current_raw_to_stage_ml(\n",
    "        project = project_id,\n",
    "        region = gcp_region,\n",
    "        bq_current_raw_url = bq_current_raw_url,\n",
    "        bq_current_stage_url = bq_current_stage_url,\n",
    "        stage_data_bucket = stage_data_bucket\n",
    "        \n",
    "    )\n",
    "    \n",
    "    dataframe = get_chicago_data(project = project_id,\n",
    "                                 region = gcp_region,\n",
    "                                 bq_source_url = bq_stage_ml.output,\n",
    "                                 stage_data_bucket = stage_data_bucket)\n",
    "    \n",
    "    # train_xgb_op = train_xgb_chicago_op(dataframe.outputs['dataset_train'],\n",
    "    #                                      project = project_id,\n",
    "    #                                      location = gcp_region)\n",
    "    \n",
    "    train_lr_op = train_lr_chicago_op(dataframe.outputs['dataset_train'],\n",
    "                                         project = project_id,\n",
    "                                         location = gcp_region)\n",
    "    \n",
    "    \n",
    "    train_rf_op = train_rf_chicago_op(dataframe.outputs['dataset_train'],\n",
    "                                         project = project_id,\n",
    "                                         location = gcp_region)\n",
    "    \n",
    "    model_selection = model_evaluation(\n",
    "        val_set = dataframe.outputs['dataset_val'],\n",
    "        # xgb_chicago_model = train_xgb_op.outputs['model'],\n",
    "        lr_chicago_model = train_lr_op.outputs['model'],\n",
    "        rf_chicago_model = train_rf_op.outputs['model'],\n",
    "    \n",
    "    )\n",
    "    \n",
    "    hp_search = best_model_hp_tuning(\n",
    "        project = project_id,\n",
    "        region = gcp_region,\n",
    "        stage_data_bucket = stage_data_bucket,\n",
    "        winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "        trials = trials,\n",
    "        parallel_trials = parallel_trials\n",
    "    )\n",
    "    \n",
    "    best_model = train_best_model_op(\n",
    "        dataset_train = dataframe.outputs['dataset_train'], \n",
    "        dataset_val = dataframe.outputs['dataset_val'],\n",
    "        project = project_id,\n",
    "        location = gcp_region,\n",
    "        winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "        parameters = hp_search.outputs['model_spec']\n",
    "    )\n",
    "    \n",
    "    best_model_eval_decision = best_model_evaluation(\n",
    "        test_set = dataframe.outputs['dataset_test'],\n",
    "        winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "        best_model = best_model.outputs['model'],\n",
    "        threshold = threshold\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        best_model_eval_decision.outputs['dep_decision']=='true',\n",
    "        name = 'predict_decision'\n",
    "    ):\n",
    "        predict_op = upload_model_to_vertex_and_batch_prediction(\n",
    "            project = project_id,\n",
    "            region = gcp_region,\n",
    "            serving_container = serving_container,\n",
    "            trained_model = best_model.outputs['model'],\n",
    "            winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "            gcs_predict_source = bq_current_ml.outputs['gcs_predict_source'],\n",
    "            gcs_predict_dest = f'gs://{stage_data_bucket}'\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "342fef71-d871-4079-a3ce-a9735b9c3084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='xgb-chicago-parte2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcc7816f-1926-40a7-b8aa-5a832a752a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"xgb-chicago-displaynamepipejob\",\n",
    "    template_path=\"xgb-chicago-parte2.json\",\n",
    "    job_id=\"xgb-chicago-parte2-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6da4bb65-6c7f-47c1-82c5-83a1cdcd52bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/155345500736/locations/us-central1/pipelineJobs/xgb-chicago-parte2-20220209212647\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/155345500736/locations/us-central1/pipelineJobs/xgb-chicago-parte2-20220209212647')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/xgb-chicago-parte2-20220209212647?project=155345500736\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25013d48-2faa-4635-bed4-9025c8389b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339433bc-0f36-4d1e-986a-0dd9592d3bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3554d55-130e-4595-9141-7947f245197a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672d109-6c73-4d2b-8bbd-90edac2e4ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a167db-1a5a-4831-92a4-bc25c6ba34af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c56d504-5ced-4e6f-9321-e6eedbce7576",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1436/1442444758.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m def upload_model_to_vertex(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mproject\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m ):\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hola'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'project' is not defined"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\", \"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def upload_model_to_vertex(\n",
    "    project: project\n",
    "):\n",
    "    print('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69b1a1cd-e276-4669-8e01-f6f83148f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_model_sample(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    display_name: str,\n",
    "    serving_container_image_uri: str,\n",
    "    artifact_uri: Optional[str] = None,\n",
    "    serving_container_predict_route: Optional[str] = None,\n",
    "    serving_container_health_route: Optional[str] = None,\n",
    "    description: Optional[str] = None,\n",
    "    serving_container_command: Optional[Sequence[str]] = None,\n",
    "    serving_container_args: Optional[Sequence[str]] = None,\n",
    "    serving_container_environment_variables: Optional[Dict[str, str]] = None,\n",
    "    serving_container_ports: Optional[Sequence[int]] = None,\n",
    "    instance_schema_uri: Optional[str] = None,\n",
    "    parameters_schema_uri: Optional[str] = None,\n",
    "    prediction_schema_uri: Optional[str] = None,\n",
    "    explanation_metadata: Optional[explain.ExplanationMetadata] = None,\n",
    "    explanation_parameters: Optional[explain.ExplanationParameters] = None,\n",
    "    sync: bool = True,\n",
    "):\n",
    "\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=artifact_uri,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        serving_container_predict_route=serving_container_predict_route,\n",
    "        serving_container_health_route=serving_container_health_route,\n",
    "        instance_schema_uri=instance_schema_uri,\n",
    "        parameters_schema_uri=parameters_schema_uri,\n",
    "        prediction_schema_uri=prediction_schema_uri,\n",
    "        description=description,\n",
    "        serving_container_command=serving_container_command,\n",
    "        serving_container_args=serving_container_args,\n",
    "        serving_container_environment_variables=serving_container_environment_variables,\n",
    "        serving_container_ports=serving_container_ports,\n",
    "        explanation_metadata=explanation_metadata,\n",
    "        explanation_parameters=explanation_parameters,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    model.wait()\n",
    "\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ffe99dab-375e-46f3-bfcd-0d3f5741232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_CONTAINER = 'us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-4:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d30639-93a1-44a5-9a95-3167ede7b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"There are no files in directory \"gs://chicago_taxi_pipelines/pipeline_root/155345500736/xgb-chicago-parte2-20220208180406/train-best-model_4522557406856609792/model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f2c102-b888-4dda-a2b4-9a7d582cb73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs://chicago_taxi_pipelines/pipeline_root/155345500736/xgb-chicago-parte2-20220208180406/train-best-model_4522557406856609792/model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99bdeec8-b3bc-4642-8cea-c63c68ab466f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/155345500736/locations/us-central1/models/205023734208135168/operations/2223916817280139264\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/155345500736/locations/us-central1/models/205023734208135168\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/155345500736/locations/us-central1/models/205023734208135168')\n",
      "prueba-uploaded-model\n",
      "projects/155345500736/locations/us-central1/models/205023734208135168\n"
     ]
    }
   ],
   "source": [
    "model_test = upload_model_sample(\n",
    "    project = PROJECT_ID,\n",
    "    location = REGION,\n",
    "    display_name = 'prueba-uploaded-model',\n",
    "    serving_container_image_uri= SERVING_CONTAINER,\n",
    "    artifact_uri = MODEL_URI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "282f24d1-6a74-4997-b261-af895efba14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google.cloud.aiplatform.models.Model"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81a254a2-85dc-4f3c-bfa2-60756fde1be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEX_DATASET_SOURCE = f'gs://{STAGE_DATA_BUCKET}/{TEST_DATA_PATH}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6058fb95-35b9-48af-9a28-bb46b6827bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicago_taxi_pipelines'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a9abfeda-7a0a-4d85-a6b1-a65006ee7139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://chicago_taxi_pipelines'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DESTINATION = f'gs://{PIPELINE_BUCKET}'\n",
    "DESTINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33d506be-7495-44bc-8d26-e9c24addf2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://chicago_taxi_stage/chicago_taxi_test.csv'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VERTEX_DATASET_SOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51a6c87c-283b-406c-a1a8-91db109fe9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056\n",
      "INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056')\n",
      "INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/6873631025902125056?project=155345500736\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob run completed. Resource name: projects/155345500736/locations/us-central1/batchPredictionJobs/6873631025902125056\n"
     ]
    }
   ],
   "source": [
    "batch_job = model_test.batch_predict(\n",
    "    job_display_name=\"prediction-123\",\n",
    "    gcs_source=VERTEX_DATASET_SOURCE,\n",
    "    instances_format=\"csv\",\n",
    "    #bigquery_destination_prefix=\"projectId.bqDatasetId.bqTableId\",\n",
    "    gcs_destination_prefix=DESTINATION,\n",
    "    machine_type = 'n1-standard-8'\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "148e4af3-a423-4aca-ad4f-664d3df2ed68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6873631025902125056'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e576d03-d0cd-44ce-b3bc-11e12f69b9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a99e4-d874-47e2-8007-e70d45d50b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fdb24b-fa84-46b9-a614-592c15a43515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273d53b-e2ea-4903-a4c6-2a07dad04adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36221bbd-36cd-4939-8fd0-2d96ae0b7c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133756e-ec2f-493e-9ca3-0ed8a8caa198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d653ab41-48a5-460c-9133-dc1f04f58953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bb4a5cd-8f43-4ea1-86c7-23ab1faaa35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aip.Model(\n",
    "    display_name = 'esto-es-una-prueba',\n",
    "    artifact_uri = MODEL_URI,\n",
    "    container_spec={\"image_uri\": SERVING_CONTAINER}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d875151c-0014-4d1b-a293-e69778d7a331",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 Invalid resource field value in the request.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Invalid resource field value in the request.\"\n\tdebug_error_string = \"{\"created\":\"@1644347190.184158835\",\"description\":\"Error received from peer ipv4:173.194.202.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1063,\"grpc_message\":\"Invalid resource field value in the request.\",\"grpc_status\":3}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1436/79252185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPELINE_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform_v1/services/model_service/client.py\u001b[0m in \u001b[0;36mupload_model\u001b[0;34m(self, request, parent, model, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;31m# Wrap the response in an operation future.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 Invalid resource field value in the request."
     ]
    }
   ],
   "source": [
    "response = clients.upload_model(parent=PIPELINE_ROOT, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cbcb1-b070-42bc-b595-49258bb04c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50088a6-ec1f-49a7-948d-86e03b38fed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501f497-398f-4963-8dc7-36f824b16a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9fba70-9773-4d3a-8cd6-884b036ac38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd4fb4-e6ce-4243-a112-1be0462dbda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de3cb1-f515-4da3-978b-59ee217e26b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aip.Model(\n",
    "            display_name = 'esto-es-una-prueba',\n",
    "            artfifact_uri = ARTIFACT_URI,\n",
    "            container_spec = serving_container\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b850be3-ee66-4f8d-a5df-925304cebf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\", \"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    #output_component_file=\"best_model_hp_tuning.yaml\"\n",
    ")\n",
    "\n",
    "def upload_model_to_vertex(\n",
    "    trained_model: Input[Model],\n",
    "    serving_container: str\n",
    "\n",
    ")-> str:\n",
    "    def upload_model(\n",
    "        DISPLAY_NAME = 'chicago-model',\n",
    "        MODEL_PATH = trained_model.path,\n",
    "        ARTIFACT_URI = MODEL_PATH + '.pkl'):\n",
    "    \n",
    "        model = aip.Model(\n",
    "            display_name = DISPLAY_NAME,\n",
    "            artfifact_uri = ARTIFACT_URI,\n",
    "            container_spec = serving_container\n",
    "        )\n",
    "    \n",
    "        response = clients[\"model\"].upload_model(parent=PARENT, model=model)\n",
    "        print(\"Long running operation:\", response.operation.name)\n",
    "        upload_model_response = response.result(timeout=180)\n",
    "        print(\"upload_model_response\")\n",
    "        print(\" model:\", upload_model_response.model)\n",
    "        return upload_model_response.model\n",
    "    \n",
    "    \n",
    "    model_to_deploy_id = upload_model(\n",
    "        \"boston-\" + TIMESTAMP, IMAGE_URI, model_path_to_deploy\n",
    "    )\n",
    "    \n",
    "    return model_to_deploy_id\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5abea55d-546e-4415-910f-47e81cda5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform_v1 as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1780428-6e91-4e8d-93b4-14b347f92f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google.cloud.aiplatform_v1.types.model.Model"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aip.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaecb3c0-268d-4693-8fcc-0a51eb6baf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_display_name = \"e2e-text-classification-batch-prediction-job-scale\"\n",
    "model = aiplatform.Model(model_name=model_name)\n",
    "\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    job_display_name=job_display_name,\n",
    "    gcs_source=f\"{BUCKET_URI}/{input_file_name}\",\n",
    "    gcs_destination_prefix=f\"{BUCKET_URI}/output\",\n",
    "    sync=True,\n",
    "    machine_type='n1-standard-16', #define machine type\n",
    "    starting_replica_count=1, \n",
    "    max_replica_count=1, # set max_replica_count > starting_replica_count to enable scaling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "575f7414-81bb-43fa-8ee2-bf32a226031b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_chicago_data = kfp.components.load_component_from_file(GET_CHICAGO_DATA_YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a38d3ab7-6eed-4070-aba7-41453b6881da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/explainable_ai/gapic-custom_tabular_regression_batch_explain.ipynb\n",
    "# https://stackoverflow.com/questions/69823351/googles-notebook-on-vertex-ai-throwing-following-error-type-name-google-vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d19ba56-7364-4b53-95d4-634cfd6b7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d31c0-6bf9-4129-9f6b-148705fe966e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3221fd0-b023-42b5-8eee-b821f5c62a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"xgb-chicago-displaynamepipejob\",\n",
    "    template_path=\"xgb-chicago-parte2.json\",\n",
    "    job_id=\"xgb-chicago-parte2-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56974053-8c3f-406f-b698-55708f9587d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/155345500736/locations/us-central1/pipelineJobs/xgb-chicago-parte2-20220208183006\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/155345500736/locations/us-central1/pipelineJobs/xgb-chicago-parte2-20220208183006')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/xgb-chicago-parte2-20220208183006?project=155345500736\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c4feb-975b-4bbe-851e-fab816d370fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creacion de las imagenes de los modelos para hpt\n",
    "# PROJECT_ID=$(gcloud config get-value project)\n",
    "# IMAGE_URI=\"gcr.io/$PROJECT_ID/xgb_hp_job:v1\"\n",
    "# docker build ./ -t $IMAGE_URI\n",
    "# docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3a481-d915-422a-9f8e-49354031e654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deaed2d-4e94-407d-9e65-00cb94f2fc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45027c14-c08a-4fe0-a537-ccfe9e7026d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47c12e-b4b6-4dca-8f73-1338cf46237d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b4d90-e56f-4156-88e7-805146aa9e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b583629-1ed8-45b7-a333-cd5c26d98449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24748c30-e01e-4777-aa38-8bfa3f341dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47091c-ab76-4e17-8869-536816d0f7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d76e5d-951c-4ac4-be1a-9c3a4ae3d69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258064f-65c6-484e-924c-4bdb3b63999f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb74c72-f698-45d7-b0a4-af3d417b84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta es una prueba standalone del de xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b4572e89-2073-40cf-94f6-1affc9ea2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_hp_tuning(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    bq_source_url: str,\n",
    "    stage_data_bucket: str,\n",
    "    dataset: Output[Dataset]\n",
    "    #dataset_train: OutputPath[Dataset],\n",
    "    #dataset_test: OutputPath[Dataset] \n",
    ")-> str: # cuando termine, va a largar un texto\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "    from google.protobuf.json_format import MessageToDict\n",
    "    import pandas as pd\n",
    "    \n",
    "    aiplatform.init(project = project,\n",
    "                    location = region)\n",
    "    \n",
    "    worker_pool_specs = [{\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": \"n1-standard-8\",\n",
    "        #\"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "        #\"accelerator_count\": 1\n",
    "    },\n",
    "    \"replica_count\": 1,\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": \"gcr.io/vertex-testing-327520/xgb_hp_job:v1\"\n",
    "    }\n",
    "    }]\n",
    "    \n",
    "    # Dicionary representing metrics to optimize.\n",
    "    # The dictionary key is the metric_id, which is reported by your training job,\n",
    "    # And the dictionary value is the optimization goal of the metric.\n",
    "    metric_spec={'accuracy':'maximize'}\n",
    "\n",
    "    # Dictionary representing parameters to optimize.\n",
    "    # The dictionary key is the parameter_id, which is passed into your training\n",
    "    # job as a command line argument,\n",
    "    # And the dictionary value is the parameter specification of the metric.\n",
    "    parameter_spec = {\n",
    "        \"learning_rate\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n",
    "        \"max_depth\": hpt.DiscreteParameterSpec(values=[4, 8, 10], scale=None),\n",
    "        \"n_estimators\": hpt.DiscreteParameterSpec(values=[5, 7, 9], scale=None)\n",
    "    }\n",
    "    \n",
    "    hp_custom_job = aiplatform.CustomJob(display_name='xgb-chicago-job-test',\n",
    "                                         worker_pool_specs=worker_pool_specs,\n",
    "                                         staging_bucket=f'gs://{STAGE_DATA_BUCKET}')\n",
    "    \n",
    "    hp_job = aiplatform.HyperparameterTuningJob(\n",
    "        display_name='xgb-chicago-test',\n",
    "        custom_job=my_custom_job,\n",
    "        metric_spec=metric_spec,\n",
    "        parameter_spec=parameter_spec,\n",
    "        max_trial_count=10,\n",
    "        parallel_trial_count=3\n",
    "    )\n",
    "\n",
    "    hp_job.run()\n",
    "    \n",
    "    # helper function\n",
    "    def get_trials_as_df(trials):\n",
    "        results = []\n",
    "        for trial in trials:\n",
    "            row = {}\n",
    "            t = MessageToDict(trial._pb)\n",
    "            # print(t)\n",
    "            row[\"Trial ID\"], row[\"Status\"], row[\"Start time\"], row[\"End time\"] = (\n",
    "                t[\"id\"],\n",
    "                t[\"state\"],\n",
    "                t[\"startTime\"],\n",
    "                t.get(\"endTime\", None),\n",
    "            )\n",
    "\n",
    "            for param in t[\"parameters\"]:\n",
    "                row[param[\"parameterId\"]] = param[\"value\"]\n",
    "\n",
    "            if t[\"state\"] == \"SUCCEEDED\":\n",
    "                row[\"Training step\"] = t[\"finalMeasurement\"][\"stepCount\"]\n",
    "                for metric in t[\"finalMeasurement\"][\"metrics\"]:\n",
    "                    row[metric[\"metricId\"]] = metric[\"value\"]\n",
    "            results.append(row)\n",
    "\n",
    "        _df = pd.DataFrame(results)\n",
    "        return _df\n",
    "    \n",
    "    df_trials = get_trials_as_df(hp_job.trials)\n",
    "    \n",
    "    # get trial id of the best run from the Trials\n",
    "    best_trial_id = df_trials.loc[df_trials[\"accuracy\"].idxmax()][\"Trial ID\"]\n",
    "    # get best run definition\n",
    "    best_run = df_trials[df_trials['Trial ID']==best_trial_id]\n",
    "    \n",
    "    # retrieve parameters tuned in this run\n",
    "    param_names = []\n",
    "\n",
    "    for i in parameter_spec.keys():\n",
    "        param_names.append(i)\n",
    "    \n",
    "    best_run_to_dict = best_run[param_names]\n",
    "    best_run_parameters = best_run_to_dict.to_dict('r')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd0c73f-3ba8-4928-9ca2-c80db551c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c50431-b6e3-475c-83ae-6620c3b2682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcr.io/vertex-testing-327520/xgb_hp_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09e6c00-0692-4c77-ba2b-cf0b5ea25e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The spec of the worker pools including machine type and Docker image\n",
    "# Be sure to replace IMAGE_URI with the path to your Docker image in GCR\n",
    "worker_pool_specs = [{\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": \"n1-standard-8\",\n",
    "        #\"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "        #\"accelerator_count\": 1\n",
    "    },\n",
    "    \"replica_count\": 1,\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": \"gcr.io/vertex-testing-327520/xgb_hp_job:v1\"\n",
    "    }\n",
    "}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc32bbb-ace5-4992-a3e9-9e0c4e736f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionary representing metrics to optimize.\n",
    "# The dictionary key is the metric_id, which is reported by your training job,\n",
    "# And the dictionary value is the optimization goal of the metric.\n",
    "metric_spec={'accuracy':'maximize'}\n",
    "\n",
    "# Dictionary representing parameters to optimize.\n",
    "# The dictionary key is the parameter_id, which is passed into your training\n",
    "# job as a command line argument,\n",
    "# And the dictionary value is the parameter specification of the metric.\n",
    "parameter_spec = {\n",
    "    \"learning_rate\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n",
    "    \"max_depth\": hpt.DiscreteParameterSpec(values=[4, 8, 10], scale=None),\n",
    "    \"n_estimators\": hpt.DiscreteParameterSpec(values=[5, 7, 9], scale=None)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a88973d6-d172-4032-81ba-ea5c0aef6530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b15dfab-afa6-474b-99ec-fd72445bc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_job = aiplatform.CustomJob(display_name='xgb-chicago-job-test',\n",
    "                              worker_pool_specs=worker_pool_specs,\n",
    "                              staging_bucket=f'gs://{STAGE_DATA_BUCKET}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf0d227-a211-4095-8338-a948fb425ccc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating HyperparameterTuningJob\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob created. Resource name: projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184\n",
      "INFO:google.cloud.aiplatform.jobs:To use this HyperparameterTuningJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:hpt_job = aiplatform.HyperparameterTuningJob.get('projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184')\n",
      "INFO:google.cloud.aiplatform.jobs:View HyperparameterTuningJob:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6563489582032093184?project=155345500736\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob run completed. Resource name: projects/155345500736/locations/us-central1/hyperparameterTuningJobs/6563489582032093184\n"
     ]
    }
   ],
   "source": [
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name='xgb-chicago-test',\n",
    "    custom_job=my_custom_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=10,\n",
    "    parallel_trial_count=3)\n",
    "\n",
    "hp_job.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ee5553ad-bbe8-4691-9c52-d37d378b1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59f0a952-8bf5-4f59-aa43-99b954d426b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2390bb0-27cc-440b-97b0-48bda6c6416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials_as_df(trials):\n",
    "    results = []\n",
    "    for trial in trials:\n",
    "        row = {}\n",
    "        t = MessageToDict(trial._pb)\n",
    "        # print(t)\n",
    "        row[\"Trial ID\"], row[\"Status\"], row[\"Start time\"], row[\"End time\"] = (\n",
    "            t[\"id\"],\n",
    "            t[\"state\"],\n",
    "            t[\"startTime\"],\n",
    "            t.get(\"endTime\", None),\n",
    "        )\n",
    "\n",
    "        for param in t[\"parameters\"]:\n",
    "            row[param[\"parameterId\"]] = param[\"value\"]\n",
    "\n",
    "        if t[\"state\"] == \"SUCCEEDED\":\n",
    "            row[\"Training step\"] = t[\"finalMeasurement\"][\"stepCount\"]\n",
    "            for metric in t[\"finalMeasurement\"][\"metrics\"]:\n",
    "                row[metric[\"metricId\"]] = metric[\"value\"]\n",
    "        results.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(results)\n",
    "    return _df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33c20058-a209-459e-8305-7457a07f28db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial ID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Start time</th>\n",
       "      <th>End time</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>Training step</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:48:14.003323963Z</td>\n",
       "      <td>2022-02-05T17:50:29Z</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:48:14.003442748Z</td>\n",
       "      <td>2022-02-05T17:50:36Z</td>\n",
       "      <td>0.139284</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.884595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:48:14.003484782Z</td>\n",
       "      <td>2022-02-05T17:50:22Z</td>\n",
       "      <td>0.006533</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:57:27.095152733Z</td>\n",
       "      <td>2022-02-05T17:59:46Z</td>\n",
       "      <td>0.239254</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:57:27.095278834Z</td>\n",
       "      <td>2022-02-05T18:01:11Z</td>\n",
       "      <td>0.097384</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:57:27.095317545Z</td>\n",
       "      <td>2022-02-05T18:01:12Z</td>\n",
       "      <td>0.086835</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T18:06:35.649231816Z</td>\n",
       "      <td>2022-02-05T18:09:13Z</td>\n",
       "      <td>0.235194</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T18:08:16.823511781Z</td>\n",
       "      <td>2022-02-05T18:10:53Z</td>\n",
       "      <td>0.152319</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T18:08:16.823638476Z</td>\n",
       "      <td>2022-02-05T18:12:12Z</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T18:15:47.603375493Z</td>\n",
       "      <td>2022-02-05T18:17:56Z</td>\n",
       "      <td>0.126417</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Trial ID     Status                      Start time              End time  \\\n",
       "0        1  SUCCEEDED  2022-02-05T17:48:14.003323963Z  2022-02-05T17:50:29Z   \n",
       "1        2  SUCCEEDED  2022-02-05T17:48:14.003442748Z  2022-02-05T17:50:36Z   \n",
       "2        3  SUCCEEDED  2022-02-05T17:48:14.003484782Z  2022-02-05T17:50:22Z   \n",
       "3        4  SUCCEEDED  2022-02-05T17:57:27.095152733Z  2022-02-05T17:59:46Z   \n",
       "4        5  SUCCEEDED  2022-02-05T17:57:27.095278834Z  2022-02-05T18:01:11Z   \n",
       "5        6  SUCCEEDED  2022-02-05T17:57:27.095317545Z  2022-02-05T18:01:12Z   \n",
       "6        7  SUCCEEDED  2022-02-05T18:06:35.649231816Z  2022-02-05T18:09:13Z   \n",
       "7        8  SUCCEEDED  2022-02-05T18:08:16.823511781Z  2022-02-05T18:10:53Z   \n",
       "8        9  SUCCEEDED  2022-02-05T18:08:16.823638476Z  2022-02-05T18:12:12Z   \n",
       "9       10  SUCCEEDED  2022-02-05T18:15:47.603375493Z  2022-02-05T18:17:56Z   \n",
       "\n",
       "   learning_rate  max_depth  n_estimators Training step  accuracy  \n",
       "0       0.031623        8.0           7.0             1  0.880762  \n",
       "1       0.139284        8.0           5.0             1  0.884595  \n",
       "2       0.006533        8.0           5.0             1  0.881333  \n",
       "3       0.239254       10.0           5.0             1  0.883024  \n",
       "4       0.097384        4.0           5.0             1  0.880357  \n",
       "5       0.086835       10.0           5.0             1  0.879310  \n",
       "6       0.235194        4.0           5.0             1  0.879786  \n",
       "7       0.152319       10.0           5.0             1  0.881571  \n",
       "8       0.003471       10.0           9.0             1  0.876643  \n",
       "9       0.126417        4.0           7.0             1  0.880714  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.protobuf.json_format import MessageToDict\n",
    "import pandas as pd\n",
    "df_trials = get_trials_as_df(hp_job.trials)\n",
    "df_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d317594d-2fcb-4ecd-8059-d69405fa0bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1337/3951390240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhp_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "hp_job.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11c5d245-cd62-4a8d-94e2-fad9f023ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trial id of the best run from the Trials\n",
    "best_trial_id = df_trials.loc[df_trials[\"accuracy\"].idxmax()][\"Trial ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a4045d94-a206-4711-a501-8698b6e6e6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1fa9c85-6788-41b0-a37e-511952a561ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial ID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Start time</th>\n",
       "      <th>End time</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>Training step</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:48:14.003442748Z</td>\n",
       "      <td>2022-02-05T17:50:36Z</td>\n",
       "      <td>0.139284</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.884595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Trial ID     Status                      Start time              End time  \\\n",
       "1        2  SUCCEEDED  2022-02-05T17:48:14.003442748Z  2022-02-05T17:50:36Z   \n",
       "\n",
       "   learning_rate  max_depth  n_estimators Training step  accuracy  \n",
       "1       0.139284        8.0           5.0             1  0.884595  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trials[df_trials['Trial ID']==best_trial_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "343d3108-8ff4-4b05-9740-26cdc10fe648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial ID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Start time</th>\n",
       "      <th>End time</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>Training step</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:48:14.003442748Z</td>\n",
       "      <td>2022-02-05T17:50:36Z</td>\n",
       "      <td>0.139284</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.884595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Trial ID     Status                      Start time              End time  \\\n",
       "1        2  SUCCEEDED  2022-02-05T17:48:14.003442748Z  2022-02-05T17:50:36Z   \n",
       "\n",
       "   learning_rate  max_depth  n_estimators Training step  accuracy  \n",
       "1       0.139284        8.0           5.0             1  0.884595  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run = df_trials[df_trials['Trial ID']==best_trial_id]\n",
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a40d71b3-6ecb-4062-b08a-9218ca98e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning_rate', 'max_depth', 'n_estimators']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_names = []\n",
    "\n",
    "for i in parameter_spec.keys():\n",
    "    param_names.append(i)\n",
    "    \n",
    "param_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f3d8c-fbda-4a31-8402-741e5e40a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "#f = open(\"my_sample.csv\")\n",
    "for line in f:\n",
    "    line = line. strip('\\n')\n",
    "    (key, val) = line. split(\",\")\n",
    "    d[key] = val.\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fb332256-3698-4446-b34d-c881cd78308b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139284</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  max_depth  n_estimators\n",
       "1       0.139284        8.0           5.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run_to_dict = best_run[param_names]\n",
    "best_run_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b3160-f6fe-4433-ae8f-453e032beb74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "43ac2135-969f-4e8a-b1da-08ca91eea3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'learning_rate': 0.1392836935516214, 'max_depth': 8.0, 'n_estimators': 5.0}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_coso = best_run_to_dict.to_dict('r')\n",
    "dict_coso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "725e5e69-bd15-403f-83eb-f4b4f2f8cdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1392836935516214, 'max_depth': 8.0, 'n_estimators': 5.0}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_coso[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "84226a1e-8c16-43ad-a10a-57023da740ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1337/1148196513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict_coso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "dict_coso.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "10727673-70d2-4526-a2d1-246a6e52eea4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1113401057.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1337/1113401057.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    d[key] = val.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "d = dict()\n",
    "#f = open(\"my_sample.csv\")\n",
    "for line in best_run_to_dict:\n",
    "    line = line. strip('\\n')\n",
    "    (key, val) = line. split(\",\")\n",
    "    d[key] = val.\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4a1abf3f-53e5-4d39-b340-dcc4c8c2acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.139284\n",
      "Name: learning_rate, dtype: float64\n",
      "1    8.0\n",
      "Name: max_depth, dtype: float64\n",
      "1    5.0\n",
      "Name: n_estimators, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i in parameter_spec.keys():\n",
    "    print(best_run[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b29196f2-a3ed-41ad-9db5-767cccf54f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': None, 'max_depth': None, 'n_estimators': None}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param_dict = dict.fromkeys(parameter_spec)\n",
    "best_param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "550f78fe-2b1d-44c5-8ac6-9f69db9586c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate\n",
      "max_depth\n",
      "n_estimators\n"
     ]
    }
   ],
   "source": [
    "for i in parameter_spec.keys():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11bfacfd-25a8-4049-87c4-0c61435a56c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate\n",
      "max_depth\n",
      "n_estimators\n"
     ]
    }
   ],
   "source": [
    "best_param_dict = {\n",
    "    \n",
    "}\n",
    "\n",
    "for i in parameter_spec.keys():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd235f-13a8-4a25-a8c4-2aa44e05dd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05cb257-6969-4b65-ba0b-d0d06f223ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17908507-c72f-45c9-8c7c-b5391bde6257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ab6a1-023d-4c0e-a632-e3a5cbe1043e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf56be-1450-47a4-a853-519189d359f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be16e0-8882-44be-ac6e-9e6edca7b842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c364f025-af41-4e5c-86d2-14f2f9a28cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = MessageToDict(hp_job._gca_resource._pb)[\"trialJobSpec\"][\n",
    "    \"baseOutputDirectory\"\n",
    "][\"outputUriPrefix\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbeb5cfc-d8bf-405d-a4be-ce653a0bd86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifacts from the Hyperparameter Tuning Job with bbest trial id 2 are located at gs://chicago_taxi_stage/aiplatform-custom-job-2022-02-05-17:48:02.724/2\n"
     ]
    }
   ],
   "source": [
    "# get the model artifacts of the best trial id\n",
    "best_model_artifact_uri = f\"{base_output_dir}/{best_trial_id}\"\n",
    "\n",
    "print(\n",
    "    f\"Model artifacts from the Hyperparameter Tuning Job with bbest trial id {best_trial_id} are located at {best_model_artifact_uri}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75065b97-bedf-4fdd-9680-a16f2f8bb1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73b707fb-88ee-4546-a641-ec87df9f3f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial ID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Start time</th>\n",
       "      <th>End time</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>Training step</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:48:14.003323963Z</td>\n",
       "      <td>2022-02-05T17:50:29Z</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:48:14.003442748Z</td>\n",
       "      <td>2022-02-05T17:50:36Z</td>\n",
       "      <td>0.139284</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.884595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:48:14.003484782Z</td>\n",
       "      <td>2022-02-05T17:50:22Z</td>\n",
       "      <td>0.006533</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:57:27.095152733Z</td>\n",
       "      <td>2022-02-05T17:59:46Z</td>\n",
       "      <td>0.239254</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:57:27.095278834Z</td>\n",
       "      <td>2022-02-05T18:01:11Z</td>\n",
       "      <td>0.097384</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:57:27.095317545Z</td>\n",
       "      <td>2022-02-05T18:01:12Z</td>\n",
       "      <td>0.086835</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T18:06:35.649231816Z</td>\n",
       "      <td>2022-02-05T18:09:13Z</td>\n",
       "      <td>0.235194</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T18:08:16.823511781Z</td>\n",
       "      <td>2022-02-05T18:10:53Z</td>\n",
       "      <td>0.152319</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T18:08:16.823638476Z</td>\n",
       "      <td>2022-02-05T18:12:12Z</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T18:15:47.603375493Z</td>\n",
       "      <td>2022-02-05T18:17:56Z</td>\n",
       "      <td>0.126417</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Trial ID     Status                      Start time              End time  \\\n",
       "0        1  SUCCEEDED  2022-02-05T17:48:14.003323963Z  2022-02-05T17:50:29Z   \n",
       "1        2  SUCCEEDED  2022-02-05T17:48:14.003442748Z  2022-02-05T17:50:36Z   \n",
       "2        3  SUCCEEDED  2022-02-05T17:48:14.003484782Z  2022-02-05T17:50:22Z   \n",
       "3        4  SUCCEEDED  2022-02-05T17:57:27.095152733Z  2022-02-05T17:59:46Z   \n",
       "4        5  SUCCEEDED  2022-02-05T17:57:27.095278834Z  2022-02-05T18:01:11Z   \n",
       "5        6  SUCCEEDED  2022-02-05T17:57:27.095317545Z  2022-02-05T18:01:12Z   \n",
       "6        7  SUCCEEDED  2022-02-05T18:06:35.649231816Z  2022-02-05T18:09:13Z   \n",
       "7        8  SUCCEEDED  2022-02-05T18:08:16.823511781Z  2022-02-05T18:10:53Z   \n",
       "8        9  SUCCEEDED  2022-02-05T18:08:16.823638476Z  2022-02-05T18:12:12Z   \n",
       "9       10  SUCCEEDED  2022-02-05T18:15:47.603375493Z  2022-02-05T18:17:56Z   \n",
       "\n",
       "   learning_rate  max_depth  n_estimators Training step  accuracy  \n",
       "0       0.031623        8.0           7.0             1  0.880762  \n",
       "1       0.139284        8.0           5.0             1  0.884595  \n",
       "2       0.006533        8.0           5.0             1  0.881333  \n",
       "3       0.239254       10.0           5.0             1  0.883024  \n",
       "4       0.097384        4.0           5.0             1  0.880357  \n",
       "5       0.086835       10.0           5.0             1  0.879310  \n",
       "6       0.235194        4.0           5.0             1  0.879786  \n",
       "7       0.152319       10.0           5.0             1  0.881571  \n",
       "8       0.003471       10.0           9.0             1  0.876643  \n",
       "9       0.126417        4.0           7.0             1  0.880714  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68c77889-2530-4792-a65d-fc845c088412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial ID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Start time</th>\n",
       "      <th>End time</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>Training step</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>2022-02-05T17:48:14.003442748Z</td>\n",
       "      <td>2022-02-05T17:50:36Z</td>\n",
       "      <td>0.139284</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.884595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Trial ID     Status                      Start time              End time  \\\n",
       "1        2  SUCCEEDED  2022-02-05T17:48:14.003442748Z  2022-02-05T17:50:36Z   \n",
       "\n",
       "   learning_rate  max_depth  n_estimators Training step  accuracy  \n",
       "1       0.139284        8.0           5.0             1  0.884595  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trials[df_trials['Trial ID']=='2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778a98e-7877-4de6-8a0d-5bd6d74a1ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04f27db1-895c-4f22-9d37-09883c6480b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Trial ID', 'Status', 'Start time', 'End time', 'learning_rate',\n",
       "       'max_depth', 'n_estimators', 'Training step', 'accuracy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trials.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d44598af-fb85-4e4f-bbc4-b4e5fb1e8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials_as_js(trials):\n",
    "    results = []\n",
    "    for trial in trials:\n",
    "        row = {}\n",
    "        t = MessageToDict(trial._pb)\n",
    "        # print(t)\n",
    "        row[\"Trial ID\"], row[\"Status\"], row[\"Start time\"], row[\"End time\"] = (\n",
    "            t[\"id\"],\n",
    "            t[\"state\"],\n",
    "            t[\"startTime\"],\n",
    "            t.get(\"endTime\", None),\n",
    "        )\n",
    "\n",
    "        for param in t[\"parameters\"]:\n",
    "            row[param[\"parameterId\"]] = param[\"value\"]\n",
    "\n",
    "        if t[\"state\"] == \"SUCCEEDED\":\n",
    "            row[\"Training step\"] = t[\"finalMeasurement\"][\"stepCount\"]\n",
    "            for metric in t[\"finalMeasurement\"][\"metrics\"]:\n",
    "                row[metric[\"metricId\"]] = metric[\"value\"]\n",
    "        results.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(results)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c50a3f-505d-4de6-a8af-fcbe83fba61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in trials:\n",
    "        row = {}\n",
    "        t = MessageToDict(trial._pb)\n",
    "        # print(t)\n",
    "        row[\"Trial ID\"], row[\"Status\"], row[\"Start time\"], row[\"End time\"] = (\n",
    "            t[\"id\"],\n",
    "            t[\"state\"],\n",
    "            t[\"startTime\"],\n",
    "            t.get(\"endTime\", None),\n",
    "        )\n",
    "\n",
    "        for param in t[\"parameters\"]:\n",
    "            row[param[\"parameterId\"]] = param[\"value\"]\n",
    "\n",
    "        if t[\"state\"] == \"SUCCEEDED\":\n",
    "            row[\"Training step\"] = t[\"finalMeasurement\"][\"stepCount\"]\n",
    "            for metric in t[\"finalMeasurement\"][\"metrics\"]:\n",
    "                row[metric[\"metricId\"]] = metric[\"value\"]\n",
    "        results.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b715bad0-d3b4-483b-943c-53165b079c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961978b-a79e-456f-b634-896c000e4e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4776be-4402-4173-ac92-b515aae55872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146975a-bd1f-4e7f-918a-b7cb2a2eee16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e3ad0-3a99-4f28-873f-0d02cc309b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e57b0f-8547-4603-be92-ed74a29d3a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f7a61c-47ae-4492-bfde-bbd4237ba29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977c13a-7b43-4ffc-9764-c4304fd0b618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e46738-9d3f-46f1-9e09-3429bccfcbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347662b-9add-452e-a29f-330e5f27b9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7e444-62b7-4ac0-b53c-b9465afd5a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e71730f-d776-4ae7-99a8-999a243ec504",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE_DATA_BUCKET = 'chicago_taxi_stage'\n",
    "TRAIN_DATA_PATH = 'chicago_taxi_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50db61d8-fc77-4615-8274-ee6f04013f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud.storage import Blob\n",
    "gcsclient = storage.Client() # tal vez vaya stage_data_bucket\n",
    "bucket = gcsclient.get_bucket(STAGE_DATA_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1a1809b-de0f-46f4-b5b5-cb189c303d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72fa3e4f-70fd-4e2d-b03e-2d1269fbaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.download_to_filename(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad81e6ac-8b70-4d12-acc2-2870ec3b6921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'trip_month', 'trip_day', 'trip_day_of_week',\n",
       "       'trip_hour', 'trip_seconds', 'trip_miles', 'euclidean', 'target',\n",
       "       'payment_type_Credit_Card', 'payment_type_Dispute',\n",
       "       'payment_type_No_Charge', 'payment_type_Pcard',\n",
       "       'payment_type_Prcard', 'payment_type_Unknown'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa6a07d5-d087-44d1-a3dd-ed8eca8341ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['trip_month', 'trip_day', 'trip_day_of_week',\n",
    "       'trip_hour', 'trip_seconds', 'trip_miles', 'euclidean', 'target',\n",
    "       'payment_type_Credit_Card', 'payment_type_Dispute',\n",
    "       'payment_type_No_Charge', 'payment_type_Pcard',\n",
    "       'payment_type_Prcard', 'payment_type_Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67fff822-4405-4311-bd85-2a8f147a1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(TRAIN_DATA_PATH, usecols=cols)\n",
    "data = data.sample(frac = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b07f739-cf8b-4efe-8a1e-25e30bc191b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_month</th>\n",
       "      <th>trip_day</th>\n",
       "      <th>trip_day_of_week</th>\n",
       "      <th>trip_hour</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>euclidean</th>\n",
       "      <th>target</th>\n",
       "      <th>payment_type_Credit_Card</th>\n",
       "      <th>payment_type_Dispute</th>\n",
       "      <th>payment_type_No_Charge</th>\n",
       "      <th>payment_type_Pcard</th>\n",
       "      <th>payment_type_Prcard</th>\n",
       "      <th>payment_type_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>637949</th>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1500</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6315.425377</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656465</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>540</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4167.851559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337739</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>540</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3678.274197</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230222</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>1020</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5271.263705</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82039</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>3060</td>\n",
       "      <td>12.6</td>\n",
       "      <td>16280.892789</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630893</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3047.073574</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468459</th>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>2160</td>\n",
       "      <td>5.9</td>\n",
       "      <td>8528.077092</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605993</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>960</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3561.336473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650433</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1082.894793</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542482</th>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1080</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3567.255865</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140000 rows 칑 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        trip_month  trip_day  trip_day_of_week  trip_hour  trip_seconds  \\\n",
       "637949          12        22                 3         12          1500   \n",
       "656465           1        23                 6          3           540   \n",
       "337739          12        18                 6         23           540   \n",
       "230222           7        15                 4         14          1020   \n",
       "82039            7        10                 6         13          3060   \n",
       "...            ...       ...               ...        ...           ...   \n",
       "630893           1         8                 5         18          1200   \n",
       "468459          12        21                 2         19          2160   \n",
       "605993           5        12                 3         13           960   \n",
       "650433           3         9                 2         20           300   \n",
       "542482           7        22                 4         22          1080   \n",
       "\n",
       "        trip_miles     euclidean  target  payment_type_Credit_Card  \\\n",
       "637949         5.8   6315.425377       0                         1   \n",
       "656465         2.7   4167.851559       1                         1   \n",
       "337739         2.3   3678.274197       0                         0   \n",
       "230222         0.2   5271.263705       1                         1   \n",
       "82039         12.6  16280.892789       1                         1   \n",
       "...            ...           ...     ...                       ...   \n",
       "630893         0.1   3047.073574       1                         1   \n",
       "468459         5.9   8528.077092       0                         0   \n",
       "605993         2.3   3561.336473       0                         0   \n",
       "650433         0.9   1082.894793       0                         0   \n",
       "542482         4.9   3567.255865       0                         0   \n",
       "\n",
       "        payment_type_Dispute  payment_type_No_Charge  payment_type_Pcard  \\\n",
       "637949                     0                       0                   0   \n",
       "656465                     0                       0                   0   \n",
       "337739                     0                       0                   0   \n",
       "230222                     0                       0                   0   \n",
       "82039                      0                       0                   0   \n",
       "...                      ...                     ...                 ...   \n",
       "630893                     0                       0                   0   \n",
       "468459                     0                       0                   0   \n",
       "605993                     0                       0                   0   \n",
       "650433                     0                       0                   0   \n",
       "542482                     0                       0                   0   \n",
       "\n",
       "        payment_type_Prcard  payment_type_Unknown  \n",
       "637949                    0                     0  \n",
       "656465                    0                     0  \n",
       "337739                    0                     0  \n",
       "230222                    0                     0  \n",
       "82039                     0                     0  \n",
       "...                     ...                   ...  \n",
       "630893                    0                     0  \n",
       "468459                    0                     0  \n",
       "605993                    0                     0  \n",
       "650433                    0                     0  \n",
       "542482                    0                     0  \n",
       "\n",
       "[140000 rows x 14 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b13d4-0293-496e-a016-45de5cf8433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.experimental.custom_job.utils import create_custom_training_job_op_from_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4745de1c-7e50-490d-9ee2-8e09332d1318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f85b46-df54-421e-8160-a7cd4ba23c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0bacbf-90fa-4b9a-9169-a2544e5b4d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33198d67-e722-42d1-9ab4-189cb0840e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934b17c-275d-4d37-9f92-b72781157453",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pipeline parte 2: listo el train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75d92623-5b35-4839-9142-94c4d8c292d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pipeline mio\n",
    "@pipeline(name=\"xgb-chicago-parte2\",\n",
    "                pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    bq_source_url: str = BQ_SOURCE_URL,\n",
    "    kfp_artifacts_bucket: str = KFP_ARTIFACTS_BUCKET,\n",
    "    stage_data_bucket: str = STAGE_DATA_BUCKET,\n",
    "    pipelines_bucket: str = PIPELINE_BUCKET,\n",
    "    pipeline_root: str = PIPELINE_ROOT,\n",
    "    train_container: str = TRAIN_CONTAINER,\n",
    "    vertex_dataset_source: str = VERTEX_DATASET_SOURCE\n",
    "    \n",
    "):\n",
    "    \n",
    "#     dataframe = get_chicago_data(project = project_id,\n",
    "#                     region = gcp_region,\n",
    "#                     bq_source_url = bq_source_url,\n",
    "#                     stage_data_bucket = stage_data_bucket)\n",
    "    \n",
    "#     with dsl.Condition(\n",
    "#         dataframe.output == 'done'\n",
    "#     ):\n",
    "#         dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "#             display_name=\"chicago-data-process\",\n",
    "#             gcs_source=vertex_dataset_source,\n",
    "#             project=project_id,\n",
    "#             location=gcp_region\n",
    "#         )\n",
    "        \n",
    "        training_job_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "            display_name= 'xgb-chicago',\n",
    "            project = project_id,\n",
    "            container_uri = train_container,\n",
    "            location = gcp_region,\n",
    "            #dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "            #training_fraction_split=1.,\n",
    "            staging_bucket = stage_data_bucket,\n",
    "            bigquery_destination=\"bq://{0}\".format(PROJECT_ID),\n",
    "            model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1\",\n",
    "            model_display_name = 'xgb-chicago-model-joblib',\n",
    "            machine_type = 'n1-standard-8'\n",
    "            \n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbca9206-be8e-4d1d-aae7-e04d7b5d799f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### pipeline mio\n",
    "@pipeline(name=\"xgb-chicago-parte2\",\n",
    "                pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    bq_source_url: str = BQ_SOURCE_URL,\n",
    "    kfp_artifacts_bucket: str = KFP_ARTIFACTS_BUCKET,\n",
    "    stage_data_bucket: str = STAGE_DATA_BUCKET,\n",
    "    pipelines_bucket: str = PIPELINE_BUCKET,\n",
    "    pipeline_root: str = PIPELINE_ROOT,\n",
    "    train_container: str = TRAIN_CONTAINER,\n",
    "    vertex_dataset_source: str = VERTEX_DATASET_SOURCE\n",
    "    \n",
    "):\n",
    "    \n",
    "    dataframe = get_chicago_data(project = project_id,\n",
    "                    region = gcp_region,\n",
    "                    bq_source_url = bq_source_url,\n",
    "                    stage_data_bucket = stage_data_bucket)\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        dataframe.output == 'done'\n",
    "    ):\n",
    "        dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "            display_name=\"chicago-data-process\",\n",
    "            gcs_source=vertex_dataset_source,\n",
    "            project=project_id,\n",
    "            location=gcp_region\n",
    "        )\n",
    "        \n",
    "        training_job_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "            display_name= 'xgb-chicago',\n",
    "            project = project_id,\n",
    "            container_uri = train_container,\n",
    "            location = gcp_region,\n",
    "            dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "            training_fraction_split=1.,\n",
    "            staging_bucket = stage_data_bucket,\n",
    "            model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1\",\n",
    "            model_display_name = 'xgb-chicago-model-joblib',\n",
    "            machine_type = 'n1-standard-8'\n",
    "            \n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5523fd0-fdb3-41b8-93ad-a09c7c70b514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='xgb-chicago-parte2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4de5788f-4bde-4d1c-9b3f-1ef8a592df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b9c6e2-753f-4cff-87ea-c05afbfc73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"xgb-chicago-displaynamepipejob\",\n",
    "    template_path=\"xgb-chicago-parte2.json\",\n",
    "    job_id=\"xgb-chicago-parte2-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74c40976-e740-4259-9e6e-95f5957bd149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/155345500736/locations/us-central1/pipelineJobs/xgb-chicago-parte2-20220205231043\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/155345500736/locations/us-central1/pipelineJobs/xgb-chicago-parte2-20220205231043')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/xgb-chicago-parte2-20220205231043?project=155345500736\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e331e-f1f2-4c99-b571-192f3af6b466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb004b2-f0f3-40e8-bfe2-10b129d1de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.cloud import bigquery#\n",
    "from google.cloud import storage\n",
    "from joblib import dump\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "STAGE_DATA_BUCKET = 'chicago_taxi_stage'\n",
    "TRAIN_DATA_PATH = 'chicago_taxi_train.csv'\n",
    "MODEL_NAME = 'model.joblib'\n",
    "\n",
    "bqclient = bigquery.Client()\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323dbbaf-55ca-4151-8626-0c9c0ae0a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(STAGE_DATA_BUCKET)\n",
    "\n",
    "blob = bucket.blob(TRAIN_DATA_PATH)\n",
    "blob.download_to_filename(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d906512b-43f1-48d5-94c6-0e29c29e2a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:27:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_TEST.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'model_TEST.joblib'\n",
    "# Download data into Pandas DataFrames, split into train / test\n",
    "X = pd.read_csv(TRAIN_DATA_PATH)\n",
    "\n",
    "#test_df = download_table(test_data_uri)\n",
    "#labels = df.pop(\"target\").tolist()\n",
    "#data = df.values.tolist()\n",
    "#test_labels = test_df.pop(\"target\").tolist()\n",
    "#test_data = test_df.values.tolist()\n",
    "\n",
    "y = X.pop('target')\n",
    "\n",
    "# Define and train the Scikit model\n",
    "#skmodel = RandomForestClassifier()\n",
    "#skmodel.fit(X, y)\n",
    "#score = skmodel.score(test_data, test_labels)\n",
    "#print('accuracy is:',score)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb.fit(X, y)\n",
    "\n",
    "# Save the model to a local file\n",
    "dump(xgb, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f582123-0be1-4707-b2aa-8c3740b96929",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_model = bucket.blob(MODEL_NAME)\n",
    "blob_model.upload_from_filename(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89972482-6258-4fe3-9003-f803077e6f1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "URI scheme must be gs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24981/3351897761.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_from_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mfrom_string\u001b[0;34m(cls, uri, client)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscheme\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"gs\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"URI scheme must be gs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: URI scheme must be gs"
     ]
    }
   ],
   "source": [
    "blob = storage.blob.Blob.from_string(MODEL_NAME, client=storage_client)\n",
    "blob.upload_from_filename(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be39be-f926-429b-8805-3f5822bbafe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cadcadb-d64b-4413-ba39-2201bfbe6681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f900d9-56c1-46fb-9a11-4102204482c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae9577-2ccd-4e40-bc2c-6ecae40e97e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecad34-a5c6-4c38-b8fd-a41d35a0ec25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde56b4f-d80d-4a91-9915-2b8c910f9077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f0ac7a8-3eac-4920-80ce-34254a2d2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.get_blob('get_chicago_data.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d498d140-d427-4583-b824-61425a4a9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coso = blob.download_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a90508a3-fa5f-4525-b736-3539ccf618eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_chicago = kfp.components.load_component_from_file(GET_CHICAGO_DATA_YAML_123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f0a31ec-f56f-482d-8016-bd921583eeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Get chicago data(project: str, region: str, url: str)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66398e0d-4769-4745-aa21-0e2286062650",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://storage.googleapis.com/chicago_taxi_artifacts/get_chicago_data.yaml",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6461/129980452.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_chicago\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_component_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://chicago_taxi_artifacts/get_chicago_data.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp/components/_components.py\u001b[0m in \u001b[0;36mload_component_from_url\u001b[0;34m(url, auth)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mOnce\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrequired\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfactory\u001b[0m \u001b[0mconstructs\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0mtask\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mContainerOp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \"\"\"\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mcomponent_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_component_spec_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_component_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mcomponent_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComponentReference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp/components/_components.py\u001b[0m in \u001b[0;36m_load_component_spec_from_url\u001b[0;34m(url, auth)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_load_component_spec_from_yaml_or_zip_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://storage.googleapis.com/chicago_taxi_artifacts/get_chicago_data.yaml"
     ]
    }
   ],
   "source": [
    "load_chicago = kfp.components.load_component_from_url('gs://chicago_taxi_artifacts/get_chicago_data.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bca17da-6151-45c8-bb24-8b05878fd32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265a29c6-fae6-4f99-8bc5-faadc71f4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client(project = PROJECT_ID, location = REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4606695-9dd7-4fe0-92d5-11334784302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = bigquery.TableReference.from_string(\n",
    "        BQ_SOURCE_URL\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee9c128-dac1-42b4-9254-581b13be6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = bqclient.list_rows(\n",
    "        table,\n",
    "        # para un grano mas fino\n",
    "        # \n",
    "        #selected_fields=[\n",
    "        #    bigquery.SchemaField(\"country_name\", \"STRING\"),\n",
    "        #    bigquery.SchemaField(\"fips_code\", \"STRING\"),\n",
    "        #],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cebe112-b132-43f7-8da0-f0e3dcef5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rows.to_dataframe(\n",
    "        # Optionally, explicitly request to use the BigQuery Storage API. As of\n",
    "        # google-cloud-bigquery version 1.26.0 and above, the BigQuery Storage\n",
    "        # API is used by default.\n",
    "        create_bqstorage_client=True, # guarda ac치\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d0cfc90-1940-4c40-a913-020e3a2e2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "train, test = tts(data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b87467-4cbb-4202-9fb0-8dcfccfd3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### subir al bucket\n",
    "#     gcsclient = storage.Client()\n",
    "    \n",
    "#     bucket = gcsclient.get_bucket('chicagotaxi-stage')\n",
    "    \n",
    "#     blob = bucket.blob('chicago-data.csv')\n",
    "    \n",
    "#     blob.upload_from_filename('chicago-data.csv')\n",
    "    \n",
    "#     return 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d21426e-401f-46f7-ab6a-f1f99dc0dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(TRAIN_DATA_PATH)\n",
    "test.to_csv(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09dd752a-b39f-4cb7-b4fe-b356b13a50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud.storage import Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a593384-73ea-4f85-a99a-d6e315e3c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cliente = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f366d5-be51-40dc-b2c7-9ffe9692145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcsclient = storage.Client(STAGE_DATA_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6aaa584-c216-490e-bd62-52343133f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = 'chicago_taxi_train.csv'\n",
    "TEST_DATA_PATH = 'chicago_taxi_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d75f53-a578-4cf9-a00c-507124a33594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f9baca4-5481-4f8c-8175-99c0f0b78134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicago_taxi_stage'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STAGE_DATA_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38b056-aa69-4169-91cd-5e04ee9669ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd6f7788-1081-469c-9c48-b676f40f20b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b73ad-d26a-425f-8c69-0eef9ede5d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0204d-2816-4336-9a89-4aeab5d0d7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff615e-918f-4137-a1d3-eb1ce7eb64e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff413d2-e595-4541-8d5e-7775a735d277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba5f5d6-c176-4486-949b-9d4861d9bb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711fb808-d9c9-492a-8b2b-15491fa727fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464cff0-6225-4176-9e05-d816ba92066a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa3f0df4-480c-445d-843c-b4e6deacc37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://chicagotaxi-stage/pipeline_root/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET_NAME=\"gs://chicagotaxi-stage\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61fea8fe-c0e1-499f-8b65-b4c237f8bbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4bc5b-23bc-46f1-9681-53b9d66cea24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1088cf-c752-4f5f-a537-52a1b1b116e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# microtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b452f880-cceb-4c5c-94b5-101cbbaa34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 01 - levantar la data de bigquery y pasar Outputs de tipo dataset\n",
    "### 01 - levantar la data de bigquery y pasar Outputs de tipo dataset\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"get_chicago_data.yaml\"\n",
    ")\n",
    "\n",
    "def get_chicago_data(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    url: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    bqclient = bigquery.Client(project = project, location = region)\n",
    "\n",
    "    # Download a table.\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        url\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "        #selected_fields=[\n",
    "        #    bigquery.SchemaField(\"country_name\", \"STRING\"),\n",
    "        #    bigquery.SchemaField(\"fips_code\", \"STRING\"),\n",
    "        #],\n",
    "    )\n",
    "    data = rows.to_dataframe(\n",
    "        # Optionally, explicitly request to use the BigQuery Storage API. As of\n",
    "        # google-cloud-bigquery version 1.26.0 and above, the BigQuery Storage\n",
    "        # API is used by default.\n",
    "        create_bqstorage_client=True, # guarda ac치\n",
    "    )\n",
    "    \n",
    "    # preproceso para poder pasar a csv\n",
    "    #df_wine = pd.read_csv(url, delimiter=\";\")\n",
    "    #df_wine['best_quality'] = [ 1 if x>=7 else 0 for x in df_wine.quality] \n",
    "    #df_wine['target'] = df_wine.best_quality\n",
    "    #df_wine = df_wine.drop(['quality', 'total sulfur dioxide', 'best_quality'], axis=1)\n",
    "      \n",
    "    train, test = tts(data, test_size=0.3)\n",
    "    train.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a5c4b68-a4a7-4602-b984-4126c4fcc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pipeline mio\n",
    "@pipeline(name=\"xgb-chicago-parte1\",\n",
    "                pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_table: str = \"vertex-testing-327520.chicago_taxi_2015.chicago_taxitrips_prep\",\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION\n",
    "):\n",
    "    \n",
    "    get_chicago_data(url = bq_table,\n",
    "                    project = project,\n",
    "                    region = gcp_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419321bd-7f64-4420-ba1b-6ed6007a221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='xgb-chicago-parte1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f42862-aa46-491d-82ac-649f9bdc46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a17d9a0-e445-411e-b9b0-26d11f5f2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"xgb-chicago-displaynamepipejob\",\n",
    "    template_path=\"xgb-chicago-parte1.json\",\n",
    "    job_id=\"xgb-chicago-parte1-{0}\".format(TIMESTAMP),\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"bucket\": BUCKET_NAME\n",
    "    },\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9542e6d-4abc-4e2b-937a-90cd6f43bcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/155345500736/locations/us-central1/pipelineJobs/xgb-chicago-parte1-20220201205832\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/155345500736/locations/us-central1/pipelineJobs/xgb-chicago-parte1-20220201205832')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/xgb-chicago-parte1-20220201205832?project=155345500736\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7fe38-e76d-44c1-8b77-07ca34424d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fb8b9-a27a-4c0f-8ab1-81c7641d736b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204ed57-b2ff-4e3b-9f58-915650b6e419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682ba96-0e54-4c75-af47-fca55c44c1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deefe2b6-f002-49d9-a5e0-054bc42f1b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b4eb4-a956-4af7-bfac-409fd9546b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5444fd9-15be-463e-84df-fc312d9a4c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d7bb7-8d4f-484f-836d-77114c5d2dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141af672-e477-4b92-ba9f-3ab3a655280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pipeline mio\n",
    "@dsl.pipeline(name=\"rf-chicago-parte2-5\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_source: str = \"bq://vertex-testing-327520.chicago_taxi_stage.chicago_ml\",\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    bq_dest: str = \"\",\n",
    "    container_uri: str = \"\",\n",
    "    # batch_destination: str = \"\"\n",
    "):\n",
    "    \n",
    "    #dataframe = get_dataframe(bq_table)\n",
    "    \n",
    "    #with dsl.Condition(\n",
    "    #    dataframe.output == 'done'\n",
    "    #):\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        display_name=\"chicago-data-process\",\n",
    "        bq_source=bq_source,\n",
    "        project=project,\n",
    "        location=gcp_region\n",
    "    )\n",
    "        \n",
    "    training_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        display_name=\"chicago-xgb-train-pipeline\",\n",
    "        container_uri=container_uri,\n",
    "        project=project,\n",
    "        location=gcp_region,\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        staging_bucket=bucket,\n",
    "        training_fraction_split=1., # Hack para poder hacer el train-test split luego\n",
    "        # validation_fraction_split=0.1,\n",
    "        # test_fraction_split=0.1,\n",
    "        bigquery_destination=bq_dest,\n",
    "        model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1\",\n",
    "        model_display_name=\"chicago-xgb-pipeline\",\n",
    "        machine_type=\"n1-standard-8\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1560d54f-5c02-492e-abe0-b2a2ae6cb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b71d46-f756-486e-aea6-ef756cf4d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "        pipeline_func=pipeline, package_path=f\"custom_train_pipeline_{TIMESTAMP}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2c631d-00d8-4225-b8d7-1ca4b10ddf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"customtrain-xgb\",\n",
    "    template_path=f\"custom_train_pipeline_{TIMESTAMP}.json\",\n",
    "    job_id=\"custom-train-pipeline-xgb-{0}\".format(TIMESTAMP),\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"bucket\": BUCKET_NAME,\n",
    "        \"bq_dest\": \"bq://{0}\".format(PROJECT_ID),\n",
    "        \"container_uri\": \"gcr.io/{0}/xgb_chicago:v1\".format(PROJECT_ID),\n",
    "        # \"batch_destination\": \"{0}/batchpredresults\".format(BUCKET_NAME)\n",
    "    },\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fed4c19d-d04b-435f-a57e-d7dba788f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/155345500736/locations/us-central1/pipelineJobs/custom-train-pipeline-xgb-20220201183920\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/155345500736/locations/us-central1/pipelineJobs/custom-train-pipeline-xgb-20220201183920')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-train-pipeline-xgb-20220201183920?project=155345500736\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774eb3c3-a659-4a9e-b32b-133b61221cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4fad7-11e6-4740-8069-62d7a9ff7580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a0d34-96b7-4781-b195-c90f79e8f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88aa95b-47fe-4c2b-b2b3-eca234cc703a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32a31b-f676-45d0-8fab-98d5e3498750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1458c3-f7b4-4f7c-ad26-350f7ccbe39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23484f-1e4f-47e4-a455-6d4e0757bff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009a340-4825-490c-b0b5-3d9973c1b45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea1289-4e70-4803-9f30-f6d035219b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b479c8-e682-4178-afee-543c0df4e7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b31fa-c29f-4aa2-b5fe-fe5118d1d793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4418133c-ae4c-4321-84be-bb04d8a6d35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bf208-e122-4533-8db4-833bf73423c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "172bb77c-6d16-4897-9ef5-29f5d75e8eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/xgboost-cpu\n",
      "WORKDIR /\n",
      "\n",
      "#Copies the trainer code to the docker image.\n",
      "COPY trainer /trainer\n",
      "\n",
      "RUN pip install sklearn xgboost scipy google-cloud-bigquery joblib pandas google-cloud-storage\n",
      "\n",
      "# Sets the entry points to invoke the trainer.\n",
      "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]\n"
     ]
    }
   ],
   "source": [
    "cat traincontainer/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf23df4a-e24d-4f37-bac2-400ab519cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#from sklearn.ensamble import RandomForestClassifier\n",
      "from sklearn.metrics import roc_curve\n",
      "from sklearn.model_selection import train_test_split\n",
      "from google.cloud import bigquery#\n",
      "from google.cloud import storage\n",
      "from joblib import dump\n",
      "\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "from xgboost import XGBClassifier\n",
      "\n",
      "bqclient = bigquery.Client()\n",
      "storage_client = storage.Client()\n",
      "\n",
      "\n",
      "\n",
      "def download_table(bq_table_uri: str):\n",
      "    prefix = \"bq://\"\n",
      "    if bq_table_uri.startswith(prefix):\n",
      "        bq_table_uri = bq_table_uri[len(prefix):]\n",
      "\n",
      "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
      "    rows = bqclient.list_rows(\n",
      "        table,\n",
      "    )\n",
      "    return rows.to_dataframe(create_bqstorage_client=False)\n",
      "\n",
      "# These environment variables are from Vertex AI managed datasets\n",
      "training_data_uri = os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
      "#test_data_uri = os.environ[\"AIP_TEST_DATA_URI\"] # los chicos borran esto\n",
      "\n",
      "# Download data into Pandas DataFrames, split into train / test\n",
      "X = download_table(training_data_uri)\n",
      "\n",
      "#test_df = download_table(test_data_uri)\n",
      "#labels = df.pop(\"target\").tolist()\n",
      "#data = df.values.tolist()\n",
      "#test_labels = test_df.pop(\"target\").tolist()\n",
      "#test_data = test_df.values.tolist()\n",
      "\n",
      "y = df.pop('target')\n",
      "\n",
      "# Define and train the Scikit model\n",
      "#skmodel = RandomForestClassifier()\n",
      "#skmodel.fit(X, y)\n",
      "#score = skmodel.score(test_data, test_labels)\n",
      "#print('accuracy is:',score)\n",
      "\n",
      "xgb = XGBClassifier()\n",
      "\n",
      "xgb.fit(X, y)\n",
      "\n",
      "# Save the model to a local file\n",
      "dump(xgb, \"model.joblib\")\n",
      "\n",
      "# Upload the saved model file to GCS\n",
      "bucket = storage_client.get_bucket(\"chicagotaxi-stage\")\n",
      "model_directory = os.environ[\"AIP_MODEL_DIR\"]\n",
      "storage_path = os.path.join(model_directory, \"model.joblib\")\n",
      "blob = storage.blob.Blob.from_string(storage_path, client=storage_client)\n",
      "blob.upload_from_filename(\"model.joblib\")"
     ]
    }
   ],
   "source": [
    "!cat traincontainer/trainer/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0817fe4-f156-4ae7-bd62-4a2d4f0bebd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
