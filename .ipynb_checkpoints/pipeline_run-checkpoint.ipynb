{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3109952b-ddd7-49d2-a4da-3183e757ac9e",
   "metadata": {},
   "source": [
    "# Vertex Pipelines - A Serverless framework for MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9aaf1-b8fc-4532-9a8c-d682321cd90c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda0d8e5-c772-46c6-85ff-4ed8e0696aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-pipeline-components==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd984ac-7ab1-454a-bac9-fbbba93d7cee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  teco-prod-adam-dev-826c\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Obtener el project ID\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e83c04-8ac8-4f76-8a42-e6f5a1a21c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "# variables de entorno: PATH local, region de GCP y timestamp\n",
    "\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "REGION=\"us-central1\" # disponibilidad completa de Vertex / Complete Vertex availability\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345caf8f-187c-41c9-8e38-b715e6c55060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros cloud (almacenamiento de objetos, outputs, etc)\n",
    "# cloud environment parameters (object storage, outputs, etc)\n",
    "STAGE_DATA_BUCKET = f'{PROJECT_ID}-chicago_taxi_stage'\n",
    "PIPELINE_BUCKET = f'{PROJECT_ID}-chicago_taxi_pipelines'\n",
    "PIPELINE_ROOT = f\"gs://{PIPELINE_BUCKET}/pipeline_root/\"\n",
    "\n",
    "# configuracion de componentes de ml\n",
    "# ML components configurations\n",
    "SERVING_CONTAINER = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'\n",
    "MACHINE_TYPE = 'n1-standard-16'\n",
    "\n",
    "# configuracion de intentos de tuneo de hiperparametros\n",
    "# configuration of hp tuning job trials\n",
    "RF_HP_IMAGE = f'gcr.io/{PROJECT_ID}/rf_hp_job:v1'\n",
    "LR_HP_IMAGE = f'gcr.io/{PROJECT_ID}/lr_hp_job:v1'\n",
    "HP_TRAIN_MACHINE = \"n1-standard-16\"\n",
    "HP_TRIALS = 3\n",
    "PARALLEL_TRIALS = 3\n",
    "\n",
    "# umbral de aceptabilidad para prediccion batch\n",
    "# acceptability threshold for batch prediction\n",
    "THRESHOLD = 0.7\n",
    "\n",
    "# habilitar cache en el pipeline para ahorrar costos\n",
    "# enable cache in pipeline execution to prevent costs\n",
    "ENABLE_CACHE = True\n",
    "\n",
    "# rutas de acceso a los datasets de train, val y test\n",
    "# access paths for train, val and test datasets\n",
    "TRAIN_DATA_PATH = 'chicago_taxi_train.csv'\n",
    "VAL_DATA_PATH = 'chicago_taxi_val.csv'\n",
    "TEST_DATA_PATH = 'chicago_taxi_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3418e6-0bda-49e1-9e66-b476df15dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery: definiciones de variables, pueden ser facilmente reemplazadas para adaptarse a otros propositos\n",
    "# BigQuery variable definitions: those can be easily changed to suit another purpose\n",
    "\n",
    "BQ_DATASET_HISTORIC_NAME = 'chicago_taxi_historic_test'\n",
    "BQ_DATASET_CURRENT_NAME = 'chicago_taxi_current_test'\n",
    "\n",
    "BQ_HISTORIC_RAW = 'raw'\n",
    "BQ_HISTORIC_STAGE = 'stage_ml'\n",
    "\n",
    "BQ_CURRENT_RAW = 'raw'\n",
    "BQ_CURRENT_STAGE = 'stage_ml'\n",
    "\n",
    "BQ_CURRENT_RAW_URL = f\"{PROJECT_ID}.{BQ_DATASET_CURRENT_NAME}.{BQ_CURRENT_RAW}\"\n",
    "BQ_CURRENT_STAGE_URL = f\"{PROJECT_ID}.{BQ_DATASET_CURRENT_NAME}.{BQ_CURRENT_STAGE}\"\n",
    "\n",
    "BQ_HISTORIC_RAW_URL = f\"{PROJECT_ID}.{BQ_DATASET_HISTORIC_NAME}.{BQ_HISTORIC_RAW}\"\n",
    "BQ_HISTORIC_STAGE_URL = f\"{PROJECT_ID}.{BQ_DATASET_HISTORIC_NAME}.{BQ_HISTORIC_STAGE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ad1f8-fee3-41b5-8a53-756be97ab822",
   "metadata": {},
   "source": [
    "### Librerias - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b73a7c-dbb8-41e8-96c1-ff756358b212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  teco-prod-adam-dev-826c\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import kfp\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath, ClassificationMetrics\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform, bigquery\n",
    "\n",
    "# We'll use this namespace for metadata querying\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental.custom_job.utils import create_custom_training_job_op_from_component\n",
    "\n",
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a7951b-6d7e-48ca-8934-33ba5bf25fd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Definicion de componentes - Component definition\n",
    "\n",
    "Hay una carpeta llamada *components* donde se encuentra una notebook que genera los distintos componentes necesarios para el pipeline y sus correspondientes yaml. Dada las caracter√≠sticas de este pipeline, muchos componentes requieren armarse de manera customizada y no depender de los preconstruidos.\n",
    "\n",
    "There's a folder named *components* with a notebook that generates the needed components for the pipeline and their corresponding yaml files. Given the complexity of this pipeline, many components needs to be custom built and not use the pre-built one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af34607a-8023-4d5d-a187-8afc438bfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_evaluation = kfp.components.load_component_from_file('./components/best_model_evaluation.yaml')\n",
    "best_model_hp_tuning = kfp.components.load_component_from_file('./components/best_model_hp_tuning.yaml')\n",
    "bq_current_raw_to_stage = kfp.components.load_component_from_file('./components/bq_current_raw_to_stage.yaml')\n",
    "bq_historic_raw_to_stage = kfp.components.load_component_from_file('./components/bq_historic_raw_to_stage.yaml')\n",
    "get_chicago_data = kfp.components.load_component_from_file('./components/get_chicago_data.yaml')\n",
    "model_evaluation = kfp.components.load_component_from_file('./components/model_evaluation.yaml')\n",
    "train_best_model = kfp.components.load_component_from_file('./components/train_best_model.yaml')\n",
    "train_lr_chicago = kfp.components.load_component_from_file('./components/train_lr_chicago.yaml')\n",
    "train_rf_chicago = kfp.components.load_component_from_file('./components/train_rf_chicago.yaml')\n",
    "upload_model_to_vertex_and_batch_prediction = kfp.components.load_component_from_file('./components/upload_model_to_vertex_and_batch_prediction.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1ee70-7119-4d8b-8f86-ebcff4317845",
   "metadata": {},
   "source": [
    "El componente de entrenamiento tiene que ser convertido en jobs para que pueda funcionar dentro del contexto de la suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a730fd-7d9b-4e62-91da-84918c501e88",
   "metadata": {},
   "source": [
    "The training components need to be turned into jobs to be performed within the context of the suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba882edb-d1c2-4975-b927-7eeb3a824907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion del job usando el componente custom previamente creado\n",
    "# Job definition using previously created custom component\n",
    "\n",
    "train_rf_chicago_op = create_custom_training_job_op_from_component(\n",
    "    train_rf_chicago,\n",
    "    machine_type=MACHINE_TYPE\n",
    ")\n",
    "\n",
    "train_best_model_op = create_custom_training_job_op_from_component(\n",
    "    train_best_model,\n",
    "    machine_type=MACHINE_TYPE\n",
    ")\n",
    "\n",
    "train_lr_chicago_op = create_custom_training_job_op_from_component(\n",
    "    train_lr_chicago,\n",
    "    machine_type=MACHINE_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28392a4f-a640-48eb-a7a1-2f364fd0a13d",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676fd8d-5ae1-4337-bed9-936a7c7954d8",
   "metadata": {},
   "source": [
    "La definici√≥n del pipeline es esencialmente una funci√≥n. Desde aqu√≠ se orquesta todo el proceso, pas√°ndole los par√°metros necesarios para que cada elemento del pipeline pueda performar. De manera similar a un orquestador, cada una de las funciones es un paso que tiene *inputs* y *outputs*, as√≠ como tambi√©n algunos par√°metros y valores que opcionalmente pueden salirse del flujo natural: esto resulta de mucha utilidad para almacenar m√©tricas o valores a partir de los cuales se tomar√°n decisiones a lo largo del camino. \n",
    "\n",
    "Esto se evidencia especialmente en la predicci√≥n batch, que utiliza el objeto Condition a partir del cual, si el modelo entrenado cumple con un cierto standard (en este caso, la m√©trica F1) el trabajo se ejecuta, y en caso contrario, se detiene todo el proceso.\n",
    "\n",
    "En cuanto a las m√©tricas, estas pueden ser consultadas en distintas etapas del proceso, as√≠ como tambi√©n visualizarse en la interfaz gr√°fica de Vertex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7aa5b-dc8f-44ea-b30a-ca1db49bd223",
   "metadata": {},
   "source": [
    "The pipeline definition is essentially a function. From here the entire process is orchestrated, passing along needed parameters so each element can trigger. Each of the functions is a step with inputs and outputs, as well as some parameters and values that can optionally skip or exit the flow: this is very useful to store metrics or values upon which certain parts of the process are triggered or not.\n",
    "\n",
    "This is specially notable in batch prediction job, based on the dsl.Condition object that executes jobs according to a certain criteria met. In this case, if the trained model has an F1 score above the threshold, the batch prediction gets triggered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64eccf5-9ba4-4549-aa50-7d2f671affb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pipeline mio\n",
    "@dsl.pipeline(name='chicago-taxi-pipeline',\n",
    "                pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    stage_data_bucket: str = STAGE_DATA_BUCKET,\n",
    "    pipelines_bucket: str = PIPELINE_BUCKET,\n",
    "    pipeline_root: str = PIPELINE_ROOT,\n",
    "    serving_container: str = SERVING_CONTAINER,\n",
    "    machine_type: str = MACHINE_TYPE,\n",
    "    trials: int = HP_TRIALS,\n",
    "    parallel_trials: int = PARALLEL_TRIALS,\n",
    "    rf_hp_image: str = RF_HP_IMAGE,\n",
    "    lr_hp_image: str = LR_HP_IMAGE,\n",
    "    hp_train_machine: str = HP_TRAIN_MACHINE,\n",
    "    bq_current_raw_url: str = BQ_CURRENT_RAW_URL,\n",
    "    bq_current_stage_url: str = BQ_CURRENT_STAGE_URL,\n",
    "    bq_historic_raw_url: str = BQ_HISTORIC_RAW_URL,\n",
    "    bq_historic_stage_url: str = BQ_HISTORIC_STAGE_URL,\n",
    "    threshold: float = THRESHOLD,\n",
    "    enable_cache: bool = ENABLE_CACHE\n",
    "    \n",
    "    \n",
    "):\n",
    "    \n",
    "    bq_stage_ml = bq_historic_raw_to_stage(\n",
    "        project = project_id,\n",
    "        region = gcp_region,\n",
    "        bq_historic_raw_url = bq_historic_raw_url,\n",
    "        bq_historic_stage_url = bq_historic_stage_url\n",
    "        \n",
    "    )\n",
    "    \n",
    "    bq_current_ml = bq_current_raw_to_stage(\n",
    "        project = project_id,\n",
    "        region = gcp_region,\n",
    "        bq_current_raw_url = bq_current_raw_url,\n",
    "        bq_current_stage_url = bq_current_stage_url,\n",
    "        stage_data_bucket = stage_data_bucket\n",
    "        \n",
    "    )\n",
    "    \n",
    "    dataframe = get_chicago_data(project = project_id,\n",
    "                                 region = gcp_region,\n",
    "                                 bq_source_url = bq_stage_ml.output,\n",
    "                                 stage_data_bucket = stage_data_bucket)\n",
    "    \n",
    "    train_lr_op = train_lr_chicago_op(dataframe.outputs['dataset_train'],\n",
    "                                         project = project_id,\n",
    "                                         location = gcp_region)\n",
    "    \n",
    "    \n",
    "    train_rf_op = train_rf_chicago_op(dataframe.outputs['dataset_train'],\n",
    "                                         project = project_id,\n",
    "                                         location = gcp_region)\n",
    "    \n",
    "    model_selection = model_evaluation(\n",
    "        val_set = dataframe.outputs['dataset_val'],\n",
    "        lr_chicago_model = train_lr_op.outputs['model'],\n",
    "        rf_chicago_model = train_rf_op.outputs['model'],\n",
    "    \n",
    "    )\n",
    "    \n",
    "    hp_search = best_model_hp_tuning(\n",
    "        project = project_id,\n",
    "        region = gcp_region,\n",
    "        stage_data_bucket = stage_data_bucket,\n",
    "        timestamp = dataframe.outputs['timestamp'],\n",
    "        winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "        trials = trials,\n",
    "        parallel_trials = parallel_trials,\n",
    "        rf_hp_image = rf_hp_image,\n",
    "        lr_hp_image = lr_hp_image,\n",
    "        hp_train_machine = hp_train_machine\n",
    "    )\n",
    "    \n",
    "    best_model = train_best_model_op(\n",
    "        dataset_train = dataframe.outputs['dataset_train'], \n",
    "        dataset_val = dataframe.outputs['dataset_val'],\n",
    "        project = project_id,\n",
    "        location = gcp_region,\n",
    "        winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "        parameters = hp_search.outputs['model_spec']\n",
    "    )\n",
    "    \n",
    "    best_model_eval_decision = best_model_evaluation(\n",
    "        test_set = dataframe.outputs['dataset_test'],\n",
    "        winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "        best_model = best_model.outputs['model'],\n",
    "        threshold = threshold\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        best_model_eval_decision.outputs['dep_decision']=='true',\n",
    "        name = 'predict_decision'\n",
    "    ):\n",
    "        predict_op = upload_model_to_vertex_and_batch_prediction(\n",
    "            project = project_id,\n",
    "            region = gcp_region,\n",
    "            serving_container = serving_container,\n",
    "            trained_model = best_model.outputs['model'],\n",
    "            winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "            gcs_predict_source = bq_current_ml.outputs['gcs_predict_source'],\n",
    "            gcs_predict_dest = f'gs://{stage_data_bucket}'\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb9b67-6c38-487a-b992-e6cd5e4d6582",
   "metadata": {},
   "source": [
    "El compilador arroja como resultado un template en formato json que puede ser reutilizado en otras ejecuciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f970e73-28c1-42bc-96de-d5d208b74052",
   "metadata": {},
   "source": [
    "The compiler creates a template in json format that can be reused in other executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342fef71-d871-4079-a3ce-a9735b9c3084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='chicago-taxi-pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daebd901-e5f0-412b-852c-4d1f39ece99a",
   "metadata": {},
   "source": [
    "El ID tiene que ser √∫nico, y es en esta parte donde se puede habilitar el cach√© para reducir costos. Por default es *true*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47745eb9-4903-4536-b5af-2d06e6eed8be",
   "metadata": {},
   "source": [
    "The ID must be unique, and here the cache can be enabled to save costs. The default value is *true*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc7816f-1926-40a7-b8aa-5a832a752a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"chicago-taxi-pipeline\",\n",
    "    template_path=\"chicago-taxi-pipeline.json\",\n",
    "    job_id=\"chicago-taxi-pipeline-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=ENABLE_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da4bb65-6c7f-47c1-82c5-83a1cdcd52bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/1085281337041/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220315175301\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/1085281337041/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220315175301')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/chicago-taxi-pipeline-20220315175301?project=1085281337041\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1085281337041/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220315175301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1085281337041/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220315175301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1085281337041/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220315175301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1085281337041/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220315175301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1085281337041/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220315175301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1085281337041/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220315175301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/1085281337041/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220315175301 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [best-model-hp-tuning].; Job (project_id = teco-prod-adam-dev-826c, job_id = 4294688020046544896) is failed due to the above error.; Failed to handle the job: {project_number = 1085281337041, job_id = 4294688020046544896}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9839/464395014.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"View Pipeline Job:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dashboard_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [best-model-hp-tuning].; Job (project_id = teco-prod-adam-dev-826c, job_id = 4294688020046544896) is failed due to the above error.; Failed to handle the job: {project_number = 1085281337041, job_id = 4294688020046544896}\"\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25013d48-2faa-4635-bed4-9025c8389b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"google.api_core.exceptions.Forbidden: 403 GET https://storage.googleapis.com/storage/v1/b/chicago_taxi_stage?projection=noAcl&prettyPrint=false: service-1085281337041@gcp-sa-aiplatform-cc.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a167db-1a5a-4831-92a4-bc25c6ba34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud services enable compute.googleapis.com         \\\n",
    "#                        containerregistry.googleapis.com  \\\n",
    "#                        aiplatform.googleapis.com  \\\n",
    "#                        cloudbuild.googleapis.com \\\n",
    "#                        cloudfunctions.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1674a20b-1817-49b4-9849-4c1ffa256aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.projects.describe) argument PROJECT_ID_OR_NUMBER: Must be specified.\n",
      "Usage: gcloud projects describe PROJECT_ID_OR_NUMBER [optional flags]\n",
      "  optional flags may be  --help\n",
      "\n",
      "For detailed information on this command and its flags, run:\n",
      "  gcloud projects describe --help\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects describe $GOOGLE_CLOUD_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8edfee6-e3b3-443f-8e04-e587a8d8196f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $GOOGLE_CLOUD_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0745027b-09ff-4192-86bb-caca693a35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "bqclient = bigquery.Client()\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40564ee6-d7a6-4d6c-bd7e-ff0fd381c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE_DATA_BUCKET = 'teco-prod-adam-dev-826c-chicago_taxi_stage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f58e079-738b-4767-b36c-078bc2bc717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = 'data/chicago_taxi_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "246c8c34-9276-4759-b6fd-0513c434b77c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/chicago_taxi_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9839/3151689261.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcsclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTAGE_DATA_BUCKET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_to_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mdownload_to_filename\u001b[0;34m(self, filename, client, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_require_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m                 client.download_blob_to_file(\n\u001b[1;32m   1284\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/chicago_taxi_train.csv'"
     ]
    }
   ],
   "source": [
    "gcsclient = storage.Client() # tal vez vaya stage_data_bucket\n",
    "bucket = gcsclient.get_bucket(STAGE_DATA_BUCKET)\n",
    "blob = bucket.blob(TRAIN_DATA_PATH)\n",
    "blob.download_to_filename(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49d14c07-8156-4f59-a1cb-65a0b1a7a388",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hola/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9839/2781318045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_to_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hola/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mdownload_to_filename\u001b[0;34m(self, filename, client, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_require_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m                 client.download_blob_to_file(\n\u001b[1;32m   1284\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hola/test.csv'"
     ]
    }
   ],
   "source": [
    "blob.download_to_filename('test.csv')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
