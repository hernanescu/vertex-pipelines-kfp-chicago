{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3109952b-ddd7-49d2-a4da-3183e757ac9e",
   "metadata": {},
   "source": [
    "# Vertex Pipelines - A Serverless framework for MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9aaf1-b8fc-4532-9a8c-d682321cd90c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd984ac-7ab1-454a-bac9-fbbba93d7cee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  vertex-testing-327520\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Obtener el project ID\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e83c04-8ac8-4f76-8a42-e6f5a1a21c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "# variables de entorno: PATH local, region de GCP y timestamp\n",
    "\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "REGION=\"us-central1\" # disponibilidad completa de Vertex / Complete Vertex availability\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345caf8f-187c-41c9-8e38-b715e6c55060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros cloud (almacenamiento de objetos, outputs, etc)\n",
    "# cloud environment parameters (object storage, outputs, etc)\n",
    "STAGE_DATA_BUCKET = f'{PROJECT_ID}-chicago_taxi_stage'\n",
    "PIPELINE_BUCKET = f'{PROJECT_ID}-chicago_taxi_pipelines'\n",
    "PIPELINE_ROOT = f\"gs://{PIPELINE_BUCKET}/pipeline_root/\"\n",
    "\n",
    "# configuracion de componentes de ml\n",
    "# ML components configurations\n",
    "SERVING_CONTAINER = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'\n",
    "MACHINE_TYPE = 'n1-standard-16'\n",
    "\n",
    "# configuracion de intentos de tuneo de hiperparametros\n",
    "# configuration of hp tuning job trials\n",
    "HP_TRIALS = 10\n",
    "PARALLEL_TRIALS = 3\n",
    "\n",
    "# umbral de aceptabilidad para prediccion batch\n",
    "# acceptability threshold for batch prediction\n",
    "THRESHOLD = 0.8\n",
    "\n",
    "# habilitar cache en el pipeline para ahorrar costos\n",
    "# enable cache in pipeline execution to prevent costs\n",
    "ENABLE_CACHE = True\n",
    "\n",
    "# rutas de acceso a los datasets de train, val y test\n",
    "# access paths for train, val and test datasets\n",
    "TRAIN_DATA_PATH = 'chicago_taxi_train.csv'\n",
    "VAL_DATA_PATH = 'chicago_taxi_val.csv'\n",
    "TEST_DATA_PATH = 'chicago_taxi_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3418e6-0bda-49e1-9e66-b476df15dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery: definiciones de variables, pueden ser facilmente reemplazadas para adaptarse a otros propositos\n",
    "# BigQuery variable definitions: those can be easily changed to suit another purpose\n",
    "\n",
    "BQ_DATASET_HISTORIC_NAME = 'chicago_taxi_historic'\n",
    "BQ_DATASET_CURRENT_NAME = 'chicago_taxi_current'\n",
    "\n",
    "BQ_HISTORIC_RAW = 'raw'\n",
    "BQ_HISTORIC_STAGE = 'stage_ml'\n",
    "\n",
    "BQ_CURRENT_RAW = 'raw'\n",
    "BQ_CURRENT_STAGE = 'stage_ml'\n",
    "\n",
    "BQ_CURRENT_RAW_URL = f\"{PROJECT_ID}.{BQ_DATASET_CURRENT_NAME}.{BQ_CURRENT_RAW}\"\n",
    "BQ_CURRENT_STAGE_URL = f\"{PROJECT_ID}.{BQ_DATASET_CURRENT_NAME}.{BQ_CURRENT_STAGE}\"\n",
    "\n",
    "BQ_HISTORIC_RAW_URL = f\"{PROJECT_ID}.{BQ_DATASET_HISTORIC_NAME}.{BQ_HISTORIC_RAW}\"\n",
    "BQ_HISTORIC_STAGE_URL = f\"{PROJECT_ID}.{BQ_DATASET_HISTORIC_NAME}.{BQ_HISTORIC_STAGE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ad1f8-fee3-41b5-8a53-756be97ab822",
   "metadata": {},
   "source": [
    "### Librerias - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b73a7c-dbb8-41e8-96c1-ff756358b212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  vertex-testing-327520\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import kfp\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath, ClassificationMetrics\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform, bigquery\n",
    "\n",
    "# We'll use this namespace for metadata querying\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental.custom_job.utils import create_custom_training_job_op_from_component\n",
    "\n",
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a7951b-6d7e-48ca-8934-33ba5bf25fd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Definicion de componentes - Component definition\n",
    "\n",
    "Hay una carpeta llamada *components* donde se encuentra una notebook que genera los distintos componentes necesarios para el pipeline y sus correspondientes yaml. Dada las características de este pipeline, muchos componentes requieren armarse de manera customizada y no depender de los preconstruidos.\n",
    "\n",
    "There's a folder named *components* with a notebook that generates the needed components for the pipeline and their corresponding yaml files. Given the complexity of this pipeline, many components needs to be custom built and not use the pre-built one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af34607a-8023-4d5d-a187-8afc438bfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_evaluation = kfp.components.load_component_from_file('./components/best_model_evaluation.yaml')\n",
    "best_model_hp_tuning = kfp.components.load_component_from_file('./components/best_model_hp_tuning.yaml')\n",
    "bq_current_raw_to_stage = kfp.components.load_component_from_file('./components/bq_current_raw_to_stage.yaml')\n",
    "bq_historic_raw_to_stage = kfp.components.load_component_from_file('./components/bq_historic_raw_to_stage.yaml')\n",
    "get_chicago_data = kfp.components.load_component_from_file('./components/get_chicago_data.yaml')\n",
    "model_evaluation = kfp.components.load_component_from_file('./components/model_evaluation.yaml')\n",
    "train_best_model = kfp.components.load_component_from_file('./components/train_best_model.yaml')\n",
    "train_lr_chicago = kfp.components.load_component_from_file('./components/train_lr_chicago.yaml')\n",
    "train_rf_chicago = kfp.components.load_component_from_file('./components/train_rf_chicago.yaml')\n",
    "upload_model_to_vertex_and_batch_prediction = kfp.components.load_component_from_file('./components/upload_model_to_vertex_and_batch_prediction.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1ee70-7119-4d8b-8f86-ebcff4317845",
   "metadata": {},
   "source": [
    "El componente de entrenamiento tiene que ser convertido en jobs para que pueda funcionar dentro del contexto de la suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a730fd-7d9b-4e62-91da-84918c501e88",
   "metadata": {},
   "source": [
    "The training components need to be turned into jobs to be performed within the context of the suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba882edb-d1c2-4975-b927-7eeb3a824907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion del job usando el componente custom previamente creado\n",
    "# Job definition using previously created custom component\n",
    "\n",
    "train_rf_chicago_op = create_custom_training_job_op_from_component(\n",
    "    train_rf_chicago,\n",
    "    machine_type=MACHINE_TYPE\n",
    ")\n",
    "\n",
    "train_best_model_op = create_custom_training_job_op_from_component(\n",
    "    train_best_model,\n",
    "    machine_type=MACHINE_TYPE\n",
    ")\n",
    "\n",
    "train_lr_chicago_op = create_custom_training_job_op_from_component(\n",
    "    train_lr_chicago,\n",
    "    machine_type=MACHINE_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28392a4f-a640-48eb-a7a1-2f364fd0a13d",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676fd8d-5ae1-4337-bed9-936a7c7954d8",
   "metadata": {},
   "source": [
    "La definición del pipeline es esencialmente una función. Desde aquí se orquesta todo el proceso, pasándole los parámetros necesarios para que cada elemento del pipeline pueda performar. De manera similar a un orquestador, cada una de las funciones es un paso que tiene *inputs* y *outputs*, así como también algunos parámetros y valores que opcionalmente pueden salirse del flujo natural: esto resulta de mucha utilidad para almacenar métricas o valores a partir de los cuales se tomarán decisiones a lo largo del camino. \n",
    "\n",
    "Esto se evidencia especialmente en la predicción batch, que utiliza el objeto Condition a partir del cual, si el modelo entrenado cumple con un cierto standard (en este caso, la métrica F1) el trabajo se ejecuta, y en caso contrario, se detiene todo el proceso.\n",
    "\n",
    "En cuanto a las métricas, estas pueden ser consultadas en distintas etapas del proceso, así como también visualizarse en la interfaz gráfica de Vertex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7aa5b-dc8f-44ea-b30a-ca1db49bd223",
   "metadata": {},
   "source": [
    "The pipeline definition is essentially a function. From here the entire process is orchestrated, passing along needed parameters so each element can trigger. Each of the functions is a step with inputs and outputs, as well as some parameters and values that can optionally skip or exit the flow: this is very useful to store metrics or values upon which certain parts of the process are triggered or not.\n",
    "\n",
    "This is specially notable in batch prediction job, based on the dsl.Condition object that executes jobs according to a certain criteria met. In this case, if the trained model has an F1 score above the threshold, the batch prediction gets triggered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64eccf5-9ba4-4549-aa50-7d2f671affb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pipeline mio\n",
    "@dsl.pipeline(name='chicago-taxi-pipeline',\n",
    "                pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    stage_data_bucket: str = STAGE_DATA_BUCKET,\n",
    "    pipelines_bucket: str = PIPELINE_BUCKET,\n",
    "    pipeline_root: str = PIPELINE_ROOT,\n",
    "    serving_container: str = SERVING_CONTAINER,\n",
    "    machine_type: str = MACHINE_TYPE,\n",
    "    trials: int = HP_TRIALS,\n",
    "    parallel_trials: int = PARALLEL_TRIALS,\n",
    "    bq_current_raw_url: str = BQ_CURRENT_RAW_URL,\n",
    "    bq_current_stage_url: str = BQ_CURRENT_STAGE_URL,\n",
    "    bq_historic_raw_url: str = BQ_HISTORIC_RAW_URL,\n",
    "    bq_historic_stage_url: str = BQ_HISTORIC_STAGE_URL,\n",
    "    threshold: float = THRESHOLD,\n",
    "    enable_cache: bool = ENABLE_CACHE\n",
    "    \n",
    "    \n",
    "):\n",
    "    \n",
    "    bq_stage_ml = bq_historic_raw_to_stage(\n",
    "        project = project_id,\n",
    "        region = gcp_region,\n",
    "        bq_historic_raw_url = bq_historic_raw_url,\n",
    "        bq_historic_stage_url = bq_historic_stage_url\n",
    "        \n",
    "    )\n",
    "    \n",
    "    bq_current_ml = bq_current_raw_to_stage(\n",
    "        project = project_id,\n",
    "        region = gcp_region,\n",
    "        bq_current_raw_url = bq_current_raw_url,\n",
    "        bq_current_stage_url = bq_current_stage_url,\n",
    "        stage_data_bucket = stage_data_bucket\n",
    "        \n",
    "    )\n",
    "    \n",
    "    dataframe = get_chicago_data(project = project_id,\n",
    "                                 region = gcp_region,\n",
    "                                 bq_source_url = bq_stage_ml.output,\n",
    "                                 stage_data_bucket = stage_data_bucket)\n",
    "    \n",
    "    train_lr_op = train_lr_chicago_op(dataframe.outputs['dataset_train'],\n",
    "                                         project = project_id,\n",
    "                                         location = gcp_region)\n",
    "    \n",
    "    \n",
    "    train_rf_op = train_rf_chicago_op(dataframe.outputs['dataset_train'],\n",
    "                                         project = project_id,\n",
    "                                         location = gcp_region)\n",
    "    \n",
    "    model_selection = model_evaluation(\n",
    "        val_set = dataframe.outputs['dataset_val'],\n",
    "        lr_chicago_model = train_lr_op.outputs['model'],\n",
    "        rf_chicago_model = train_rf_op.outputs['model'],\n",
    "    \n",
    "    )\n",
    "    \n",
    "    hp_search = best_model_hp_tuning(\n",
    "        project = project_id,\n",
    "        region = gcp_region,\n",
    "        stage_data_bucket = stage_data_bucket,\n",
    "        winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "        trials = trials,\n",
    "        parallel_trials = parallel_trials\n",
    "    )\n",
    "    \n",
    "    best_model = train_best_model_op(\n",
    "        dataset_train = dataframe.outputs['dataset_train'], \n",
    "        dataset_val = dataframe.outputs['dataset_val'],\n",
    "        project = project_id,\n",
    "        location = gcp_region,\n",
    "        winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "        parameters = hp_search.outputs['model_spec']\n",
    "    )\n",
    "    \n",
    "    best_model_eval_decision = best_model_evaluation(\n",
    "        test_set = dataframe.outputs['dataset_test'],\n",
    "        winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "        best_model = best_model.outputs['model'],\n",
    "        threshold = threshold\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        best_model_eval_decision.outputs['dep_decision']=='true',\n",
    "        name = 'predict_decision'\n",
    "    ):\n",
    "        predict_op = upload_model_to_vertex_and_batch_prediction(\n",
    "            project = project_id,\n",
    "            region = gcp_region,\n",
    "            serving_container = serving_container,\n",
    "            trained_model = best_model.outputs['model'],\n",
    "            winning_model_name = model_selection.outputs['winning_model_name'],\n",
    "            gcs_predict_source = bq_current_ml.outputs['gcs_predict_source'],\n",
    "            gcs_predict_dest = f'gs://{stage_data_bucket}'\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb9b67-6c38-487a-b992-e6cd5e4d6582",
   "metadata": {},
   "source": [
    "El compilador arroja como resultado un template en formato json que puede ser reutilizado en otras ejecuciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f970e73-28c1-42bc-96de-d5d208b74052",
   "metadata": {},
   "source": [
    "The compiler creates a template in json format that can be reused in other executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342fef71-d871-4079-a3ce-a9735b9c3084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='chicago-taxi-pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daebd901-e5f0-412b-852c-4d1f39ece99a",
   "metadata": {},
   "source": [
    "El ID tiene que ser único, y es en esta parte donde se puede habilitar el caché para reducir costos. Por default es *true*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47745eb9-4903-4536-b5af-2d06e6eed8be",
   "metadata": {},
   "source": [
    "The ID must be unique, and here the cache can be enabled to save costs. The default value is *true*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc7816f-1926-40a7-b8aa-5a832a752a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"chicago-taxi-pipeline\",\n",
    "    template_path=\"chicago-taxi-pipeline.json\",\n",
    "    job_id=\"chicago-taxi-pipeline-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=ENABLE_CACHE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da4bb65-6c7f-47c1-82c5-83a1cdcd52bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/155345500736/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220210140111\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/155345500736/locations/us-central1/pipelineJobs/chicago-taxi-pipeline-20220210140111')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/chicago-taxi-pipeline-20220210140111?project=155345500736\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25013d48-2faa-4635-bed4-9025c8389b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a167db-1a5a-4831-92a4-bc25c6ba34af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
