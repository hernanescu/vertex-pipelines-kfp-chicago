{
  "pipelineSpec": {
    "components": {
      "comp-best-model-evaluation": {
        "executorLabel": "exec-best-model-evaluation",
        "inputDefinitions": {
          "artifacts": {
            "best_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            },
            "test_set": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "winning_model_name": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "threshold": {
              "type": "DOUBLE"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "best_model_kpi": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "dep_decision": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-best-model-hp-tuning": {
        "executorLabel": "exec-best-model-hp-tuning",
        "inputDefinitions": {
          "artifacts": {
            "winning_model_name": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "hp_train_machine": {
              "type": "STRING"
            },
            "lr_hp_image": {
              "type": "STRING"
            },
            "parallel_trials": {
              "type": "INT"
            },
            "project": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "rf_hp_image": {
              "type": "STRING"
            },
            "stage_data_bucket": {
              "type": "STRING"
            },
            "timestamp": {
              "type": "STRING"
            },
            "trials": {
              "type": "INT"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "kpi": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "model_name": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "model_spec": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-bq-current-raw-to-stage-ml": {
        "executorLabel": "exec-bq-current-raw-to-stage-ml",
        "inputDefinitions": {
          "parameters": {
            "bq_current_raw_url": {
              "type": "STRING"
            },
            "bq_current_stage_url": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "stage_data_bucket": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "gcs_predict_source": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-bq-historic-raw-to-stage-ml": {
        "executorLabel": "exec-bq-historic-raw-to-stage-ml",
        "inputDefinitions": {
          "parameters": {
            "bq_historic_raw_url": {
              "type": "STRING"
            },
            "bq_historic_stage_url": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-condition-predict-decision-1": {
        "dag": {
          "tasks": {
            "upload-model-to-vertex-and-batch-prediction": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-upload-model-to-vertex-and-batch-prediction"
              },
              "inputs": {
                "artifacts": {
                  "trained_model": {
                    "componentInputArtifact": "pipelineparam--train-best-model-model"
                  },
                  "winning_model_name": {
                    "componentInputArtifact": "pipelineparam--model-evaluation-winning_model_name"
                  }
                },
                "parameters": {
                  "gcs_predict_dest": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "gs://{{$.inputs.parameters['pipelineparam--stage_data_bucket']}}"
                      }
                    }
                  },
                  "gcs_predict_source": {
                    "componentInputParameter": "pipelineparam--bq-current-raw-to-stage-ml-gcs_predict_source"
                  },
                  "pipelineparam--stage_data_bucket": {
                    "componentInputParameter": "pipelineparam--stage_data_bucket"
                  },
                  "project": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "region": {
                    "componentInputParameter": "pipelineparam--gcp_region"
                  },
                  "serving_container": {
                    "componentInputParameter": "pipelineparam--serving_container"
                  }
                }
              },
              "taskInfo": {
                "name": "upload-model-to-vertex-and-batch-prediction"
              }
            }
          }
        },
        "inputDefinitions": {
          "artifacts": {
            "pipelineparam--model-evaluation-winning_model_name": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            },
            "pipelineparam--train-best-model-model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "pipelineparam--best-model-evaluation-dep_decision": {
              "type": "STRING"
            },
            "pipelineparam--bq-current-raw-to-stage-ml-gcs_predict_source": {
              "type": "STRING"
            },
            "pipelineparam--gcp_region": {
              "type": "STRING"
            },
            "pipelineparam--project_id": {
              "type": "STRING"
            },
            "pipelineparam--serving_container": {
              "type": "STRING"
            },
            "pipelineparam--stage_data_bucket": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-get-chicago-data": {
        "executorLabel": "exec-get-chicago-data",
        "inputDefinitions": {
          "parameters": {
            "bq_source_url": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "stage_data_bucket": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "dataset_test": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "dataset_train": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "dataset_val": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "timestamp": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-model-evaluation": {
        "executorLabel": "exec-model-evaluation",
        "inputDefinitions": {
          "artifacts": {
            "lr_chicago_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            },
            "rf_chicago_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            },
            "val_set": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "lr_kpi": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "rf_kpi": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "winning_model_name": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-train-best-model": {
        "executorLabel": "exec-train-best-model",
        "inputDefinitions": {
          "artifacts": {
            "dataset_train": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "dataset_val": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "parameters": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            },
            "winning_model_name": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "base_output_directory": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "network": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "service_account": {
              "type": "STRING"
            },
            "tensorboard": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "gcp_resources": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-train-lr-chicago": {
        "executorLabel": "exec-train-lr-chicago",
        "inputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "base_output_directory": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "network": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "service_account": {
              "type": "STRING"
            },
            "tensorboard": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "gcp_resources": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-train-rf-chicago": {
        "executorLabel": "exec-train-rf-chicago",
        "inputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "base_output_directory": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "network": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "service_account": {
              "type": "STRING"
            },
            "tensorboard": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "gcp_resources": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-upload-model-to-vertex-and-batch-prediction": {
        "executorLabel": "exec-upload-model-to-vertex-and-batch-prediction",
        "inputDefinitions": {
          "artifacts": {
            "trained_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            },
            "winning_model_name": {
              "artifactType": {
                "schemaTitle": "system.Artifact",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "gcs_predict_dest": {
              "type": "STRING"
            },
            "gcs_predict_source": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "region": {
              "type": "STRING"
            },
            "serving_container": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-best-model-evaluation": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "best_model_evaluation"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'sklearn' 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef best_model_evaluation(\n    test_set:  Input[Dataset],\n    winning_model_name: Input[Artifact], # tiene que saber qu\u00e9 objeto instanciar adentro\n    best_model: Input[Model], # y ac\u00e1 tomar los datos para cargarlo\n    best_model_kpi: Output[Metrics],\n    threshold: float\n)-> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):\n\n    '''\n    Toma el mejor modelo entrenado y lo evalua usando el set de test. Si pasa un cierto umbral, devuelve \"true\" y marca el inicio del proximo paso, si no lo hace, el proceso se detiene.\n    Takes the trained best model and evaluates it using the test set. If it passes a certain threshold, it returns \"true\" and sets the beginning of the next step, if it doesn't, the process halts.\n\n    '''\n\n\n    #from xgboost import XGBClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n    import pandas as pd\n    import logging \n    import pickle\n    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, f1_score\n    import json\n    import typing\n\n    model_dict = winning_model_name.metadata\n    WINNING_MODEL_NAME = model_dict.get('model')\n\n    #TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n    if WINNING_MODEL_NAME == 'LogisticRegression':\n        model = LogisticRegression()\n    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n        model = RandomForestClassifier()\n    else:\n        model = None \n\n\n    file_name = best_model.path + \".pkl\"\n    with open(file_name, 'rb') as file:  \n        model = pickle.load(file)\n\n    data = pd.read_csv(test_set.path+\".csv\")\n    y_test = data.drop(columns=[\"target\"])\n    y_target=data.target\n\n\n    y_pred = model.predict(y_test)\n\n\n\n    # evaluacion de modelo \n    f1_value = f1_score(data.target, y_pred.round())\n\n    # toma decision\n\n    if f1_value >= threshold:\n        dep_decision = 'true'\n    else:\n        dep_decision = 'false'\n\n    # guarda la metrica\n    best_model_kpi.log_metric(\"f1_score\", float(f1_value))\n\n    return (dep_decision, )\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-best-model-hp-tuning": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "best_model_hp_tuning"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'scikit-learn==1.0.0' 'google-cloud-bigquery' 'google-cloud-bigquery-storage' 'google-cloud-aiplatform' 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef best_model_hp_tuning(\n    project: str,\n    region: str,\n    stage_data_bucket: str,\n    timestamp: str,\n    winning_model_name: Input[Artifact],\n    model_spec: Output[Artifact],\n    trials: int,\n    parallel_trials: int,\n    rf_hp_image: str,\n    lr_hp_image: str,\n    hp_train_machine: str,\n    kpi: Output[Metrics],\n    model_name: Output[Metrics]\n): \n    '''\n    Tuneo de hiperparametros. Toma el nombre del modelo ganador y utiliza la imagen de Docker correspondiente para lanzar un job de entrenamiento. Los hiperparametros obtenidos son pasados como componentes.\n    Hyperparameter tuning. Takes the name of the winning model and uses the corresponding Docker image to launch a training job. The chosen hyperparameters are passed as a component.\n    '''\n    from google.cloud import aiplatform\n    from google.cloud.aiplatform import hyperparameter_tuning as hpt\n    from google.protobuf.json_format import MessageToDict\n    import pandas as pd\n    from google.cloud import storage\n    from google.cloud.storage import Blob\n    import os\n\n    HP_TRIALS_DIR = 'hp_trials'\n    TIMESTAMP = timestamp\n\n    HP_TRIALS_NAME = f'{TIMESTAMP}_hp_trials.csv'\n\n    if not os.path.exists(HP_TRIALS_DIR):\n        os.mkdir(HP_TRIALS_DIR)\n\n    HP_DATA_PATH = os.path.join(HP_TRIALS_DIR, HP_TRIALS_NAME)\n\n    aiplatform.init(project = project,\n                    location = region)\n\n    # train images definition\n    RF_HP_IMAGE = rf_hp_image\n    LR_HP_IMAGE = lr_hp_image\n\n    # get model name\n    model_dict = winning_model_name.metadata\n\n    WINNING_MODEL_NAME = model_dict.get('model')\n\n\n\n    if WINNING_MODEL_NAME == 'LogisticRegression':\n        WINNING_MODEL_IMAGE = LR_HP_IMAGE\n    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n        WINNING_MODEL_IMAGE = RF_HP_IMAGE\n    else:\n        WINNING_MODEL_IMAGE = None \n\n    worker_pool_specs = [{\n    \"machine_spec\": {\n        \"machine_type\": hp_train_machine,\n        #\"accelerator_type\": \"NVIDIA_TESLA_T4\",\n        #\"accelerator_count\": 1\n    },\n    \"replica_count\": 1,\n    \"container_spec\": {\n        \"image_uri\": WINNING_MODEL_IMAGE\n    }\n    }]\n\n\n    metric_spec={'f1_score':'maximize'}\n\n    # Dictionary representing parameters to optimize.\n    # The dictionary key is the parameter_id, which is passed into your training\n    # job as a command line argument,\n    # And the dictionary value is the parameter specification of the metric.\n\n    lr_parameter_spec = {\n        \"penalty\": hpt.CategoricalParameterSpec(values=['l1', 'l2']),\n        \"C\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n        \"solver\": hpt.CategoricalParameterSpec(values=['saga', 'liblinear'])\n    }\n\n    rf_parameter_spec = {\n        \"max_leaf_nodes\": hpt.DiscreteParameterSpec(values=[4, 8, 10], scale=None),\n        \"max_depth\": hpt.DiscreteParameterSpec(values=[4, 8, 10], scale=None),\n        \"n_estimators\": hpt.DiscreteParameterSpec(values=[5, 7, 9], scale=None)\n    }\n\n\n    if WINNING_MODEL_NAME == 'LogisticRegression':\n        parameter_spec = lr_parameter_spec\n    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n        parameter_spec = rf_parameter_spec\n    else:\n        parameter_spec = None \n\n    DISPLAY_NAME = f\"{WINNING_MODEL_NAME}-{TIMESTAMP}\"\n\n    hp_custom_job = aiplatform.CustomJob(display_name=DISPLAY_NAME,\n                                         worker_pool_specs=worker_pool_specs,\n                                         staging_bucket=f'gs://{stage_data_bucket}')\n\n\n    hp_job = aiplatform.HyperparameterTuningJob(\n        display_name=DISPLAY_NAME,\n        custom_job=hp_custom_job,\n        metric_spec=metric_spec,\n        parameter_spec=parameter_spec, \n        max_trial_count=trials,\n        parallel_trial_count=parallel_trials\n    )\n\n    hp_job.run()\n\n    # helper function\n    def get_trials_as_df(trials):\n        results = []\n        for trial in trials:\n            row = {}\n            t = MessageToDict(trial._pb)\n            # print(t)\n            row[\"Trial ID\"], row[\"Status\"], row[\"Start time\"], row[\"End time\"] = (\n                t[\"id\"],\n                t[\"state\"],\n                t[\"startTime\"],\n                t.get(\"endTime\", None),\n            )\n\n            for param in t[\"parameters\"]:\n                row[param[\"parameterId\"]] = param[\"value\"]\n\n            if t[\"state\"] == \"SUCCEEDED\":\n                row[\"Training step\"] = t[\"finalMeasurement\"][\"stepCount\"]\n                for metric in t[\"finalMeasurement\"][\"metrics\"]:\n                    row[metric[\"metricId\"]] = metric[\"value\"]\n            results.append(row)\n\n        _df = pd.DataFrame(results)\n        return _df\n\n    df_trials = get_trials_as_df(hp_job.trials)\n\n    df_trials.to_csv(HP_DATA_PATH)\n\n    # get trial id of the best run from the Trials\n    best_trial_id = df_trials.loc[df_trials[\"f1_score\"].idxmax()][\"Trial ID\"]\n    # get best run definition\n    best_run = df_trials[df_trials['Trial ID']==best_trial_id]\n\n    # retrieve parameters tuned in this run\n    param_names = []\n\n    for i in parameter_spec.keys():\n        param_names.append(i)\n\n    best_run_to_dict = best_run[param_names]\n    best_run_parameters = best_run_to_dict.to_dict('r')[0]\n\n    model_spec.metadata = best_run_parameters\n\n    kpi_acc = float(best_run['f1_score'])\n\n    kpi.log_metric(\"f1_score\", float(kpi_acc))\n    model_name.log_metric('model', WINNING_MODEL_NAME)\n\n    # subir el df con los resultados al bucket\n    # upload df with results to bucket\n\n    gcsclient = storage.Client() \n    bucket = gcsclient.get_bucket(stage_data_bucket)\n\n    blob_hp = bucket.blob(HP_DATA_PATH)\n    blob_hp.upload_from_filename(HP_DATA_PATH)\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-bq-current-raw-to-stage-ml": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bq_current_raw_to_stage_ml"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'pyarrow' 'scikit-learn==1.0.0' 'google-cloud-bigquery' 'google-cloud-bigquery-storage' 'pandas-gbq' 'google-cloud-aiplatform' 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bq_current_raw_to_stage_ml(\n    project: str,\n    region: str,\n    bq_current_raw_url: str,\n    bq_current_stage_url: str,\n    stage_data_bucket: str,\n    gcs_predict_source: OutputPath(str)\n):\n    '''\n    Toma el dataset de BigQuery establecido como el presente y lo procesa, colocandolo en la tabla stage_ml. Tambien sube una version timestamped de la data en csv.\n    Takes the Bigquery Dataset established as the present and processes it, placing it in the stage_ml table. It also uploads a timestamped csv version of the data.\n    '''\n    import pandas as pd\n    import pandas_gbq\n    import numpy as np\n    from google.cloud import bigquery\n    from google.cloud import storage\n    from google.cloud.storage import Blob\n    from google.cloud import aiplatform\n\n    aiplatform.init(project = project,\n                    location = region)\n\n    ### get data from bq_source\n    bqclient = bigquery.Client(project = project, location = region)\n\n\n    # Download a table\n    table = bigquery.TableReference.from_string(\n        bq_current_raw_url\n    )\n    rows = bqclient.list_rows(\n        table\n    )\n    data = rows.to_dataframe(\n        create_bqstorage_client=True, # guarda ac\u00e1\n    )\n\n    # process\n\n    # eliminamos el target para simular la realidad / eliminate target to simulate reality\n    df = data[['trip_month', 'trip_day', 'trip_day_of_week', 'trip_hour', 'trip_seconds', 'trip_miles', 'payment_type', 'euclidean']]\n\n    df2 = pd.get_dummies(df, columns = ['payment_type'], drop_first = True)\n\n    df2.columns = df2.columns.str.replace(' ','_')\n\n    # upload to bq\n\n    df2.to_gbq(bq_current_stage_url,\n               project,\n               chunksize=None, \n               if_exists='replace', # el default tira error, aca queremos que siempre reemplace / default throws error, here we want it to replace always\n               table_schema=[{'name': 'trip_month','type': 'INTEGER'},\n                             {'name': 'trip_day','type': 'INTEGER'},\n                             {'name': 'trip_day_of_week','type': 'INTEGER'},\n                             {'name': 'trip_hour','type': 'INTEGER'},\n                             {'name': 'trip_seconds','type': 'INTEGER'},\n                             {'name': 'trip_miles','type': 'FLOAT'},\n                             {'name': 'euclidean','type': 'FLOAT'},\n                             #{'name': 'target','type': 'INTEGER'}, eliminamos el target para simular la realidad / eliminate target to simulate reality\n                             {'name': 'payment_type_Credit_Card','type': 'INTEGER'},\n                             {'name': 'payment_type_Dispute','type': 'INTEGER'},\n                             {'name': 'payment_type_Mobile','type': 'INTEGER'},\n                             {'name': 'payment_type_No_Charge','type': 'INTEGER'},\n                             {'name': 'payment_type_Prcard','type': 'INTEGER'},\n                             {'name': 'payment_type_Unknown','type': 'INTEGER'}\n                             ]\n    )\n\n    # ponerle a la data tambien un timestamp\n    # also timestamp the data    \n\n    from datetime import datetime\n    import os\n\n    DATA_DIR = 'batch_predict_data'\n\n    if not os.path.exists(DATA_DIR):\n        os.mkdir(DATA_DIR)\n\n    TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n    DATA_NAME = f\"predicted_data_{TIMESTAMP}.csv\"\n\n    DATA_PATH = os.path.join(DATA_DIR, DATA_NAME)\n\n    df2.to_csv(DATA_PATH, index = False)\n\n    gcsclient = storage.Client() \n    bucket = gcsclient.get_bucket(stage_data_bucket)\n\n    blob_train = bucket.blob(DATA_PATH)\n    blob_train.upload_from_filename(DATA_PATH)\n\n\n    GCS_PREDICT_SOURCE = f\"gs://{stage_data_bucket}/{DATA_PATH}\"\n\n    with open(gcs_predict_source, 'w') as f:\n              f.write(GCS_PREDICT_SOURCE)\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-bq-historic-raw-to-stage-ml": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bq_historic_raw_to_stage_ml"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'pyarrow' 'scikit-learn==1.0.0' 'google-cloud-bigquery' 'google-cloud-bigquery-storage' 'pandas-gbq' 'google-cloud-aiplatform' 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bq_historic_raw_to_stage_ml(\n    project: str,\n    region: str,\n    bq_historic_raw_url: str,\n    bq_historic_stage_url: str,\n\n) -> str:\n\n    '''\n    Toma el dataset de BigQuery establecido como el periodo historico y lo procesa, colocandolo en la tabla stage_ml.\n    Takes the Bigquery Dataset established as historic and processes it, placing it in the stage_ml table.\n    '''\n    import pandas as pd\n    import pandas_gbq\n    import numpy as np\n    from google.cloud import bigquery\n    from google.cloud import storage\n    from google.cloud.storage import Blob\n    from google.cloud import aiplatform\n\n    aiplatform.init(project = project,\n                    location = region)\n\n    ### get data from bq_source\n    bqclient = bigquery.Client(project = project, location = region)\n\n\n    # Download a table.\n    table = bigquery.TableReference.from_string(\n        bq_historic_raw_url\n    )\n    rows = bqclient.list_rows(\n        table\n    )\n    data = rows.to_dataframe(\n        create_bqstorage_client=True, \n    )\n\n    df = data[['trip_month', 'trip_day', 'trip_day_of_week', 'trip_hour', 'trip_seconds', 'trip_miles', 'payment_type', 'euclidean', 'tip_bin']]\n\n    df = df.rename(columns = {'tip_bin':'target'})\n\n    df2 = pd.get_dummies(df, columns = ['payment_type'], drop_first = True)\n\n    df2.columns = df2.columns.str.replace(' ','_')\n\n    df2.to_gbq(bq_historic_stage_url,\n               project,\n               chunksize=None, \n               if_exists='replace', # el default tira error, aca queremos que siempre reemplace / default throws error, here we want it to replace always\n               table_schema=[{'name': 'trip_month','type': 'INTEGER'},\n                             {'name': 'trip_day','type': 'INTEGER'},\n                             {'name': 'trip_day_of_week','type': 'INTEGER'},\n                             {'name': 'trip_hour','type': 'INTEGER'},\n                             {'name': 'trip_seconds','type': 'INTEGER'},\n                             {'name': 'trip_miles','type': 'FLOAT'},\n                             {'name': 'euclidean','type': 'FLOAT'},\n                             {'name': 'target','type': 'INTEGER'},\n                             {'name': 'payment_type_Credit_Card','type': 'INTEGER'},\n                             {'name': 'payment_type_Dispute','type': 'INTEGER'},\n                             {'name': 'payment_type_Mobile','type': 'INTEGER'},\n                             {'name': 'payment_type_No_Charge','type': 'INTEGER'},\n                             {'name': 'payment_type_Prcard','type': 'INTEGER'},\n                             {'name': 'payment_type_Unknown','type': 'INTEGER'}\n                             ]\n    )\n\n    URL_TO_GO = bq_historic_stage_url\n\n    return URL_TO_GO\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-get-chicago-data": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "get_chicago_data"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'pyarrow' 'scikit-learn==1.0.0' 'google-cloud-bigquery' 'google-cloud-bigquery-storage' 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef get_chicago_data(\n    project: str,\n    region: str,\n    bq_source_url: str,\n    stage_data_bucket: str,\n    dataset_train: Output[Dataset],\n    dataset_val: Output[Dataset],\n    dataset_test: Output[Dataset],\n    timestamp: OutputPath(str)\n):\n    '''\n    Toma los datos que se consideran historicos de la tabla de BQ y separa en train, validation y test. Ademas de pasarlos como componentes del pipeline, guarda una version de los datos en el bucket de stage.\n    Takes the data considered as historic from the BQ table and splits it into train, validation and test. Besides passing them as pipeline component, it stores a version of the data in the stage bucket.\n    '''\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split as tts\n    from google.cloud import bigquery\n    from google.cloud import storage\n    from google.cloud.storage import Blob\n    from datetime import datetime\n    import os\n    ## creamos la carpeta donde vamos a almacenar la data que persistiremos, respetando la estructura del bucket\n    ## create the folder where we store the persisted data, respecting the bucket structure\n\n    DATA_DIR = 'data'\n\n    if not os.path.exists(DATA_DIR):\n        os.mkdir(DATA_DIR)\n\n    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n    with open(timestamp, 'w') as f:\n              f.write(TIMESTAMP)\n\n    TRAIN_DATA_NAME = f'{TIMESTAMP}_chicago_taxi_train.csv'\n    VAL_DATA_NAME = f'{TIMESTAMP}_chicago_taxi_val.csv'\n    TEST_DATA_NAME = f'{TIMESTAMP}_chicago_taxi_test.csv'\n\n    TRAIN_DATA_PATH = os.path.join(DATA_DIR, TRAIN_DATA_NAME)\n    VAL_DATA_PATH = os.path.join(DATA_DIR, VAL_DATA_NAME)\n    TEST_DATA_PATH = os.path.join(DATA_DIR, TEST_DATA_NAME)\n\n\n    ### get data from bq_source\n    bqclient = bigquery.Client(project = project, location = region)\n\n\n    # Download the table.\n    table = bigquery.TableReference.from_string(\n        bq_source_url\n    )\n    rows = bqclient.list_rows(\n        table,\n\n    )\n    data = rows.to_dataframe(\n        create_bqstorage_client=True, # guarda ac\u00e1\n    )\n\n    # splits in train, val and test\n\n    train, test = tts(data, test_size=0.3)\n    train_data, val_data = tts(train, test_size = 0.2)\n\n\n\n    train_data.to_csv(TRAIN_DATA_PATH)\n    val_data.to_csv(VAL_DATA_PATH)\n    test.to_csv(TEST_DATA_PATH)\n\n    ### so far we have the paths, we have to upload them to the bucket / hasta aca est\u00e1n los csvs en los PATH, ahora lo subimos al bucket\n    gcsclient = storage.Client() \n    bucket = gcsclient.get_bucket(stage_data_bucket)\n\n    blob_train = bucket.blob(TRAIN_DATA_PATH)\n    blob_train.upload_from_filename(TRAIN_DATA_PATH)\n\n    blob_train = bucket.blob(VAL_DATA_PATH)\n    blob_train.upload_from_filename(VAL_DATA_PATH)\n\n    blob_test = bucket.blob(TEST_DATA_PATH)\n    blob_test.upload_from_filename(TEST_DATA_PATH)\n\n    train_data.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n    val_data.to_csv(dataset_val.path + \".csv\" , index=False, encoding='utf-8-sig')\n    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-model-evaluation": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "model_evaluation"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'sklearn' 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef model_evaluation(\n    val_set:  Input[Dataset],\n    lr_chicago_model: Input[Model],\n    rf_chicago_model: Input[Model],\n    lr_kpi: Output[Metrics],\n    rf_kpi: Output[Metrics],\n    winning_model_name: Output[Artifact],\n):\n    '''\n    Evaluacion de modelos entrenados. Toma los modelos previamente entrenados (pkls) y los evalua segun la metrica F1. El nombre del ganador y NO el modelo en si mismo son pasados como componente, asi como tambien la metrica kpi deseada. \n    Evaluation of trained models. Grabs the previously trained models (pkls) and evaluates them according to F1 score metric. The name of the winner and NOT the model itself gets passed as component, as well as chosen kpi metrics.\n\n    '''\n\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n\n    import pandas as pd\n    import logging \n    import pickle\n    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, f1_score\n    import json\n    import typing\n\n    rf_model = RandomForestClassifier()\n    file_name = rf_chicago_model.path + \".pkl\"\n    with open(file_name, 'rb') as file:  \n        rf_model = pickle.load(file)\n\n    lr_model = LogisticRegression()\n    file_name = lr_chicago_model.path + \".pkl\"\n    with open(file_name, 'rb') as file:  \n        lr_model = pickle.load(file)\n\n    data = pd.read_csv(val_set.path+\".csv\")\n    y_test = data.drop(columns=[\"target\"])\n    y_target=data.target\n\n\n    y_pred_rf = rf_model.predict(y_test)\n    y_pred_lr = lr_model.predict(y_test)\n\n\n    # seleccion de modelo\n    rf_f1 = f1_score(data.target, y_pred_rf.round())\n    lr_f1 = f1_score(data.target, y_pred_lr.round())\n\n\n\n    model_dict = dict({lr_f1: lr_model, rf_f1: rf_model})\n\n    def model_check(val1, val2):\n        if val1 >= val2:\n            return val1\n        else:\n            return val2\n\n    best_f1 = model_check(lr_f1, rf_f1)\n    best_model = model_dict[best_f1]\n\n\n    #xgb_kpi.log_metric(\"f1_score\", float(xgb_f1))\n    rf_kpi.log_metric(\"f1_score\", float(rf_f1))\n    lr_kpi.log_metric(\"f1_score\", float(lr_f1))\n\n\n    winning_model_name_str = type(best_model).__name__\n\n    winning_dict = {'model': winning_model_name_str}\n\n    winning_model_name.metadata = winning_dict\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-train-best-model": {
          "container": {
            "args": [
              "--type",
              "CustomJob",
              "--payload",
              "{\"display_name\": \"Train best model\", \"job_spec\": {\"worker_pool_specs\": [{\"machine_spec\": {\"machine_type\": \"n1-standard-16\"}, \"replica_count\": 1, \"container_spec\": {\"image_uri\": \"python:3.9\", \"command\": [\"sh\", \"-c\", \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'sklearn' 'kfp==1.8.9' && \\\"$0\\\" \\\"$@\\\"\\n\", \"sh\", \"-ec\", \"program_path=$(mktemp -d)\\nprintf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"\\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\\n\", \"\\nimport kfp\\nfrom kfp.v2 import dsl\\nfrom kfp.v2.dsl import *\\nfrom typing import *\\n\\ndef train_best_model(\\n    dataset_train:  Input[Dataset],\\n    dataset_val: Input[Dataset],\\n    model: Output[Model],\\n    parameters: Input[Artifact],\\n    winning_model_name: Input[Artifact],\\n):\\n\\n    '''\\n    Entrenamiento del modelo seleccionado con los hiperparametros elegidos. Combina la seleccion de algoritmo e hiperparametros, entrena y pasa el pkl como componente.\\n    Training of the chosen model with its hyperparameters. Combines the algorithm selection and training, and passes the pkl as a component.\\n\\n    '''\\n\\n    #from xgboost import XGBClassifier\\n    from sklearn.ensemble import RandomForestClassifier\\n    from sklearn.linear_model import LogisticRegression\\n    import pandas as pd\\n    import pickle\\n\\n    # get model name and parameters \\n\\n    best_parameters = parameters.metadata\\n\\n    model_dict = winning_model_name.metadata\\n    WINNING_MODEL_NAME = model_dict.get('model')\\n\\n    # choose model and place parameters\\n    if WINNING_MODEL_NAME == 'LogisticRegression':\\n        best_model = LogisticRegression(**best_parameters)\\n    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\\n        best_model = RandomForestClassifier(**best_parameters)\\n    else:\\n        best_model = None \\n\\n    # get data \\n\\n    data_train = pd.read_csv(dataset_train.path+\\\".csv\\\")\\n    data_val = pd.read_csv(dataset_val.path+\\\".csv\\\")\\n\\n    data = pd.concat([data_train, data_val])\\n\\n    # train\\n    best_model.fit(\\n        data.drop(columns=[\\\"target\\\"]),\\n        data.target,\\n    )\\n    model.metadata[\\\"framework\\\"] = WINNING_MODEL_NAME\\n\\n    file_name = model.path + f\\\".pkl\\\"\\n    with open(file_name, 'wb') as file:  \\n        pickle.dump(best_model, file)\\n\\n\"], \"args\": [\"--executor_input\", \"{{$.json_escape[1]}}\", \"--function_to_execute\", \"train_best_model\"]}, \"disk_spec\": {\"boot_disk_type\": \"pd-ssd\", \"boot_disk_size_gb\": 100}}], \"service_account\": \"{{$.inputs.parameters['service_account']}}\", \"network\": \"{{$.inputs.parameters['network']}}\", \"tensorboard\": \"{{$.inputs.parameters['tensorboard']}}\", \"base_output_directory\": {\"output_uri_prefix\": \"{{$.inputs.parameters['base_output_directory']}}\"}}}",
              "--project",
              "{{$.inputs.parameters['project']}}",
              "--location",
              "{{$.inputs.parameters['location']}}",
              "--gcp_resources",
              "{{$.outputs.parameters['gcp_resources'].output_file}}"
            ],
            "command": [
              "python3",
              "-u",
              "-m",
              "google_cloud_pipeline_components.container.experimental.gcp_launcher.launcher"
            ],
            "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:0.2.0"
          }
        },
        "exec-train-lr-chicago": {
          "container": {
            "args": [
              "--type",
              "CustomJob",
              "--payload",
              "{\"display_name\": \"Train lr chicago\", \"job_spec\": {\"worker_pool_specs\": [{\"machine_spec\": {\"machine_type\": \"n1-standard-16\"}, \"replica_count\": 1, \"container_spec\": {\"image_uri\": \"python:3.9\", \"command\": [\"sh\", \"-c\", \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'sklearn' 'kfp==1.8.9' && \\\"$0\\\" \\\"$@\\\"\\n\", \"sh\", \"-ec\", \"program_path=$(mktemp -d)\\nprintf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"\\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\\n\", \"\\nimport kfp\\nfrom kfp.v2 import dsl\\nfrom kfp.v2.dsl import *\\nfrom typing import *\\n\\ndef train_lr_chicago(\\n    dataset:  Input[Dataset],\\n    model: Output[Model], \\n):\\n    '''\\n    Definicion de componente custom: entrena un modelo Regresion Logistica usando la data que viene de la particion de train, pasada como componente.\\n    Custom component definition: train a Logistic Regression model using the data that comes from the train partition, passed as component.\\n    '''\\n\\n    from sklearn.linear_model import LogisticRegression\\n    import pandas as pd\\n    import pickle\\n\\n    data = pd.read_csv(dataset.path+\\\".csv\\\")\\n    model_lr = LogisticRegression()\\n    model_lr.fit(\\n        data.drop(columns=[\\\"target\\\"]),\\n        data.target,\\n    )\\n    model.metadata[\\\"framework\\\"] = \\\"LR\\\"\\n    file_name = model.path + f\\\".pkl\\\"\\n    with open(file_name, 'wb') as file:  \\n        pickle.dump(model_lr, file)\\n\\n\"], \"args\": [\"--executor_input\", \"{{$.json_escape[1]}}\", \"--function_to_execute\", \"train_lr_chicago\"]}, \"disk_spec\": {\"boot_disk_type\": \"pd-ssd\", \"boot_disk_size_gb\": 100}}], \"service_account\": \"{{$.inputs.parameters['service_account']}}\", \"network\": \"{{$.inputs.parameters['network']}}\", \"tensorboard\": \"{{$.inputs.parameters['tensorboard']}}\", \"base_output_directory\": {\"output_uri_prefix\": \"{{$.inputs.parameters['base_output_directory']}}\"}}}",
              "--project",
              "{{$.inputs.parameters['project']}}",
              "--location",
              "{{$.inputs.parameters['location']}}",
              "--gcp_resources",
              "{{$.outputs.parameters['gcp_resources'].output_file}}"
            ],
            "command": [
              "python3",
              "-u",
              "-m",
              "google_cloud_pipeline_components.container.experimental.gcp_launcher.launcher"
            ],
            "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:0.2.0"
          }
        },
        "exec-train-rf-chicago": {
          "container": {
            "args": [
              "--type",
              "CustomJob",
              "--payload",
              "{\"display_name\": \"Train rf chicago\", \"job_spec\": {\"worker_pool_specs\": [{\"machine_spec\": {\"machine_type\": \"n1-standard-16\"}, \"replica_count\": 1, \"container_spec\": {\"image_uri\": \"python:3.9\", \"command\": [\"sh\", \"-c\", \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'sklearn' 'kfp==1.8.9' && \\\"$0\\\" \\\"$@\\\"\\n\", \"sh\", \"-ec\", \"program_path=$(mktemp -d)\\nprintf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"\\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\\n\", \"\\nimport kfp\\nfrom kfp.v2 import dsl\\nfrom kfp.v2.dsl import *\\nfrom typing import *\\n\\ndef train_rf_chicago(\\n    dataset:  Input[Dataset],\\n    model: Output[Model], \\n):\\n    '''\\n    Definicion de componente custom: entrena un modelo Random Forest usando la data que viene de la particion de train, pasada como componente.\\n    Custom component definition: train a Random Forest model using the data that comes from the train partition, passed as component.\\n    '''\\n\\n    from sklearn.ensemble import RandomForestClassifier\\n    import pandas as pd\\n    import pickle\\n\\n    data = pd.read_csv(dataset.path+\\\".csv\\\")\\n    model_rf = RandomForestClassifier()\\n    model_rf.fit(\\n        data.drop(columns=[\\\"target\\\"]),\\n        data.target,\\n    )\\n    model.metadata[\\\"framework\\\"] = \\\"RF\\\"\\n    file_name = model.path + f\\\".pkl\\\"\\n    with open(file_name, 'wb') as file:  \\n        pickle.dump(model_rf, file)\\n\\n\"], \"args\": [\"--executor_input\", \"{{$.json_escape[1]}}\", \"--function_to_execute\", \"train_rf_chicago\"]}, \"disk_spec\": {\"boot_disk_type\": \"pd-ssd\", \"boot_disk_size_gb\": 100}}], \"service_account\": \"{{$.inputs.parameters['service_account']}}\", \"network\": \"{{$.inputs.parameters['network']}}\", \"tensorboard\": \"{{$.inputs.parameters['tensorboard']}}\", \"base_output_directory\": {\"output_uri_prefix\": \"{{$.inputs.parameters['base_output_directory']}}\"}}}",
              "--project",
              "{{$.inputs.parameters['project']}}",
              "--location",
              "{{$.inputs.parameters['location']}}",
              "--gcp_resources",
              "{{$.outputs.parameters['gcp_resources'].output_file}}"
            ],
            "command": [
              "python3",
              "-u",
              "-m",
              "google_cloud_pipeline_components.container.experimental.gcp_launcher.launcher"
            ],
            "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:0.2.0"
          }
        },
        "exec-upload-model-to-vertex-and-batch-prediction": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "upload_model_to_vertex_and_batch_prediction"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'scikit-learn==1.0.0' 'google-cloud-bigquery' 'google-cloud-bigquery-storage' 'google-cloud-aiplatform' 'kfp==1.8.9' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef upload_model_to_vertex_and_batch_prediction(\n    project: str,\n    region: str,\n    serving_container: str,\n    trained_model: Input[Model],\n    winning_model_name: Input[Artifact],\n    gcs_predict_source: str,\n    gcs_predict_dest: str\n\n):\n    '''\n    Toma el mejor modelo entrenado en formato pkl y lo convierte en un Vertex Managed Model a partir del cual se realizan las predicciones en formato batch.\n    Takes the trained best model in pkl format and uploads it to a Vertex Managed Model and uses it to do a batch prediction job.\n    '''\n\n    from typing import Dict, Optional, Sequence\n\n    from google.cloud import aiplatform\n\n    from datetime import datetime\n\n    model_dict = winning_model_name.metadata\n    WINNING_MODEL_NAME = model_dict.get('model')\n\n    TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n    DISPLAY_NAME = WINNING_MODEL_NAME +'-' + TIMESTAMP\n\n    MODEL_URI = trained_model.uri\n    MODEL_PATH = MODEL_URI[:-5] # peque\u00f1o hack para que encuentre el directorio con el modelo\n\n    def upload_model_sample(\n        project: str,\n        location: str,\n        display_name: str,\n        serving_container_image_uri: str,\n        artifact_uri: Optional[str] = None,\n        sync: bool = True,\n    ):\n\n\n        aiplatform.init(project=project, location=location)\n\n        model = aiplatform.Model.upload(\n            display_name=display_name,\n            artifact_uri=artifact_uri,\n            serving_container_image_uri=serving_container,\n            sync=sync,\n        )\n\n        model.wait()\n\n        print(model.display_name)\n        print(model.resource_name)\n        return model\n\n    model_test = upload_model_sample(\n        project = project,\n        location = region,\n        display_name = DISPLAY_NAME,\n        serving_container_image_uri= serving_container,\n        artifact_uri = MODEL_PATH\n    )\n\n    batch_job = model_test.batch_predict(\n        job_display_name=DISPLAY_NAME,\n        gcs_source = gcs_predict_source,\n        instances_format=\"csv\",\n        gcs_destination_prefix=gcs_predict_dest,\n        machine_type = 'n1-standard-16'\n    )\n\n"
            ],
            "image": "python:3.9"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "chicago-taxi-pipeline"
    },
    "root": {
      "dag": {
        "outputs": {
          "artifacts": {
            "best-model-evaluation-best_model_kpi": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "best_model_kpi",
                  "producerSubtask": "best-model-evaluation"
                }
              ]
            },
            "best-model-hp-tuning-kpi": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "kpi",
                  "producerSubtask": "best-model-hp-tuning"
                }
              ]
            },
            "best-model-hp-tuning-model_name": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "model_name",
                  "producerSubtask": "best-model-hp-tuning"
                }
              ]
            },
            "model-evaluation-lr_kpi": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "lr_kpi",
                  "producerSubtask": "model-evaluation"
                }
              ]
            },
            "model-evaluation-rf_kpi": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "rf_kpi",
                  "producerSubtask": "model-evaluation"
                }
              ]
            }
          }
        },
        "tasks": {
          "best-model-evaluation": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-best-model-evaluation"
            },
            "dependentTasks": [
              "get-chicago-data",
              "model-evaluation",
              "train-best-model"
            ],
            "inputs": {
              "artifacts": {
                "best_model": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "model",
                    "producerTask": "train-best-model"
                  }
                },
                "test_set": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset_test",
                    "producerTask": "get-chicago-data"
                  }
                },
                "winning_model_name": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "winning_model_name",
                    "producerTask": "model-evaluation"
                  }
                }
              },
              "parameters": {
                "threshold": {
                  "componentInputParameter": "threshold"
                }
              }
            },
            "taskInfo": {
              "name": "best-model-evaluation"
            }
          },
          "best-model-hp-tuning": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-best-model-hp-tuning"
            },
            "dependentTasks": [
              "get-chicago-data",
              "model-evaluation"
            ],
            "inputs": {
              "artifacts": {
                "winning_model_name": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "winning_model_name",
                    "producerTask": "model-evaluation"
                  }
                }
              },
              "parameters": {
                "hp_train_machine": {
                  "componentInputParameter": "hp_train_machine"
                },
                "lr_hp_image": {
                  "componentInputParameter": "lr_hp_image"
                },
                "parallel_trials": {
                  "componentInputParameter": "parallel_trials"
                },
                "project": {
                  "componentInputParameter": "project_id"
                },
                "region": {
                  "componentInputParameter": "gcp_region"
                },
                "rf_hp_image": {
                  "componentInputParameter": "rf_hp_image"
                },
                "stage_data_bucket": {
                  "componentInputParameter": "stage_data_bucket"
                },
                "timestamp": {
                  "taskOutputParameter": {
                    "outputParameterKey": "timestamp",
                    "producerTask": "get-chicago-data"
                  }
                },
                "trials": {
                  "componentInputParameter": "trials"
                }
              }
            },
            "taskInfo": {
              "name": "best-model-hp-tuning"
            }
          },
          "bq-current-raw-to-stage-ml": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-bq-current-raw-to-stage-ml"
            },
            "inputs": {
              "parameters": {
                "bq_current_raw_url": {
                  "componentInputParameter": "bq_current_raw_url"
                },
                "bq_current_stage_url": {
                  "componentInputParameter": "bq_current_stage_url"
                },
                "project": {
                  "componentInputParameter": "project_id"
                },
                "region": {
                  "componentInputParameter": "gcp_region"
                },
                "stage_data_bucket": {
                  "componentInputParameter": "stage_data_bucket"
                }
              }
            },
            "taskInfo": {
              "name": "bq-current-raw-to-stage-ml"
            }
          },
          "bq-historic-raw-to-stage-ml": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-bq-historic-raw-to-stage-ml"
            },
            "inputs": {
              "parameters": {
                "bq_historic_raw_url": {
                  "componentInputParameter": "bq_historic_raw_url"
                },
                "bq_historic_stage_url": {
                  "componentInputParameter": "bq_historic_stage_url"
                },
                "project": {
                  "componentInputParameter": "project_id"
                },
                "region": {
                  "componentInputParameter": "gcp_region"
                }
              }
            },
            "taskInfo": {
              "name": "bq-historic-raw-to-stage-ml"
            }
          },
          "condition-predict-decision-1": {
            "componentRef": {
              "name": "comp-condition-predict-decision-1"
            },
            "dependentTasks": [
              "best-model-evaluation",
              "bq-current-raw-to-stage-ml",
              "model-evaluation",
              "train-best-model"
            ],
            "inputs": {
              "artifacts": {
                "pipelineparam--model-evaluation-winning_model_name": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "winning_model_name",
                    "producerTask": "model-evaluation"
                  }
                },
                "pipelineparam--train-best-model-model": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "model",
                    "producerTask": "train-best-model"
                  }
                }
              },
              "parameters": {
                "pipelineparam--best-model-evaluation-dep_decision": {
                  "taskOutputParameter": {
                    "outputParameterKey": "dep_decision",
                    "producerTask": "best-model-evaluation"
                  }
                },
                "pipelineparam--bq-current-raw-to-stage-ml-gcs_predict_source": {
                  "taskOutputParameter": {
                    "outputParameterKey": "gcs_predict_source",
                    "producerTask": "bq-current-raw-to-stage-ml"
                  }
                },
                "pipelineparam--gcp_region": {
                  "componentInputParameter": "gcp_region"
                },
                "pipelineparam--project_id": {
                  "componentInputParameter": "project_id"
                },
                "pipelineparam--serving_container": {
                  "componentInputParameter": "serving_container"
                },
                "pipelineparam--stage_data_bucket": {
                  "componentInputParameter": "stage_data_bucket"
                }
              }
            },
            "taskInfo": {
              "name": "condition-predict-decision-1"
            },
            "triggerPolicy": {
              "condition": "inputs.parameters['pipelineparam--best-model-evaluation-dep_decision'].string_value == 'true'"
            }
          },
          "get-chicago-data": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-get-chicago-data"
            },
            "dependentTasks": [
              "bq-historic-raw-to-stage-ml"
            ],
            "inputs": {
              "parameters": {
                "bq_source_url": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "bq-historic-raw-to-stage-ml"
                  }
                },
                "project": {
                  "componentInputParameter": "project_id"
                },
                "region": {
                  "componentInputParameter": "gcp_region"
                },
                "stage_data_bucket": {
                  "componentInputParameter": "stage_data_bucket"
                }
              }
            },
            "taskInfo": {
              "name": "get-chicago-data"
            }
          },
          "model-evaluation": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-model-evaluation"
            },
            "dependentTasks": [
              "get-chicago-data",
              "train-lr-chicago",
              "train-rf-chicago"
            ],
            "inputs": {
              "artifacts": {
                "lr_chicago_model": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "model",
                    "producerTask": "train-lr-chicago"
                  }
                },
                "rf_chicago_model": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "model",
                    "producerTask": "train-rf-chicago"
                  }
                },
                "val_set": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset_val",
                    "producerTask": "get-chicago-data"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "model-evaluation"
            }
          },
          "train-best-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-best-model"
            },
            "dependentTasks": [
              "best-model-hp-tuning",
              "get-chicago-data",
              "model-evaluation"
            ],
            "inputs": {
              "artifacts": {
                "dataset_train": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset_train",
                    "producerTask": "get-chicago-data"
                  }
                },
                "dataset_val": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset_val",
                    "producerTask": "get-chicago-data"
                  }
                },
                "parameters": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "model_spec",
                    "producerTask": "best-model-hp-tuning"
                  }
                },
                "winning_model_name": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "winning_model_name",
                    "producerTask": "model-evaluation"
                  }
                }
              },
              "parameters": {
                "base_output_directory": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "location": {
                  "componentInputParameter": "gcp_region"
                },
                "network": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "project": {
                  "componentInputParameter": "project_id"
                },
                "service_account": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "tensorboard": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "train-best-model"
            }
          },
          "train-lr-chicago": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-lr-chicago"
            },
            "dependentTasks": [
              "get-chicago-data"
            ],
            "inputs": {
              "artifacts": {
                "dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset_train",
                    "producerTask": "get-chicago-data"
                  }
                }
              },
              "parameters": {
                "base_output_directory": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "location": {
                  "componentInputParameter": "gcp_region"
                },
                "network": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "project": {
                  "componentInputParameter": "project_id"
                },
                "service_account": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "tensorboard": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "train-lr-chicago"
            }
          },
          "train-rf-chicago": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-rf-chicago"
            },
            "dependentTasks": [
              "get-chicago-data"
            ],
            "inputs": {
              "artifacts": {
                "dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset_train",
                    "producerTask": "get-chicago-data"
                  }
                }
              },
              "parameters": {
                "base_output_directory": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "location": {
                  "componentInputParameter": "gcp_region"
                },
                "network": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "project": {
                  "componentInputParameter": "project_id"
                },
                "service_account": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "tensorboard": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "train-rf-chicago"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "bq_current_raw_url": {
            "type": "STRING"
          },
          "bq_current_stage_url": {
            "type": "STRING"
          },
          "bq_historic_raw_url": {
            "type": "STRING"
          },
          "bq_historic_stage_url": {
            "type": "STRING"
          },
          "enable_cache": {
            "type": "STRING"
          },
          "gcp_region": {
            "type": "STRING"
          },
          "hp_train_machine": {
            "type": "STRING"
          },
          "lr_hp_image": {
            "type": "STRING"
          },
          "machine_type": {
            "type": "STRING"
          },
          "parallel_trials": {
            "type": "INT"
          },
          "pipeline_root": {
            "type": "STRING"
          },
          "pipelines_bucket": {
            "type": "STRING"
          },
          "project_id": {
            "type": "STRING"
          },
          "rf_hp_image": {
            "type": "STRING"
          },
          "serving_container": {
            "type": "STRING"
          },
          "stage_data_bucket": {
            "type": "STRING"
          },
          "threshold": {
            "type": "DOUBLE"
          },
          "trials": {
            "type": "INT"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "best-model-evaluation-best_model_kpi": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "best-model-hp-tuning-kpi": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "best-model-hp-tuning-model_name": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "model-evaluation-lr_kpi": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "model-evaluation-rf_kpi": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.9"
  },
  "runtimeConfig": {
    "gcsOutputDirectory": "gs://teco-prod-adam-dev-826c-chicago_taxi_pipelines/pipeline_root/",
    "parameters": {
      "bq_current_raw_url": {
        "stringValue": "teco-prod-adam-dev-826c.chicago_taxi_current.raw"
      },
      "bq_current_stage_url": {
        "stringValue": "teco-prod-adam-dev-826c.chicago_taxi_current.stage_ml"
      },
      "bq_historic_raw_url": {
        "stringValue": "teco-prod-adam-dev-826c.chicago_taxi_historic.raw"
      },
      "bq_historic_stage_url": {
        "stringValue": "teco-prod-adam-dev-826c.chicago_taxi_historic.stage_ml"
      },
      "enable_cache": {
        "stringValue": "True"
      },
      "gcp_region": {
        "stringValue": "us-central1"
      },
      "hp_train_machine": {
        "stringValue": "n1-standard-16"
      },
      "lr_hp_image": {
        "stringValue": "gcr.io/teco-prod-adam-dev-826c/lr_hp_job:v1"
      },
      "machine_type": {
        "stringValue": "n1-standard-16"
      },
      "parallel_trials": {
        "intValue": "3"
      },
      "pipeline_root": {
        "stringValue": "gs://teco-prod-adam-dev-826c-chicago_taxi_pipelines/pipeline_root/"
      },
      "pipelines_bucket": {
        "stringValue": "teco-prod-adam-dev-826c-chicago_taxi_pipelines"
      },
      "project_id": {
        "stringValue": "teco-prod-adam-dev-826c"
      },
      "rf_hp_image": {
        "stringValue": "gcr.io/teco-prod-adam-dev-826c/rf_hp_job:v1"
      },
      "serving_container": {
        "stringValue": "us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest"
      },
      "stage_data_bucket": {
        "stringValue": "teco-prod-adam-dev-826c-chicago_taxi_stage"
      },
      "threshold": {
        "doubleValue": 0.7
      },
      "trials": {
        "intValue": "10"
      }
    }
  }
}