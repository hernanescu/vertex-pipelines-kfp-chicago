name: Bq current raw to stage ml
description: Toma el dataset de BigQuery establecido como el presente y lo procesa,
  colocandolo en la tabla stage_ml. Tambien sube una version timestamped de la data
  en csv.
inputs:
- {name: project, type: String}
- {name: region, type: String}
- {name: bq_current_raw_url, type: String}
- {name: bq_current_stage_url, type: String}
- {name: stage_data_bucket, type: String}
outputs:
- {name: gcs_predict_source, type: String}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'pyarrow' 'scikit-learn==1.0.0' 'google-cloud-bigquery' 'google-cloud-bigquery-storage' 'pandas-gbq' 'google-cloud-aiplatform' 'kfp==1.8.9' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef bq_current_raw_to_stage_ml(\n    project: str,\n    region:\
      \ str,\n    bq_current_raw_url: str,\n    bq_current_stage_url: str,\n    stage_data_bucket:\
      \ str,\n    gcs_predict_source: OutputPath(str)\n):\n    '''\n    Toma el dataset\
      \ de BigQuery establecido como el presente y lo procesa, colocandolo en la tabla\
      \ stage_ml. Tambien sube una version timestamped de la data en csv.\n    Takes\
      \ the Bigquery Dataset established as the present and processes it, placing\
      \ it in the stage_ml table. It also uploads a timestamped csv version of the\
      \ data.\n    '''\n    import pandas as pd\n    import pandas_gbq\n    import\
      \ numpy as np\n    from google.cloud import bigquery\n    from google.cloud\
      \ import storage\n    from google.cloud.storage import Blob\n    from google.cloud\
      \ import aiplatform\n\n    aiplatform.init(project = project,\n            \
      \        location = region)\n\n    ### get data from bq_source\n    bqclient\
      \ = bigquery.Client(project = project, location = region)\n\n\n    # Download\
      \ a table\n    table = bigquery.TableReference.from_string(\n        bq_current_raw_url\n\
      \    )\n    rows = bqclient.list_rows(\n        table\n    )\n    data = rows.to_dataframe(\n\
      \        create_bqstorage_client=True, # guarda ac\xE1\n    )\n\n    # process\n\
      \n    # eliminamos el target para simular la realidad / eliminate target to\
      \ simulate reality\n    df = data[['trip_month', 'trip_day', 'trip_day_of_week',\
      \ 'trip_hour', 'trip_seconds', 'trip_miles', 'payment_type', 'euclidean']]\n\
      \n    df2 = pd.get_dummies(df, columns = ['payment_type'], drop_first = True)\n\
      \n    df2.columns = df2.columns.str.replace(' ','_')\n\n    # upload to bq\n\
      \n    df2.to_gbq(bq_current_stage_url,\n               project,\n          \
      \     chunksize=None, \n               if_exists='replace', # el default tira\
      \ error, aca queremos que siempre reemplace / default throws error, here we\
      \ want it to replace always\n               table_schema=[{'name': 'trip_month','type':\
      \ 'INTEGER'},\n                             {'name': 'trip_day','type': 'INTEGER'},\n\
      \                             {'name': 'trip_day_of_week','type': 'INTEGER'},\n\
      \                             {'name': 'trip_hour','type': 'INTEGER'},\n   \
      \                          {'name': 'trip_seconds','type': 'INTEGER'},\n   \
      \                          {'name': 'trip_miles','type': 'FLOAT'},\n       \
      \                      {'name': 'euclidean','type': 'FLOAT'},\n            \
      \                 #{'name': 'target','type': 'INTEGER'}, eliminamos el target\
      \ para simular la realidad / eliminate target to simulate reality\n        \
      \                     {'name': 'payment_type_Credit_Card','type': 'INTEGER'},\n\
      \                             {'name': 'payment_type_Dispute','type': 'INTEGER'},\n\
      \                             {'name': 'payment_type_Mobile','type': 'INTEGER'},\n\
      \                             {'name': 'payment_type_No_Charge','type': 'INTEGER'},\n\
      \                             {'name': 'payment_type_Prcard','type': 'INTEGER'},\n\
      \                             {'name': 'payment_type_Unknown','type': 'INTEGER'}\n\
      \                             ]\n    )\n\n    # ponerle a la data tambien un\
      \ timestamp\n    # also timestamp the data    \n\n    from datetime import datetime\n\
      \    import os\n\n    DATA_DIR = 'batch_predict_data'\n\n    if not os.path.exists(DATA_DIR):\n\
      \        os.mkdir(DATA_DIR)\n\n    TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\"\
      )\n\n    DATA_NAME = f\"predicted_data_{TIMESTAMP}.csv\"\n\n    DATA_PATH =\
      \ os.path.join(DATA_DIR, DATA_NAME)\n\n    df2.to_csv(DATA_PATH, index = False)\n\
      \n    gcsclient = storage.Client() \n    bucket = gcsclient.get_bucket(stage_data_bucket)\n\
      \n    blob_train = bucket.blob(DATA_PATH)\n    blob_train.upload_from_filename(DATA_PATH)\n\
      \n\n    GCS_PREDICT_SOURCE = f\"gs://{stage_data_bucket}/{DATA_PATH}\"\n\n \
      \   with open(gcs_predict_source, 'w') as f:\n              f.write(GCS_PREDICT_SOURCE)\n\
      \n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - bq_current_raw_to_stage_ml
