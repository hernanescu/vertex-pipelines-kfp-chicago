{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363eb2b1-4ec6-4e93-92b3-527e240638df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vertex AI - KubeFlow Pipelines (KFP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8094c1c1-2718-4b9c-a43a-6ec83709df8e",
   "metadata": {},
   "source": [
    "A lo largo del pipeline, se utilizan componentes custom que son creados a partir de código Python y cuentan con una serie de objetos propios de KFP, que son definidos en esta notebook y almacenados como objetos yaml para ser leídos en la notebook donde se ejecuta el pipeline en sí.\n",
    "\n",
    "Cabe mencionar que también se puede realizar pipelines -y componentes custom- utilizando Tensorflow Extended (TFX)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a42494-7229-49d2-b7d0-e77f32e31b39",
   "metadata": {},
   "source": [
    "Throughout the pipeline, we use custom component created from Python code and with some objects that come from KFP. These are defined within this notebook, stored as yaml objects and then read by the main notebook where the pipeline itself its executed.\n",
    "\n",
    "It's worth mentioning that these pipelines -and custom components as well- can also be built using Tensorflow Extended (TFX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bac374-f07e-4a06-ad75-251b03c53128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca2b1a2-e6f8-46b4-ace2-68ab44914107",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\", \"pandas-gbq\", \"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"bq_current_raw_to_stage.yaml\"\n",
    ")\n",
    "\n",
    "def bq_current_raw_to_stage_ml(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    bq_current_raw_url: str,\n",
    "    bq_current_stage_url: str,\n",
    "    stage_data_bucket: str,\n",
    "    gcs_predict_source: OutputPath(str)\n",
    "):\n",
    "    '''\n",
    "    Toma el dataset de BigQuery establecido como el presente y lo procesa, colocandolo en la tabla stage_ml. Tambien sube una version timestamped de la data en csv.\n",
    "    Takes the Bigquery Dataset established as the present and processes it, placing it in the stage_ml table. It also uploads a timestamped csv version of the data.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    import numpy as np\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage import Blob\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    aiplatform.init(project = project,\n",
    "                    location = region)\n",
    "    \n",
    "    ### get data from bq_source\n",
    "    bqclient = bigquery.Client(project = project, location = region)\n",
    "    \n",
    "\n",
    "    # Download a table\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_current_raw_url\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table\n",
    "    )\n",
    "    data = rows.to_dataframe(\n",
    "        create_bqstorage_client=True, # guarda acá\n",
    "    )\n",
    "    \n",
    "    # process\n",
    "    \n",
    "    df = data[['trip_month', 'trip_day', 'trip_day_of_week', 'trip_hour', 'trip_seconds', 'trip_miles', 'payment_type', 'euclidean']]\n",
    "    \n",
    "    df2 = pd.get_dummies(df, columns = ['payment_type'], drop_first = True)\n",
    "    \n",
    "    df2.columns = df2.columns.str.replace(' ','_')\n",
    "    \n",
    "    # upload to bq\n",
    "    \n",
    "    df2.to_gbq(bq_current_stage_url,\n",
    "               project,\n",
    "               chunksize=None, \n",
    "               if_exists='replace', # el default tira error, aca queremos que siempre reemplace\n",
    "               table_schema=[{'name': 'trip_month','type': 'INTEGER'},\n",
    "                             {'name': 'trip_day','type': 'INTEGER'},\n",
    "                             {'name': 'trip_day_of_week','type': 'INTEGER'},\n",
    "                             {'name': 'trip_hour','type': 'INTEGER'},\n",
    "                             {'name': 'trip_seconds','type': 'INTEGER'},\n",
    "                             {'name': 'trip_miles','type': 'FLOAT'},\n",
    "                             {'name': 'euclidean','type': 'FLOAT'},\n",
    "                             #{'name': 'target','type': 'INTEGER'}, eliminamos el target para simular la realidad\n",
    "                             {'name': 'payment_type_Credit_Card','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Dispute','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Mobile','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_No_Charge','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Prcard','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Unknown','type': 'INTEGER'}\n",
    "                             ]\n",
    "    )\n",
    "    \n",
    "    # ponerle a la data tambien un timestamp\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "\n",
    "    TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    DATA_PATH = f\"predicted_data_{TIMESTAMP}.csv\"\n",
    "    \n",
    "    df2.to_csv(DATA_PATH, index = False)\n",
    "    \n",
    "    gcsclient = storage.Client() # tal vez vaya stage_data_bucket\n",
    "    bucket = gcsclient.get_bucket(stage_data_bucket)\n",
    "    \n",
    "    blob_train = bucket.blob(DATA_PATH)\n",
    "    blob_train.upload_from_filename(DATA_PATH)\n",
    "    \n",
    "    \n",
    "    GCS_PREDICT_SOURCE = f\"gs://{stage_data_bucket}/{DATA_PATH}\"\n",
    "    \n",
    "    with open(gcs_predict_source, 'w') as f:\n",
    "              f.write(GCS_PREDICT_SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c987ec4-6d28-40b1-9cf0-de8a5efd5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\", \"pandas-gbq\", \"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"bq_historic_raw_to_stage.yaml\"\n",
    ")\n",
    "\n",
    "def bq_historic_raw_to_stage_ml(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    bq_historic_raw_url: str,\n",
    "    bq_historic_stage_url: str,\n",
    "    \n",
    ") -> str:\n",
    "    \n",
    "    '''\n",
    "    Toma el dataset de BigQuery establecido como el periodo historico y lo procesa, colocandolo en la tabla stage_ml.\n",
    "    Takes the Bigquery Dataset established as historic and processes it, placing it in the stage_ml table.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    import numpy as np\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage import Blob\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    aiplatform.init(project = project,\n",
    "                    location = region)\n",
    "    \n",
    "    ### get data from bq_source\n",
    "    bqclient = bigquery.Client(project = project, location = region)\n",
    "    \n",
    "\n",
    "    # Download a table.\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_historic_raw_url\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table\n",
    "    )\n",
    "    data = rows.to_dataframe(\n",
    "        create_bqstorage_client=True, # guarda acá\n",
    "    )\n",
    "    \n",
    "    df = data[['trip_month', 'trip_day', 'trip_day_of_week', 'trip_hour', 'trip_seconds', 'trip_miles', 'payment_type', 'euclidean', 'tip_bin']]\n",
    "    \n",
    "    df = df.rename(columns = {'tip_bin':'target'})\n",
    "    \n",
    "    df2 = pd.get_dummies(df, columns = ['payment_type'], drop_first = True)\n",
    "    \n",
    "    df2.columns = df2.columns.str.replace(' ','_')\n",
    "    \n",
    "    df2.to_gbq(bq_historic_stage_url,\n",
    "               project,\n",
    "               chunksize=None, # I have tried with several chunk sizes, it runs faster when it's one big chunk (at least for me)\n",
    "               if_exists='replace', # el default tira error, aca no queremos eso\n",
    "               #verbose=False\n",
    "               table_schema=[{'name': 'trip_month','type': 'INTEGER'},\n",
    "                             {'name': 'trip_day','type': 'INTEGER'},\n",
    "                             {'name': 'trip_day_of_week','type': 'INTEGER'},\n",
    "                             {'name': 'trip_hour','type': 'INTEGER'},\n",
    "                             {'name': 'trip_seconds','type': 'INTEGER'},\n",
    "                             {'name': 'trip_miles','type': 'FLOAT'},\n",
    "                             {'name': 'euclidean','type': 'FLOAT'},\n",
    "                             {'name': 'target','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Credit_Card','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Dispute','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Mobile','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_No_Charge','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Prcard','type': 'INTEGER'},\n",
    "                             {'name': 'payment_type_Unknown','type': 'INTEGER'}\n",
    "                             ]\n",
    "    )\n",
    "    \n",
    "    URL_TO_GO = bq_historic_stage_url\n",
    "    \n",
    "    return URL_TO_GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a24c2d3-07e6-450a-8fd1-bae470999a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"get_chicago_data.yaml\"\n",
    ")\n",
    "\n",
    "def get_chicago_data(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    bq_source_url: str,\n",
    "    stage_data_bucket: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_val: Output[Dataset],\n",
    "    dataset_test: Output[Dataset] \n",
    "):\n",
    "    '''\n",
    "    Toma los datos que se consideran historicos de la tabla de BQ y separa en train, validation y test. Ademas de pasarlos como componentes del pipeline, guarda una version de los datos en el bucket de stage.\n",
    "    Takes the data considered as historic from the BQ table and splits it into train, validation and test. Besides passing them as pipeline component, it stores a version of the data in the stage bucket.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage import Blob\n",
    "    \n",
    "    TRAIN_DATA_PATH = 'chicago_taxi_train.csv'\n",
    "    VAL_DATA_PATH = 'chicago_taxi_val.csv'\n",
    "    TEST_DATA_PATH = 'chicago_taxi_test.csv'\n",
    "    \n",
    "    ### get data from bq_source\n",
    "    bqclient = bigquery.Client(project = project, location = region)\n",
    "    \n",
    "\n",
    "    # Download tje table.\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_source_url\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "\n",
    "    )\n",
    "    data = rows.to_dataframe(\n",
    "        create_bqstorage_client=True, # guarda acá\n",
    "    )\n",
    "    \n",
    "    # splits in train, val and test\n",
    "      \n",
    "    train, test = tts(data, test_size=0.3)\n",
    "    train_data, val_data = tts(train, test_size = 0.2)\n",
    "    \n",
    "    train_data.to_csv(TRAIN_DATA_PATH)\n",
    "    val_data.to_csv(VAL_DATA_PATH)\n",
    "    test.to_csv(TEST_DATA_PATH)\n",
    "    \n",
    "    ### so far we have the paths, we have to upload them to the bucket / hasta aca están los csvs en los PATH\n",
    "    gcsclient = storage.Client() \n",
    "    bucket = gcsclient.get_bucket(stage_data_bucket)\n",
    "    \n",
    "    blob_train = bucket.blob(TRAIN_DATA_PATH)\n",
    "    blob_train.upload_from_filename(TRAIN_DATA_PATH)\n",
    "    \n",
    "    blob_train = bucket.blob(VAL_DATA_PATH)\n",
    "    blob_train.upload_from_filename(VAL_DATA_PATH)\n",
    "    \n",
    "    blob_test = bucket.blob(TEST_DATA_PATH)\n",
    "    blob_test.upload_from_filename(TEST_DATA_PATH)\n",
    "    \n",
    "    train_data.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    val_data.to_csv(dataset_val.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4acb8b6-0843-46a5-a8e2-b740ae324409",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "    ], base_image=\"python:3.9\",\n",
    "    output_component_file=\"train_rf_chicago.yaml\"\n",
    ")\n",
    "def train_rf_chicago(\n",
    "    dataset:  Input[Dataset],\n",
    "    model: Output[Model], \n",
    "):\n",
    "    '''\n",
    "    Definicion de componente custom: entrena un modelo Random Forest usando la data que viene de la particion de train, pasada como componente.\n",
    "    Custom component definition: train a Random Forest model using the data that comes from the train partition, passed as component.\n",
    "    '''\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "\n",
    "    data = pd.read_csv(dataset.path+\".csv\")\n",
    "    model_rf = RandomForestClassifier()\n",
    "    model_rf.fit(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "    model.metadata[\"framework\"] = \"RF\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(model_rf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1bcb8b9-1419-461b-89b2-b734cf5ef72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "    ], base_image=\"python:3.9\",\n",
    "    output_component_file=\"train_lr_chicago.yaml\"\n",
    ")\n",
    "def train_lr_chicago(\n",
    "    dataset:  Input[Dataset],\n",
    "    model: Output[Model], \n",
    "):\n",
    "    '''\n",
    "    Definicion de componente custom: entrena un modelo Regresion Logistica usando la data que viene de la particion de train, pasada como componente.\n",
    "    Custom component definition: train a Logistic Regression model using the data that comes from the train partition, passed as component.\n",
    "    '''\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "\n",
    "    data = pd.read_csv(dataset.path+\".csv\")\n",
    "    model_lr = LogisticRegression()\n",
    "    model_lr.fit(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "    model.metadata[\"framework\"] = \"LR\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(model_lr, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb3dfc88-dd29-4402-bb4e-350e00189de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "    ], base_image=\"python:3.9\",\n",
    "    output_component_file=\"model_evaluation.yaml\"\n",
    ")\n",
    "def model_evaluation(\n",
    "    val_set:  Input[Dataset],\n",
    "    lr_chicago_model: Input[Model],\n",
    "    rf_chicago_model: Input[Model],\n",
    "    lr_kpi: Output[Metrics],\n",
    "    rf_kpi: Output[Metrics],\n",
    "    winning_model_name: Output[Artifact],\n",
    "):\n",
    "    '''\n",
    "    Evaluacion de modelos entrenados. Toma los modelos previamente entrenados (pkls) y los evalua segun la metrica F1. El nombre del ganador y NO el modelo en si mismo son pasados como componente, asi como tambien la metrica kpi deseada. \n",
    "    Evaluation of trained models. Grabs the previously trained models (pkls) and evaluates them according to F1 score metric. The name of the winner and NOT the model itself gets passed as component, as well as chosen kpi metrics.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import pickle\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, f1_score\n",
    "    import json\n",
    "    import typing\n",
    "    \n",
    "    rf_model = RandomForestClassifier()\n",
    "    file_name = rf_chicago_model.path + \".pkl\"\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        rf_model = pickle.load(file)\n",
    "        \n",
    "    lr_model = LogisticRegression()\n",
    "    file_name = lr_chicago_model.path + \".pkl\"\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        lr_model = pickle.load(file)\n",
    "    \n",
    "    data = pd.read_csv(val_set.path+\".csv\")\n",
    "    y_test = data.drop(columns=[\"target\"])\n",
    "    y_target=data.target\n",
    "    \n",
    "    \n",
    "    y_pred_rf = rf_model.predict(y_test)\n",
    "    y_pred_lr = lr_model.predict(y_test)\n",
    "\n",
    "  \n",
    "    # seleccion de modelo\n",
    "    rf_f1 = f1_score(data.target, y_pred_rf.round())\n",
    "    lr_f1 = f1_score(data.target, y_pred_lr.round())\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_dict = dict({lr_f1: lr_model, rf_f1: rf_model})\n",
    "    \n",
    "    def model_check(val1, val2):\n",
    "        if val1 >= val2:\n",
    "            return val1\n",
    "        else:\n",
    "            return val2\n",
    "    \n",
    "    best_f1 = model_check(lr_f1, rf_f1)\n",
    "    best_model = model_dict[best_f1]\n",
    "    \n",
    "        \n",
    "    #xgb_kpi.log_metric(\"f1_score\", float(xgb_f1))\n",
    "    rf_kpi.log_metric(\"f1_score\", float(rf_f1))\n",
    "    lr_kpi.log_metric(\"f1_score\", float(lr_f1))\n",
    "    \n",
    "    \n",
    "    winning_model_name_str = type(best_model).__name__\n",
    "    \n",
    "    winning_dict = {'model': winning_model_name_str}\n",
    "    \n",
    "    winning_model_name.metadata = winning_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9079dd54-6359-40a4-8b30-46b1ce472600",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\", \"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"best_model_hp_tuning.yaml\"\n",
    ")\n",
    "\n",
    "def best_model_hp_tuning(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    stage_data_bucket: str,\n",
    "    winning_model_name: Input[Artifact],\n",
    "    model_spec: Output[Artifact],\n",
    "    trials: int,\n",
    "    parallel_trials: int,\n",
    "    kpi: Output[Metrics],\n",
    "    model_name: Output[Metrics] \n",
    "): \n",
    "    '''\n",
    "    Tuneo de hiperparametros. Toma el nombre del modelo ganador y utiliza la imagen de Docker correspondiente para lanzar un job de entrenamiento. Los hiperparametros obtenidos son pasados como componentes.\n",
    "    Hyperparameter tuning. Takes the name of the winning model and uses the corresponding Docker image to launch a training job. The chosen hyperparameters are passed as a component.\n",
    "    '''\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "    from google.protobuf.json_format import MessageToDict\n",
    "    import pandas as pd\n",
    "    \n",
    "    aiplatform.init(project = project,\n",
    "                    location = region)\n",
    "    \n",
    "    # train images definition\n",
    "    RF_HP_IMAGE = \"gcr.io/vertex-testing-327520/rf_hp_job:v1\"\n",
    "    LR_HP_IMAGE = \"gcr.io/vertex-testing-327520/lr_hp_job:v1\"\n",
    "    \n",
    "    # get model name\n",
    "    model_dict = winning_model_name.metadata\n",
    "        \n",
    "    WINNING_MODEL_NAME = model_dict.get('model')\n",
    "    \n",
    "    from datetime import datetime\n",
    "\n",
    "    TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    if WINNING_MODEL_NAME == 'LogisticRegression':\n",
    "        WINNING_MODEL_IMAGE = LR_HP_IMAGE\n",
    "    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n",
    "        WINNING_MODEL_IMAGE = RF_HP_IMAGE\n",
    "    else:\n",
    "        WINNING_MODEL_IMAGE = None \n",
    "    \n",
    "    worker_pool_specs = [{\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": \"n1-standard-8\",\n",
    "        #\"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "        #\"accelerator_count\": 1\n",
    "    },\n",
    "    \"replica_count\": 1,\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": WINNING_MODEL_IMAGE\n",
    "    }\n",
    "    }]\n",
    "    \n",
    "    \n",
    "    metric_spec={'f1_score':'maximize'}\n",
    "\n",
    "    # Dictionary representing parameters to optimize.\n",
    "    # The dictionary key is the parameter_id, which is passed into your training\n",
    "    # job as a command line argument,\n",
    "    # And the dictionary value is the parameter specification of the metric.\n",
    "    \n",
    "    lr_parameter_spec = {\n",
    "        \"penalty\": hpt.CategoricalParameterSpec(values=['l1', 'l2']),\n",
    "        \"C\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n",
    "        \"solver\": hpt.CategoricalParameterSpec(values=['saga', 'liblinear'])\n",
    "    }\n",
    "    \n",
    "    rf_parameter_spec = {\n",
    "        \"max_leaf_nodes\": hpt.DiscreteParameterSpec(values=[4, 8, 10], scale=None),\n",
    "        \"max_depth\": hpt.DiscreteParameterSpec(values=[4, 8, 10], scale=None),\n",
    "        \"n_estimators\": hpt.DiscreteParameterSpec(values=[5, 7, 9], scale=None)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if WINNING_MODEL_NAME == 'LogisticRegression':\n",
    "        parameter_spec = lr_parameter_spec\n",
    "    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n",
    "        parameter_spec = rf_parameter_spec\n",
    "    else:\n",
    "        parameter_spec = None \n",
    "    \n",
    "    DISPLAY_NAME = f\"{WINNING_MODEL_NAME}-{TIMESTAMP}\"\n",
    "    \n",
    "    hp_custom_job = aiplatform.CustomJob(display_name=DISPLAY_NAME,\n",
    "                                         worker_pool_specs=worker_pool_specs,\n",
    "                                         staging_bucket=f'gs://{stage_data_bucket}')\n",
    "    \n",
    "    \n",
    "    hp_job = aiplatform.HyperparameterTuningJob(\n",
    "        display_name=DISPLAY_NAME,\n",
    "        custom_job=hp_custom_job,\n",
    "        metric_spec=metric_spec,\n",
    "        parameter_spec=parameter_spec, \n",
    "        max_trial_count=trials,\n",
    "        parallel_trial_count=parallel_trials\n",
    "    )\n",
    "\n",
    "    hp_job.run()\n",
    "    \n",
    "    # helper function\n",
    "    def get_trials_as_df(trials):\n",
    "        results = []\n",
    "        for trial in trials:\n",
    "            row = {}\n",
    "            t = MessageToDict(trial._pb)\n",
    "            # print(t)\n",
    "            row[\"Trial ID\"], row[\"Status\"], row[\"Start time\"], row[\"End time\"] = (\n",
    "                t[\"id\"],\n",
    "                t[\"state\"],\n",
    "                t[\"startTime\"],\n",
    "                t.get(\"endTime\", None),\n",
    "            )\n",
    "\n",
    "            for param in t[\"parameters\"]:\n",
    "                row[param[\"parameterId\"]] = param[\"value\"]\n",
    "\n",
    "            if t[\"state\"] == \"SUCCEEDED\":\n",
    "                row[\"Training step\"] = t[\"finalMeasurement\"][\"stepCount\"]\n",
    "                for metric in t[\"finalMeasurement\"][\"metrics\"]:\n",
    "                    row[metric[\"metricId\"]] = metric[\"value\"]\n",
    "            results.append(row)\n",
    "\n",
    "        _df = pd.DataFrame(results)\n",
    "        return _df\n",
    "    \n",
    "    df_trials = get_trials_as_df(hp_job.trials)\n",
    "    \n",
    "    # get trial id of the best run from the Trials\n",
    "    best_trial_id = df_trials.loc[df_trials[\"f1_score\"].idxmax()][\"Trial ID\"]\n",
    "    # get best run definition\n",
    "    best_run = df_trials[df_trials['Trial ID']==best_trial_id]\n",
    "    \n",
    "    # retrieve parameters tuned in this run\n",
    "    param_names = []\n",
    "\n",
    "    for i in parameter_spec.keys():\n",
    "        param_names.append(i)\n",
    "    \n",
    "    best_run_to_dict = best_run[param_names]\n",
    "    best_run_parameters = best_run_to_dict.to_dict('r')[0]\n",
    "    \n",
    "    model_spec.metadata = best_run_parameters\n",
    "    \n",
    "    kpi_acc = float(best_run['f1_score'])\n",
    "    \n",
    "    kpi.log_metric(\"f1_score\", float(kpi_acc))\n",
    "    model_name.log_metric('model', WINNING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d500b3d0-9bd6-41d6-aa35-cbea3479ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "        #\"xgboost==1.4\"\n",
    "    ], base_image=\"python:3.9\",\n",
    "    output_component_file=\"train_best_model.yaml\"\n",
    ")\n",
    "def train_best_model(\n",
    "    dataset_train:  Input[Dataset],\n",
    "    dataset_val: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    parameters: Input[Artifact],\n",
    "    winning_model_name: Input[Artifact],\n",
    "):\n",
    "    \n",
    "    '''\n",
    "    Entrenamiento del modelo seleccionado con los hiperparametros elegidos. Combina la seleccion de algoritmo e hiperparametros, entrena y pasa el pkl como componente.\n",
    "    Training of the chosen model with its hyperparameters. Combines the algorithm selection and training, and passes the pkl as a component.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #from xgboost import XGBClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    \n",
    "    # get model name and parameters \n",
    "    \n",
    "    best_parameters = parameters.metadata\n",
    "        \n",
    "    model_dict = winning_model_name.metadata\n",
    "    WINNING_MODEL_NAME = model_dict.get('model')\n",
    "    \n",
    "    # choose model and place parameters\n",
    "    if WINNING_MODEL_NAME == 'LogisticRegression':\n",
    "        best_model = LogisticRegression(**best_parameters)\n",
    "    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n",
    "        best_model = RandomForestClassifier(**best_parameters)\n",
    "    else:\n",
    "        best_model = None \n",
    "        \n",
    "    # get data \n",
    "\n",
    "    data_train = pd.read_csv(dataset_train.path+\".csv\")\n",
    "    data_val = pd.read_csv(dataset_val.path+\".csv\")\n",
    "    \n",
    "    data = pd.concat([data_train, data_val])\n",
    "    \n",
    "    # train\n",
    "    best_model.fit(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "    model.metadata[\"framework\"] = WINNING_MODEL_NAME\n",
    "    \n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(best_model, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1db29d7-e626-4add-ba9d-2eb9a6a7a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "        #\"xgboost==1.4\"\n",
    "    ], base_image=\"python:3.9\",\n",
    "    output_component_file=\"best_model_evaluation.yaml\"\n",
    ")\n",
    "def best_model_evaluation(\n",
    "    test_set:  Input[Dataset],\n",
    "    winning_model_name: Input[Artifact], # tiene que saber qué objeto instanciar adentro\n",
    "    best_model: Input[Model], # y acá tomar los datos para cargarlo\n",
    "    best_model_kpi: Output[Metrics],\n",
    "    threshold: float\n",
    ")-> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):\n",
    "    \n",
    "    '''\n",
    "    Toma el mejor modelo entrenado y lo evalua usando el set de test. Si pasa un cierto umbral, devuelve \"true\" y marca el inicio del proximo paso, si no lo hace, el proceso se detiene.\n",
    "    Takes the trained best model and evaluates it using the test set. If it passes a certain threshold, it returns \"true\" and sets the beginning of the next step, if it doesn't, the process halts.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    #from xgboost import XGBClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import pickle\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, f1_score\n",
    "    import json\n",
    "    import typing\n",
    "    \n",
    "    model_dict = winning_model_name.metadata\n",
    "    WINNING_MODEL_NAME = model_dict.get('model')\n",
    "    \n",
    "    #TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    if WINNING_MODEL_NAME == 'LogisticRegression':\n",
    "        model = LogisticRegression()\n",
    "    elif WINNING_MODEL_NAME == 'RandomForestClassifier':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = None \n",
    "\n",
    "    \n",
    "    file_name = best_model.path + \".pkl\"\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        model = pickle.load(file)\n",
    "        \n",
    "    data = pd.read_csv(test_set.path+\".csv\")\n",
    "    y_test = data.drop(columns=[\"target\"])\n",
    "    y_target=data.target\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(y_test)\n",
    "    \n",
    "\n",
    "  \n",
    "    # evaluacion de modelo \n",
    "    f1_value = f1_score(data.target, y_pred.round())\n",
    "    \n",
    "    # toma decision\n",
    "    \n",
    "    if f1_value >= threshold:\n",
    "        dep_decision = 'true'\n",
    "    else:\n",
    "        dep_decision = 'false'\n",
    "    \n",
    "    # guarda la metrica\n",
    "    best_model_kpi.log_metric(\"f1_score\", float(f1_value))\n",
    "    \n",
    "    return (dep_decision, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc4c579-bf91-4d1f-a05f-7799ca099721",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.0.0\", \"google-cloud-bigquery\", \"google-cloud-bigquery-storage\", \"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"upload_model_to_vertex_and_batch_prediction.yaml\"\n",
    ")\n",
    "def upload_model_to_vertex_and_batch_prediction(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    serving_container: str,\n",
    "    trained_model: Input[Model],\n",
    "    winning_model_name: Input[Artifact],\n",
    "    gcs_predict_source: str,\n",
    "    gcs_predict_dest: str\n",
    "\n",
    "):\n",
    "    '''\n",
    "    Toma el mejor modelo entrenado en formato pkl y lo convierte en un Vertex Managed Model a partir del cual se realizan las predicciones en formato batch.\n",
    "    Takes the trained best model in pkl format and uploads it to a Vertex Managed Model and uses it to do a batch prediction job.\n",
    "    '''\n",
    "    \n",
    "    from typing import Dict, Optional, Sequence\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    from datetime import datetime\n",
    "    \n",
    "    model_dict = winning_model_name.metadata\n",
    "    WINNING_MODEL_NAME = model_dict.get('model')\n",
    "    \n",
    "    TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    DISPLAY_NAME = WINNING_MODEL_NAME +'-' + TIMESTAMP\n",
    "    \n",
    "    MODEL_URI = trained_model.uri\n",
    "    MODEL_PATH = MODEL_URI[:-5] # pequeño hack para que encuentre el directorio con el modelo\n",
    "    \n",
    "    def upload_model_sample(\n",
    "        project: str,\n",
    "        location: str,\n",
    "        display_name: str,\n",
    "        serving_container_image_uri: str,\n",
    "        artifact_uri: Optional[str] = None,\n",
    "        sync: bool = True,\n",
    "    ):\n",
    "        \n",
    "\n",
    "        aiplatform.init(project=project, location=location)\n",
    "\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name=display_name,\n",
    "            artifact_uri=artifact_uri,\n",
    "            serving_container_image_uri=serving_container,\n",
    "            sync=sync,\n",
    "        )\n",
    "\n",
    "        model.wait()\n",
    "\n",
    "        print(model.display_name)\n",
    "        print(model.resource_name)\n",
    "        return model\n",
    "    \n",
    "    model_test = upload_model_sample(\n",
    "        project = project,\n",
    "        location = region,\n",
    "        display_name = DISPLAY_NAME,\n",
    "        serving_container_image_uri= serving_container,\n",
    "        artifact_uri = MODEL_PATH\n",
    "    )\n",
    "    \n",
    "    batch_job = model_test.batch_predict(\n",
    "        job_display_name=DISPLAY_NAME,\n",
    "        gcs_source = gcs_predict_source,\n",
    "        instances_format=\"csv\",\n",
    "        gcs_destination_prefix=gcs_predict_dest,\n",
    "        machine_type = 'n1-standard-16'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32bb45f-9baf-4392-b865-f959f6ff2190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66039057-582c-4133-b143-e3f2283377a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
