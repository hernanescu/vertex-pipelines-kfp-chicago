{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cabc30f7-950a-4f6a-9e9f-f7288e56fb5c",
   "metadata": {},
   "source": [
    "# GCP Resources Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc99a1-a00e-4419-9f99-526b69d5c5ae",
   "metadata": {},
   "source": [
    "En esta notebook se configura todo lo necesario para la ejecución del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc9a37-498d-4368-821d-987bc38aba4d",
   "metadata": {},
   "source": [
    "In this notebook we configure everything we need to run the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83182f7-b8e4-4d82-84f8-1541305f97e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install libraries - Paquetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad07f0-98b4-49a7-b128-4658df6bbf7b",
   "metadata": {},
   "source": [
    "Se ofrecen dos versiones un archivo de texto con las librerías: una con las instalaciones realizadas de manera directa (*requirements.txt*) y una que tiene la totalidad del entorno con el que se realizó este pipeline (*full_requirements.txt*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f389360-a865-4af2-8b42-c52407757805",
   "metadata": {},
   "source": [
    "We present two requirement text files: one with the critical imports needed (*requirements.txt*) and another with the entirety of the environment with which the pipeline was run (*full_requirements.txt*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "288598db-64d6-49cd-a956-a593d9db6ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-cloud-pipeline-components==0.2.0\n",
      "kfp==1.8.9\n",
      "scikit-learn==1.0.0\n",
      "google-cloud-bigquery\n",
      "google-cloud-bigquery-storage\n",
      "google-cloud-aiplatform\n",
      "pandas\n",
      "numpy"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1748bf-7332-45be-869d-bf315a35bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c86100e-0e9e-4338-989d-0f229e4ebe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  teco-prod-adam-dev-826c\n",
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import kfp\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath, ClassificationMetrics\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# We'll use this namespace for metadata querying\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "    \n",
    "    \n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d874cf39-cc2c-4767-a5c9-475a0d644a92",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BigQuery - Database (Raw stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0333a1e-24e8-46e6-b99a-327f840fee98",
   "metadata": {},
   "source": [
    "Necesitamos crear dos datasets en BigQuery: uno que oficiará de histórico, y otro que representará el momento actual (no va a tener variable target). Para eso usaremos el gcloud cli, junto con algunas variables de Python. \n",
    "\n",
    "**Importante**: Este código tiene que correrse sólo una vez, es para crear las bases necesarias que se usarán a lo largo del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd435d5-5669-4c61-b6bf-2ec8b5cb17c7",
   "metadata": {},
   "source": [
    "We need to create two datasets in BigQuery: one will be our historical data, and the other one will simulate the current time (the target variable will be missing). We'll use the gcloud cli, along with some Python variables.\n",
    "\n",
    "**Important**: This code needs to run only once to set up the necessary datasets to be used through the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdbfb06-728c-4996-9ea2-cabf3c603037",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET_HISTORIC_NAME = 'chicago_taxi_historic_test'\n",
    "BQ_DATASET_CURRENT_NAME = 'chicago_taxi_current_test'\n",
    "\n",
    "BQ_HISTORIC_RAW = 'raw'\n",
    "BQ_CURRENT_RAW = 'raw'\n",
    "\n",
    "BQ_LOCATION = 'US'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99a9501-e3b4-4bd5-a93f-48646a5be31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'vertex-testing-327520:chicago_taxi_historic_test' successfully created.\n"
     ]
    }
   ],
   "source": [
    "# !bq --location=US mk -d \\\n",
    "# $PROJECT_ID:$BQ_DATASET_HISTORIC_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8507fa20-92e5-4f37-9819-abbac7dc30ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'vertex-testing-327520:chicago_taxi_current_test' successfully created.\n"
     ]
    }
   ],
   "source": [
    "# !bq --location=US mk -d \\\n",
    "# $PROJECT_ID:$BQ_DATASET_CURRENT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5236d-434b-4546-9357-89acf31f5141",
   "metadata": {},
   "source": [
    "Se preparan dos funciones para automatizar y parametrizar la búsqueda de variables de acuerdo a tiempos y volúmenes deseados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac677955-336a-46f8-8cba-d023eece9ef2",
   "metadata": {},
   "source": [
    "We create two functions in order to automate and parametrize variables according to times and size needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46488da4-cbba-4e33-8326-fbbed6cadae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def get_year_and_month():\n",
    "    previous_month = (dt.date.today().replace(day=1) - dt.timedelta(days=33)).month\n",
    "    year = dt.date.today().year\n",
    "    \n",
    "    if previous_month == 12:\n",
    "        year = year-1\n",
    "    else:\n",
    "        year\n",
    "    return year, previous_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd5c5c5-d161-4681-aa20-327b6edc666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_and_month_hist(current_year, current_month):\n",
    "    month_hist = current_month - 1\n",
    "    if month_hist == 0:\n",
    "        year_hist = current_year -1\n",
    "        month_hist = 12\n",
    "    else:\n",
    "        year_hist = current_year\n",
    "    \n",
    "    return year_hist, month_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69312c25-4efc-43b3-8ce3-a9f8da059499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current year:  2022\n",
      "current month:  1\n",
      "sample size:  100000\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "SAMPLE_SIZE = 100000\n",
    "YEAR, MONTH = get_year_and_month()\n",
    "\n",
    "print('current year: ', YEAR)\n",
    "print('current month: ', MONTH)\n",
    "print('sample size: ', SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a2d234-978a-4f42-8c11-d03198f4866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past year:  2021\n",
      "past month:  12\n"
     ]
    }
   ],
   "source": [
    "HIST_YEAR, HIST_MONTH = get_year_and_month_hist(YEAR, MONTH)\n",
    "print('past year: ', HIST_YEAR)\n",
    "print('past month: ', HIST_MONTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a3525-c78c-424f-87e7-7026aa7aa88f",
   "metadata": {},
   "source": [
    "### Dataset actual - Current dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3102d8-2b9d-4e24-a7c2-36f8da2b11ce",
   "metadata": {},
   "source": [
    "Usamos una query de SQL para poblar la tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc1336-c92f-4472-96ac-5bb1eb194647",
   "metadata": {},
   "source": [
    "We use a SQL query to load the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f6fc18e-c8e1-4e43-9113-d5bedbc0e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_sql_script = '''\n",
    "CREATE OR REPLACE TABLE `@PROJECT_ID.@DATASET.@TABLE` \n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "      WHERE 1=1 \n",
    "      AND pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "      AND EXTRACT(MONTH FROM trip_start_timestamp) = @MONTH\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      CONCAT(\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
    "              pickup_latitude), 0.1)), \n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
    "              dropoff_latitude), 0.1))\n",
    "      ) AS loc_cross,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      IF(ABS(MOD(FARM_FINGERPRINT(STRING(trip_start_timestamp)), 10)) < 9, 'UNASSIGNED', 'TEST') AS data_split\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT @LIMIT\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5e930e-8924-4230-8c7e-ada1da94f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_sql_script = current_sql_script.replace(\n",
    "    '@PROJECT_ID', PROJECT_ID).replace(\n",
    "    '@DATASET', BQ_DATASET_CURRENT_NAME).replace(\n",
    "    '@TABLE', BQ_CURRENT_RAW).replace(\n",
    "    '@YEAR', str(YEAR)).replace(\n",
    "    '@LIMIT', str(SAMPLE_SIZE)).replace(\n",
    "    '@MONTH', str(MONTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e812b3-c1e5-43e8-b104-b836e16bfd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=BQ_LOCATION)\n",
    "job = bq_client.query(current_sql_script)\n",
    "_ = job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947fb15-68b3-4df4-aaa6-70b263daf6db",
   "metadata": {},
   "source": [
    "### Dataset historico - Historic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0703445-9c4a-46d9-b1e8-983f2be54257",
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_sql_script = '''\n",
    "CREATE OR REPLACE TABLE `@PROJECT_ID.@DATASET.@TABLE` \n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "      WHERE 1=1 \n",
    "      AND pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "      AND EXTRACT(MONTH FROM trip_start_timestamp) = @MONTH\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      CONCAT(\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
    "              pickup_latitude), 0.1)), \n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
    "              dropoff_latitude), 0.1))\n",
    "      ) AS loc_cross,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      IF(ABS(MOD(FARM_FINGERPRINT(STRING(trip_start_timestamp)), 10)) < 9, 'UNASSIGNED', 'TEST') AS data_split\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT @LIMIT\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb835be7-7c16-4376-9cf2-382e09973f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_sql_script = historic_sql_script.replace(\n",
    "    '@PROJECT_ID', PROJECT_ID).replace(\n",
    "    '@DATASET', BQ_DATASET_HISTORIC_NAME).replace(\n",
    "    '@TABLE', BQ_HISTORIC_RAW).replace(\n",
    "    '@YEAR', str(HIST_YEAR)).replace(\n",
    "    '@LIMIT', str(SAMPLE_SIZE)).replace(\n",
    "    '@MONTH', str(HIST_MONTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46bce57e-14be-475f-b5ec-646bb7c0f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=BQ_LOCATION)\n",
    "job = bq_client.query(historic_sql_script)\n",
    "_ = job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5be86-b757-4feb-b937-8d087536d394",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cloud Storage - Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94519545-d44b-428a-9734-5d5413a24e4c",
   "metadata": {},
   "source": [
    "Crearemos buckets en GCS para almacenar distintos tipos de objetos que va produciendo el pipeline a lo largo del camino. En *stage* se guardarán algunos más relevantes, como respaldos de las particiones de train, validación y test para poder acceder más rapidamente y fácilmente, mientras que en *pipelines* habrá mayormente logs y resultados de ejecuciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8f6b7-17f2-461e-a0a1-ecf79b0e21cb",
   "metadata": {},
   "source": [
    "We'll create GCS buckets to store different types of objects that the pipeline produces through its execution. In *stage* there will be some relevant files, such as easily accesible backups of train, validation and test data in order to perform quick reviews if needed, whereas in *pipeline* there will mostly be execution outputs and logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f96c959-4834-43e5-a817-d7c91dfea381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage bucket:  teco-prod-adam-dev-826c-chicago_taxi_stage\n",
      "Pipeline bucket:  teco-prod-adam-dev-826c-chicago_taxi_pipelines\n"
     ]
    }
   ],
   "source": [
    "STAGE_DATA_BUCKET = f'{PROJECT_ID}-chicago_taxi_stage'\n",
    "PIPELINE_DATA_BUCKET = f'{PROJECT_ID}-chicago_taxi_pipelines'\n",
    "\n",
    "print('Stage bucket: ', STAGE_DATA_BUCKET)\n",
    "print('Pipeline bucket: ', PIPELINE_DATA_BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0320778-c128-4cc6-87f6-53c86189c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def create_bucket_class_location(bucket_name, location):\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    new_bucket = storage_client.create_bucket(bucket, location=location)\n",
    "\n",
    "    print(\n",
    "        \"Created bucket {} in {} \".format(\n",
    "            new_bucket.name, new_bucket.location\n",
    "        )\n",
    "    )\n",
    "    return new_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00f9a226-8ac0-44bb-a8a3-656f736fd4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket vertex-testing-327520-chicago_taxi_stage in US-CENTRAL1 \n"
     ]
    }
   ],
   "source": [
    "bucket_stage = create_bucket_class_location(STAGE_DATA_BUCKET, REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28d3f34-14c4-4f95-9c30-6ca498cb54d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket vertex-testing-327520-chicago_taxi_pipelines in US-CENTRAL1 \n"
     ]
    }
   ],
   "source": [
    "bucket_pipeline = create_bucket_class_location(PIPELINE_DATA_BUCKET, REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4da4b-b33f-425d-89cf-828bb1384ce5",
   "metadata": {},
   "source": [
    "## Container Registry - Hyperparameter tuning jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c647c-4d2d-4142-a05d-90adff9bdf35",
   "metadata": {},
   "source": [
    "Para realizar un job de tuneo de hiperparámetros, la forma más conveniente es mediante el uso de imágenes de Docker. Se ofrecen dos configuraciones para algoritmos: Random Forest y regresión logística."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0cb3d6-f653-4c4a-9587-78f5ab4f6d5f",
   "metadata": {},
   "source": [
    "To perform a hyperparameter tuning job, the most convenient way is through the use of Docker images. There are two algorithm configurations: Random Forest and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabf7ab-21a5-4c39-9c86-12480aa66dd9",
   "metadata": {},
   "source": [
    "#### General structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "080af8de-a682-4d27-b478-0cd8845b254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mhp_lr\u001b[00m\n",
      "├── Dockerfile\n",
      "└── \u001b[01;34mtrainer\u001b[00m\n",
      "    └── task.py\n",
      "\n",
      "1 directory, 2 files\n"
     ]
    }
   ],
   "source": [
    "!tree hp_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f227501-3723-4eea-8abf-b164fa23f5c5",
   "metadata": {},
   "source": [
    "#### Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87afdb17-a474-4651-a336-e8e26baf3b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from gcr.io/deeplearning-platform-release/sklearn-cpu\n",
      "\n",
      "WORKDIR /\n",
      "\n",
      "# Installs hypertune library\n",
      "RUN pip install cloudml-hypertune sklearn scipy google-cloud-bigquery joblib pandas google-cloud-storage\n",
      "\n",
      "# Copies the trainer code to the docker image.\n",
      "COPY trainer /trainer\n",
      "\n",
      "# Sets up the entry point to invoke the trainer.\n",
      "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
     ]
    }
   ],
   "source": [
    "cat hp_lr/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8902a9-96d1-4b24-81c9-88490bb6fca5",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f4de654-b1f7-40f2-8dc0-7bcae1e5267b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, f1_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from google.cloud import bigquery#\n",
      "from google.cloud import storage\n",
      "from joblib import dump\n",
      "\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "#from xgboost import XGBClassifier\n",
      "#from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "import argparse\n",
      "import hypertune\n",
      "from sklearn.model_selection import train_test_split as tts\n",
      "\n",
      "\n",
      "\n",
      "STAGE_DATA_BUCKET = 'your_bucket'\n",
      "TRAIN_DATA_PATH = 'data/chicago_taxi_train.csv'\n",
      "LOCAL_DATA_PATH = 'chicago_taxi_train.csv'\n",
      "\n",
      "cols = ['trip_month', 'trip_day', 'trip_day_of_week',\n",
      "       'trip_hour', 'trip_seconds', 'trip_miles', 'euclidean', 'target',\n",
      "       'payment_type_Credit_Card', 'payment_type_Dispute', 'payment_type_Mobile',\n",
      "       'payment_type_No_Charge', 'payment_type_Prcard', 'payment_type_Unknown']\n",
      "\n",
      "def get_args():\n",
      "    '''Parses args. Must include all hyperparameters you want to tune.'''\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\n",
      "      '--penalty',\n",
      "      required=True,\n",
      "      type=str,\n",
      "      help='Penalty')\n",
      "    parser.add_argument(\n",
      "      '--C',\n",
      "      required=True,\n",
      "      type=float,\n",
      "      help='Inverse of regularization')\n",
      "    parser.add_argument(\n",
      "      '--solver',\n",
      "      required=True,\n",
      "      type=str,\n",
      "      help='Solver')\n",
      "    args = parser.parse_args()\n",
      "    return args\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def create_dataset():\n",
      "    \n",
      "    bqclient = bigquery.Client()\n",
      "    storage_client = storage.Client()\n",
      "\n",
      "    gcsclient = storage.Client() # tal vez vaya stage_data_bucket\n",
      "    bucket = gcsclient.get_bucket(STAGE_DATA_BUCKET)\n",
      "    blob = bucket.blob(TRAIN_DATA_PATH)\n",
      "    blob.download_to_filename(LOCAL_DATA_PATH)\n",
      "\n",
      "    data = pd.read_csv(LOCAL_DATA_PATH, usecols=cols)\n",
      "    # Si estas usando volumenes muy grnades de datos, usa un sample \n",
      "    # If you're dealing with really big data, use a sample data = data.sample(frac = 0.2, random_state = 42)\n",
      "    train_data, validation_data = tts(data, test_size=0.3)\n",
      "    return train_data, validation_data\n",
      "\n",
      "def split_data_and_labels(data):\n",
      "    y = data.pop('target')\n",
      "    return data, y\n",
      "\n",
      "    \n",
      "\n",
      "def create_model(penalty, C, solver):\n",
      "    model = LogisticRegression(\n",
      "        penalty = penalty,\n",
      "        C = C,\n",
      "        solver = solver\n",
      "    )\n",
      "    \n",
      "    return model\n",
      "\n",
      "def main():\n",
      "    args = get_args()\n",
      "    \n",
      "    train_data, validation_data = create_dataset()\n",
      "    x_train, y_train = split_data_and_labels(train_data)\n",
      "    x_test, y_test = split_data_and_labels(validation_data)\n",
      "    \n",
      "    model = create_model(args.penalty, args.C, args.solver)\n",
      "    model = model.fit(x_train, y_train)\n",
      "    \n",
      "    y_pred = model.predict(x_test)\n",
      "    f1_value = f1_score(y_test, y_pred)\n",
      "    \n",
      "    hpt = hypertune.HyperTune()\n",
      "    hpt.report_hyperparameter_tuning_metric(\n",
      "        hyperparameter_metric_tag='f1_score',\n",
      "        metric_value=f1_value\n",
      "    )\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "cat hp_lr/trainer/task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4ce80-1aa6-490b-9947-c5aa7fba7264",
   "metadata": {},
   "source": [
    "- Se pasan los hiperparametros a iterar como argumentos (argparser).\n",
    "- Se selecciona la metrica (f1_score) y el objetivo (maximizar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a68875-0a8f-4687-a6e5-343d7304074f",
   "metadata": {},
   "source": [
    "- Pass the hyperparameters to iterate as arguments (argparser).\n",
    "- Choose metrics (f1_score) and goal (maximize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90004a19-d1d0-4ab2-a484-583be01d9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/your_bucket/{STAGE_DATA_BUCKET}/' hp_lr/trainer/task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1773ce3-c3bc-4cd6-a80d-222356b7b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/your_bucket/{STAGE_DATA_BUCKET}/' hp_rf/trainer/task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005446b-7a18-4fde-a831-0d1029d2eb8c",
   "metadata": {},
   "source": [
    "#### Image: build and push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24350634-2f6a-4870-afbb-9c9538e644cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/teco-prod-adam-dev-826c/rf_hp_job:v1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_FOREST_IMAGE = f'gcr.io/{PROJECT_ID}/rf_hp_job:v1'\n",
    "RANDOM_FOREST_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2804df14-270c-4e7c-9b17-eb3bdad608a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/teco-prod-adam-dev-826c/lr_hp_job:v1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_REG_IMAGE = f'gcr.io/{PROJECT_ID}/lr_hp_job:v1'\n",
    "LOG_REG_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2594c50-7f1b-4006-9361-aa9ea75bc096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/5 : from gcr.io/deeplearning-platform-release/sklearn-cpu\n",
      " ---> 2574879dfd34\n",
      "Step 2/5 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 3dbd66708df1\n",
      "Step 3/5 : RUN pip install cloudml-hypertune sklearn scipy google-cloud-bigquery joblib pandas google-cloud-storage\n",
      " ---> Using cache\n",
      " ---> 0040d68f198b\n",
      "Step 4/5 : COPY trainer /trainer\n",
      " ---> 5f12dfadda9e\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in 457ce0cad30d\n",
      "Removing intermediate container 457ce0cad30d\n",
      " ---> ed1dea4c596c\n",
      "Successfully built ed1dea4c596c\n",
      "Successfully tagged gcr.io/teco-prod-adam-dev-826c/rf_hp_job:v1\n"
     ]
    }
   ],
   "source": [
    "# !docker build ./hp_rf -t $RANDOM_FOREST_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92b8bf14-0847-4ff9-ae21-d7dd32aa07dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/5 : from gcr.io/deeplearning-platform-release/sklearn-cpu\n",
      " ---> 2574879dfd34\n",
      "Step 2/5 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 3dbd66708df1\n",
      "Step 3/5 : RUN pip install cloudml-hypertune sklearn scipy google-cloud-bigquery joblib pandas google-cloud-storage\n",
      " ---> Using cache\n",
      " ---> 0040d68f198b\n",
      "Step 4/5 : COPY trainer /trainer\n",
      " ---> 78bbf259a494\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in f75c53f39ec8\n",
      "Removing intermediate container f75c53f39ec8\n",
      " ---> 47e042de7f52\n",
      "Successfully built 47e042de7f52\n",
      "Successfully tagged gcr.io/teco-prod-adam-dev-826c/lr_hp_job:v1\n"
     ]
    }
   ],
   "source": [
    "# !docker build ./hp_lr -t $LOG_REG_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6ddf581-dfc3-40df-886a-436a594288c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/teco-prod-adam-dev-826c/lr_hp_job]\n",
      "\n",
      "\u001b[1B8b9ad592: Preparing \n",
      "\u001b[1Bc56a3432: Preparing \n",
      "\u001b[1Bbfb2e242: Preparing \n",
      "\u001b[1Bb0d69ede: Preparing \n",
      "\u001b[1Bc3dd1b30: Preparing \n",
      "\u001b[1Bf55e5b0f: Preparing \n",
      "\u001b[1Bb8042115: Preparing \n",
      "\u001b[1Be7ceaeea: Preparing \n",
      "\u001b[1B659ee3aa: Preparing \n",
      "\u001b[1B153ced2f: Preparing \n",
      "\u001b[1Bb90e8bce: Preparing \n",
      "\u001b[1Bd686dc1d: Preparing \n",
      "\u001b[1Bc56dcfc0: Preparing \n",
      "\u001b[1Bdd09476c: Preparing \n",
      "\u001b[1B384be1ed: Preparing \n",
      "\u001b[1B0864cc76: Preparing \n",
      "\u001b[1B24e4876e: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B282950fe: Preparing \n",
      "\u001b[1B0b19050d: Preparing \n",
      "\u001b[1Bb453bec5: Preparing \n",
      "\u001b[22Bb9ad592: Pushed lready exists 5kB\u001b[21A\u001b[2K\u001b[16A\u001b[2K\u001b[17A\u001b[2K\u001b[10A\u001b[2K\u001b[8A\u001b[2K\u001b[3A\u001b[2K\u001b[4A\u001b[2K\u001b[22A\u001b[2Kv1: digest: sha256:8c44e9838cd84363ded24fb75c08d46d654127c0e69c93ac175e7d74a87a243b size: 4916\n"
     ]
    }
   ],
   "source": [
    "# !docker push $LOG_REG_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d24529c8-5c49-4c4a-950d-413e7b38d274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/teco-prod-adam-dev-826c/rf_hp_job]\n",
      "\n",
      "\u001b[1B9f902063: Preparing \n",
      "\u001b[1Bc56a3432: Preparing \n",
      "\u001b[1Bbfb2e242: Preparing \n",
      "\u001b[1Bb0d69ede: Preparing \n",
      "\u001b[1Bc3dd1b30: Preparing \n",
      "\u001b[1Bf55e5b0f: Preparing \n",
      "\u001b[1Bb8042115: Preparing \n",
      "\u001b[1Be7ceaeea: Preparing \n",
      "\u001b[1B659ee3aa: Preparing \n",
      "\u001b[1B153ced2f: Preparing \n",
      "\u001b[1Bb90e8bce: Preparing \n",
      "\u001b[1Bd686dc1d: Preparing \n",
      "\u001b[1Bc56dcfc0: Preparing \n",
      "\u001b[1Bdd09476c: Preparing \n",
      "\u001b[1B384be1ed: Preparing \n",
      "\u001b[1B0864cc76: Preparing \n",
      "\u001b[1B24e4876e: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B282950fe: Preparing \n",
      "\u001b[1B0b19050d: Preparing \n",
      "\u001b[1Bb453bec5: Preparing \n",
      "\u001b[22Bf902063: Pushed lready exists 3kB\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[22A\u001b[2Kv1: digest: sha256:c45a8a6c16f117abf1776d2ecadb12e8f9f4494dba0cfccae0ed23771ae0a70b size: 4916\n"
     ]
    }
   ],
   "source": [
    "# !docker push $RANDOM_FOREST_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c59a04-851f-4878-be5c-633c97e7c68b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
